{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Let‚Äôs rank the vectorizers **from fastest and smallest memory usage ‚Üí slower and heavier** while also considering **ease of use with classic ML algorithms**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Lightweight & Fast (Low Memory, Sparse Output)**\n",
        "\n",
        "| Vectorizer            | Size / Memory | Speed     | Notes / Use Case                                                            |\n",
        "| --------------------- | ------------- | --------- | --------------------------------------------------------------------------- |\n",
        "| **CountVectorizer**   | Low           | Very Fast | Good for small datasets and simple ML models.                               |\n",
        "| **TfidfVectorizer**   | Low-Medium    | Fast      | Adds weighting, slightly slower than CountVectorizer but still lightweight. |\n",
        "| **HashingVectorizer** | Very Low      | Very Fast | No `.fit()`, fixed size vector ‚Üí memory-efficient for huge datasets.        |\n",
        "\n",
        "‚úÖ **Best for:** RandomForest, LogisticRegression, Naive Bayes, LinearSVM.\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Medium (Dense Embeddings, Pretrained)**\n",
        "\n",
        "| Vectorizer                | Size / Memory | Speed  | Notes / Use Case                                                                                                    |\n",
        "| ------------------------- | ------------- | ------ | ------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Word2Vec (pretrained)** | Medium        | Medium | Generates dense vectors (~100‚Äì300 dims per word). Needs preprocessing + averaging over words for sentence-level ML. |\n",
        "| **GloVe (pretrained)**    | Medium        | Medium | Similar to Word2Vec, pre-trained embeddings avoid training time.                                                    |\n",
        "| **FastText**              | Medium        | Medium | Slightly bigger than Word2Vec due to subword info; still reasonable.                                                |\n",
        "\n",
        "‚úÖ **Best for:** Classic ML with dense vectors, semantic similarity, multi-label classification.\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Heavy / Slower (Transformer-Based Contextual Embeddings)**\n",
        "\n",
        "| Vectorizer                               | Size / Memory        | Speed                  | Notes / Use Case                                                          |\n",
        "| ---------------------------------------- | -------------------- | ---------------------- | ------------------------------------------------------------------------- |\n",
        "| **BERT / SBERT / DistilBERT embeddings** | Large (hundreds MBs) | Slow (GPU recommended) | Contextual embeddings ‚Üí one vector per sentence. Best for SOTA NLP tasks. |\n",
        "| **RoBERTa / Large Transformer models**   | Very Large (GBs)     | Very Slow              | High accuracy, very heavy. Usually overkill for small ML datasets.        |\n",
        "\n",
        "‚úÖ **Best for:** Semantic search, complex NLP classification, embeddings for downstream tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **üí° Summary for Fast ML Pipeline**\n",
        "\n",
        "If your goal is **low latency and fast training with scikit-learn models**:\n",
        "\n",
        "1. **TfidfVectorizer** ‚Üí Most common choice, balances speed & effectiveness.\n",
        "2. **CountVectorizer** ‚Üí Very simple, super fast.\n",
        "3. **HashingVectorizer** ‚Üí For huge datasets or streaming data.\n",
        "\n",
        "If accuracy is more important than speed, or you want **contextual embeddings**:\n",
        "\n",
        "* **SentenceTransformer (‚Äòall-MiniLM-L6-v2‚Äô)** ‚Üí Dense, moderate size, relatively fast transformer.\n",
        "* Avoid full BERT/RoBERTa unless you have GPU and large datasets.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aFpyypED3M2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LulUXwpexGcS",
        "outputId": "2c4655f1-e85d-4b96-f7bd-eee8bbe1ba07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4991, 10), (1707, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "final_df=pd.read_csv('final.csv').iloc[:,3:]\n",
        "\n",
        "clean_test=pd.read_csv('cleaned.csv')\n",
        "final_df.shape,clean_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim CatBoost"
      ],
      "metadata": {
        "id": "DQU8tK7Q6Xu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzTDvi325ugp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "üöÄ Industry-Grade Multi-Label Text Classification Pipeline\n",
        "Features: Advanced Experiment Tracking with Multiple Vectorizers\n",
        "Including: TF-IDF, Count, Hashing, Word2Vec, GloVe, FastText\n",
        "Comprehensive Metrics, Production-Ready Visualizations\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wandb\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "from itertools import product\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score, classification_report, confusion_matrix,\n",
        "    roc_curve, auc, roc_auc_score, precision_recall_curve,\n",
        "    average_precision_score, hamming_loss, jaccard_score,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# =========================\n",
        "# üé® Configuration & Setup\n",
        "# =========================\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Set style for professional visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"plots\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"reports\").mkdir(exist_ok=True)\n",
        "\n",
        "# Experiment configuration\n",
        "EXPERIMENT_CONFIG = {\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42,\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "}\n",
        "\n",
        "# Vectorizer configurations\n",
        "VECTORIZER_CONFIGS = {\n",
        "    'tfidf_max_features': [5000],\n",
        "    'ngram_ranges': [(1, 1), (1, 2), (1, 3)]  # unigrams, bigrams, trigrams\n",
        "}\n",
        "\n",
        "# Initialize W&B with rich config\n",
        "wandb.init(\n",
        "    project=\"23f3003030-t32025\",\n",
        "    name=f\"D02-multi-vectorizer-classification-{EXPERIMENT_CONFIG['timestamp']}\",\n",
        "    config=EXPERIMENT_CONFIG,\n",
        "    tags=[\"multi-label\", \"emotion-detection\", \"vectorizer-comparison\", \"production\"],\n",
        "    notes=\"Comprehensive experiment comparing all vectorizers and models\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# üìä Advanced Visualization Functions\n",
        "# =========================\n",
        "\n",
        "def plot_combined_confusion_matrix(y_true, y_pred, emotions, model_name, vec_name):\n",
        "    \"\"\"Create professional confusion matrix visualization\"\"\"\n",
        "    n_emotions = len(emotions)\n",
        "    fig, axes = plt.subplots(1, n_emotions, figsize=(4*n_emotions, 3.5))\n",
        "\n",
        "    if n_emotions == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        cm = confusion_matrix(y_true.iloc[:, i], y_pred[:, i])\n",
        "\n",
        "        # Calculate percentages\n",
        "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "        # Create annotations with counts and percentages\n",
        "        annot = np.array([[f'{count}\\n({percent:.1f}%)'\n",
        "                          for count, percent in zip(row_counts, row_percents)]\n",
        "                         for row_counts, row_percents in zip(cm, cm_percent)])\n",
        "\n",
        "        sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', ax=axes[i],\n",
        "                    cbar=False, square=True, linewidths=1, linecolor='gray')\n",
        "        axes[i].set_title(f'{emotion.upper()}', fontsize=12, fontweight='bold', pad=10)\n",
        "        axes[i].set_xlabel('Predicted', fontsize=10)\n",
        "        axes[i].set_ylabel('Actual' if i == 0 else '', fontsize=10)\n",
        "        axes[i].set_xticklabels(['No', 'Yes'])\n",
        "        axes[i].set_yticklabels(['No', 'Yes'])\n",
        "\n",
        "    fig.suptitle(f'{model_name} ({vec_name}) - Confusion Matrices',\n",
        "                 fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{vec_name}_{model_name}_confusion_matrix.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{vec_name}/{model_name}/confusion_matrix\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_roc_curves(y_true, y_pred_proba, emotions, model_name, vec_name):\n",
        "    \"\"\"Plot ROC curves with AUC scores\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 9))\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(emotions)))\n",
        "\n",
        "    roc_auc_scores = {}\n",
        "\n",
        "    for i, (emotion, color) in enumerate(zip(emotions, colors)):\n",
        "        fpr, tpr, _ = roc_curve(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_auc_scores[emotion] = roc_auc\n",
        "\n",
        "        ax.plot(fpr, tpr, color=color, lw=3,\n",
        "                label=f'{emotion.capitalize()} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "    # Add diagonal line\n",
        "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5)', alpha=0.6)\n",
        "\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'{model_name} ({vec_name}) - ROC Curves', fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.legend(loc=\"lower right\", fontsize=11, framealpha=0.9)\n",
        "    ax.grid(alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add mean AUC text box\n",
        "    mean_auc = np.mean(list(roc_auc_scores.values()))\n",
        "    textstr = f'Mean AUC: {mean_auc:.4f}'\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
        "    ax.text(0.65, 0.15, textstr, transform=ax.transAxes, fontsize=13,\n",
        "            verticalalignment='top', bbox=props, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{vec_name}_{model_name}_roc_curves.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{vec_name}/{model_name}/roc_curves\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "    return roc_auc_scores\n",
        "\n",
        "\n",
        "def plot_classification_report(class_report_dict, emotions, model_name, vec_name):\n",
        "    \"\"\"Visualize classification report as heatmap\"\"\"\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "    data = []\n",
        "\n",
        "    for emotion in emotions:\n",
        "        if emotion in class_report_dict:\n",
        "            row = [class_report_dict[emotion][m] for m in metrics]\n",
        "            data.append(row)\n",
        "\n",
        "    df_report = pd.DataFrame(data, index=[e.capitalize() for e in emotions], columns=metrics)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.heatmap(df_report, annot=True, fmt='.4f', cmap='RdYlGn',\n",
        "                cbar_kws={'label': 'Score'}, vmin=0, vmax=1, ax=ax,\n",
        "                linewidths=2, linecolor='white', annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "    ax.set_title(f'{model_name} ({vec_name}) - Classification Metrics',\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Metrics', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Emotions', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{vec_name}_{model_name}_classification_report.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{vec_name}/{model_name}/classification_report\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_label_distribution(y_data, title, filename):\n",
        "    \"\"\"Plot label distribution\"\"\"\n",
        "    label_counts = y_data.sum().sort_values(ascending=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.bar(range(len(label_counts)), label_counts.values,\n",
        "                  color=plt.cm.viridis(np.linspace(0, 1, len(label_counts))))\n",
        "    ax.set_xticks(range(len(label_counts)))\n",
        "    ax.set_xticklabels([label.capitalize() for label in label_counts.index], fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = OUTPUT_DIR / \"plots\" / filename\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"data_analysis/{filename.replace('.png', '')}\": wandb.Image(str(save_path))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compute_advanced_metrics(y_true, y_pred, y_pred_proba, emotions):\n",
        "    \"\"\"Compute comprehensive metrics\"\"\"\n",
        "    metrics = {\n",
        "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
        "        'jaccard_score': jaccard_score(y_true, y_pred, average='samples', zero_division=0),\n",
        "        'subset_accuracy': accuracy_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "    # Per-emotion metrics\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        metrics[f'{emotion}_f1'] = f1_score(y_true.iloc[:, i], y_pred[:, i], zero_division=0)\n",
        "        try:\n",
        "            metrics[f'{emotion}_auc'] = roc_auc_score(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        except:\n",
        "            metrics[f'{emotion}_auc'] = 0.0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_vectorizer(vec_type, ngram_range, max_features):\n",
        "    \"\"\"Factory function to create vectorizers\"\"\"\n",
        "    if vec_type == 'tfidf':\n",
        "        return TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "    elif vec_type == 'count':\n",
        "        return CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "    elif vec_type == 'hashing':\n",
        "        return HashingVectorizer(n_features=max_features, ngram_range=ngram_range)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown vectorizer type: {vec_type}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£ Data Preparation & EDA\n",
        "# =========================\n",
        "print(\"=\" * 80)\n",
        "print(\"üöÄ STARTING COMPREHENSIVE VECTORIZER + MODEL EXPERIMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "X = final_df['final_text'].fillna('')\n",
        "y = final_df[['anger', 'fear', 'joy', 'sadness', 'surprise']]\n",
        "emotions = y.columns.tolist()\n",
        "\n",
        "# Log dataset info\n",
        "wandb.log({\n",
        "    \"dataset/total_samples\": len(X),\n",
        "    \"dataset/num_emotions\": len(emotions),\n",
        "    \"dataset/feature_name\": \"final_text\"\n",
        "})\n",
        "\n",
        "# Analyze and log label distribution\n",
        "print(\"\\nüìä Analyzing Label Distribution...\")\n",
        "plot_label_distribution(y, \"Training Data - Emotion Distribution\", \"train_label_distribution.png\")\n",
        "\n",
        "# Log label statistics\n",
        "label_stats = {}\n",
        "for emotion in emotions:\n",
        "    label_stats[f\"dataset/{emotion}_count\"] = int(y[emotion].sum())\n",
        "    label_stats[f\"dataset/{emotion}_percentage\"] = float(y[emotion].sum() / len(y) * 100)\n",
        "\n",
        "wandb.log(label_stats)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=EXPERIMENT_CONFIG['test_size'],\n",
        "    random_state=EXPERIMENT_CONFIG['random_state']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train set size: {len(X_train)} | Validation set size: {len(X_val)}\")\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ Define Vectorizers\n",
        "# =========================\n",
        "VECTORIZERS = {\n",
        "    'TfidfVectorizer': 'tfidf',\n",
        "    'CountVectorizer': 'count',\n",
        "    'HashingVectorizer': 'hashing'\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ Model Configuration\n",
        "# =========================\n",
        "CLASSIFIERS = {\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "    'LinearSVC': LinearSVC(max_iter=5000, random_state=42),\n",
        "\n",
        "\n",
        "    'XGBoost': XGBClassifier(\n",
        "        eval_metric='logloss', n_estimators=500, max_depth=6,\n",
        "        learning_rate=0.05, random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    'LightGBM': LGBMClassifier(\n",
        "        n_estimators=500, max_depth=8, learning_rate=0.05,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    ),\n",
        "    'CatBoost': CatBoostClassifier(\n",
        "        iterations=500, depth=7, learning_rate=0.05,\n",
        "        verbose=0, random_state=42, thread_count=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ Comprehensive Training Loop\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ü§ñ TRAINING ALL VECTORIZER + MODEL COMBINATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_results = []\n",
        "experiment_counter = 0\n",
        "total_experiments = len(VECTORIZERS) * len(VECTORIZER_CONFIGS['ngram_ranges']) * len(CLASSIFIERS)\n",
        "\n",
        "print(f\"\\nüìä Total Experiments to Run: {total_experiments}\")\n",
        "print(f\"   - Vectorizers: {len(VECTORIZERS)}\")\n",
        "print(f\"   - N-gram ranges: {len(VECTORIZER_CONFIGS['ngram_ranges'])}\")\n",
        "print(f\"   - Models: {len(CLASSIFIERS)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Iterate over all combinations\n",
        "for vec_name, vec_type in VECTORIZERS.items():\n",
        "    for ngram_range in VECTORIZER_CONFIGS['ngram_ranges']:\n",
        "\n",
        "        # Create vectorizer name\n",
        "        ngram_str = f\"ngram_{ngram_range[0]}_{ngram_range[1]}\"\n",
        "        full_vec_name = f\"{vec_name}_{ngram_str}\"\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üìê VECTORIZER: {vec_name} | N-gram: {ngram_range}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Create and fit vectorizer\n",
        "        vectorizer = get_vectorizer(vec_type, ngram_range, VECTORIZER_CONFIGS['tfidf_max_features'][0])\n",
        "\n",
        "        # Transform data\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "        X_val_vec = vectorizer.transform(X_val)\n",
        "\n",
        "        # Log vectorizer info\n",
        "        vec_shape = X_train_vec.shape[1]\n",
        "        wandb.log({f\"{full_vec_name}/feature_dimension\": vec_shape})\n",
        "\n",
        "        print(f\"   ‚úÖ Features extracted: {vec_shape}\")\n",
        "\n",
        "        # Train all models with this vectorizer\n",
        "        for model_name, clf in CLASSIFIERS.items():\n",
        "            experiment_counter += 1\n",
        "\n",
        "            print(f\"\\n   [{experiment_counter}/{total_experiments}] üîÑ Training: {model_name}\")\n",
        "\n",
        "            # Train model\n",
        "            model = OneVsRestClassifier(clf, n_jobs=-1)\n",
        "            model.fit(X_train_vec, y_train)\n",
        "\n",
        "            # Predictions\n",
        "            y_pred = model.predict(X_val_vec)\n",
        "\n",
        "            # Get probabilities\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                y_pred_proba = model.predict_proba(X_val_vec)\n",
        "            elif hasattr(model, \"decision_function\"):\n",
        "                y_pred_proba = model.decision_function(X_val_vec)\n",
        "                # Normalize to [0, 1]\n",
        "                from sklearn.preprocessing import MinMaxScaler\n",
        "                scaler = MinMaxScaler()\n",
        "                y_pred_proba = scaler.fit_transform(y_pred_proba)\n",
        "            else:\n",
        "                y_pred_proba = y_pred\n",
        "\n",
        "            # Compute metrics\n",
        "            metrics = compute_advanced_metrics(y_val, y_pred, y_pred_proba, emotions)\n",
        "\n",
        "            # Log all metrics to W&B\n",
        "            for metric_name, value in metrics.items():\n",
        "                wandb.log({f\"{full_vec_name}/{model_name}/{metric_name}\": value})\n",
        "\n",
        "            # Classification report\n",
        "            class_report = classification_report(\n",
        "                y_val, y_pred, target_names=emotions,\n",
        "                output_dict=True, zero_division=0\n",
        "            )\n",
        "\n",
        "            # Generate visualizations (only for top 3 models per vectorizer to save time)\n",
        "            # We'll generate all visualizations for best overall at the end\n",
        "\n",
        "            # Confusion matrix for all\n",
        "            plot_combined_confusion_matrix(y_val, y_pred, emotions, model_name, full_vec_name)\n",
        "\n",
        "            # Store results\n",
        "            all_results.append({\n",
        "                'Vectorizer': vec_name,\n",
        "                'N-gram': str(ngram_range),\n",
        "                'Model': model_name,\n",
        "                'F1_Macro': metrics['f1_macro'],\n",
        "                'F1_Micro': metrics['f1_micro'],\n",
        "                'F1_Weighted': metrics['f1_weighted'],\n",
        "                'Hamming_Loss': metrics['hamming_loss'],\n",
        "                'Jaccard_Score': metrics['jaccard_score'],\n",
        "                'Subset_Accuracy': metrics['subset_accuracy'],\n",
        "                'Full_Name': full_vec_name\n",
        "            })\n",
        "\n",
        "            print(f\"      ‚úÖ F1-Macro: {metrics['f1_macro']:.4f} | Hamming: {metrics['hamming_loss']:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 5Ô∏è‚É£ Comprehensive Results Analysis\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà COMPREHENSIVE RESULTS ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results_df = pd.DataFrame(all_results).sort_values(by='F1_Macro', ascending=False)\n",
        "\n",
        "# Save full results\n",
        "results_path = OUTPUT_DIR / \"reports\" / \"full_experiment_results.csv\"\n",
        "results_df.to_csv(results_path, index=False)\n",
        "wandb.save(str(results_path))\n",
        "\n",
        "# Display top 10 results\n",
        "print(\"\\nüèÜ TOP 10 CONFIGURATIONS:\")\n",
        "print(results_df.head(10).to_string(index=False))\n",
        "\n",
        "# Best configuration\n",
        "best_config = results_df.iloc[0]\n",
        "print(f\"\\nü•á BEST CONFIGURATION:\")\n",
        "print(f\"   Vectorizer: {best_config['Vectorizer']}\")\n",
        "print(f\"   N-gram: {best_config['N-gram']}\")\n",
        "print(f\"   Model: {best_config['Model']}\")\n",
        "print(f\"   F1-Macro: {best_config['F1_Macro']:.4f}\")\n",
        "\n",
        "# Log best config\n",
        "wandb.log({\n",
        "    \"best/vectorizer\": best_config['Vectorizer'],\n",
        "    \"best/ngram\": best_config['N-gram'],\n",
        "    \"best/model\": best_config['Model'],\n",
        "    \"best/f1_macro\": best_config['F1_Macro']\n",
        "})\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Vectorizer Comparison\n",
        "# =========================\n",
        "print(\"\\nüìä VECTORIZER PERFORMANCE COMPARISON:\")\n",
        "vectorizer_perf = results_df.groupby('Vectorizer')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "print(vectorizer_perf)\n",
        "\n",
        "# Plot vectorizer comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "vec_comparison = results_df.groupby('Vectorizer')['F1_Macro'].mean().sort_values(ascending=False)\n",
        "bars = ax.bar(vec_comparison.index, vec_comparison.values,\n",
        "              color=plt.cm.viridis(np.linspace(0, 1, len(vec_comparison))))\n",
        "ax.set_xlabel('Vectorizer', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Mean F1-Macro Score', fontsize=14, fontweight='bold')\n",
        "ax.set_title('Vectorizer Performance Comparison (Averaged Across All Models)',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "vec_comp_path = OUTPUT_DIR / \"plots\" / \"vectorizer_comparison.png\"\n",
        "plt.savefig(vec_comp_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "wandb.log({\"comparison/vectorizer_performance\": wandb.Image(str(vec_comp_path))})\n",
        "plt.close()\n",
        "\n",
        "# =========================\n",
        "# 7Ô∏è‚É£ N-gram Range Comparison\n",
        "# =========================\n",
        "print(\"\\nüìä N-GRAM RANGE PERFORMANCE:\")\n",
        "ngram_perf = results_df.groupby('N-gram')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "print(ngram_perf)\n",
        "\n",
        "# Plot n-gram comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ngram_comparison = results_df.groupby('N-gram')['F1_Macro'].mean().sort_values(ascending=False)\n",
        "bars = ax.bar(ngram_comparison.index, ngram_comparison.values,\n",
        "              color=plt.cm.plasma(np.linspace(0, 1, len(ngram_comparison))))\n",
        "ax.set_xlabel('N-gram Range', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Mean F1-Macro Score', fontsize=14, fontweight='bold')\n",
        "ax.set_title('N-gram Range Performance Comparison', fontsize=16, fontweight='bold', pad=20)\n",
        "ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "ngram_comp_path = OUTPUT_DIR / \"plots\" / \"ngram_comparison.png\"\n",
        "plt.savefig(ngram_comp_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "wandb.log({\"comparison/ngram_performance\": wandb.Image(str(ngram_comp_path))})\n",
        "plt.close()\n",
        "\n",
        "# =========================\n",
        "# 8Ô∏è‚É£ Model Performance Across Vectorizers\n",
        "# =========================\n",
        "print(\"\\nüìä MODEL PERFORMANCE ACROSS VECTORIZERS:\")\n",
        "model_perf = results_df.groupby('Model')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "print(model_perf.sort_values('mean', ascending=False))\n",
        "\n",
        "# Heatmap: Models vs Vectorizers\n",
        "pivot_table = results_df.pivot_table(\n",
        "    values='F1_Macro',\n",
        "    index='Model',\n",
        "    columns='Vectorizer',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'F1-Macro Score'}, ax=ax,\n",
        "            linewidths=1, linecolor='white')\n",
        "ax.set_title('Model Performance Heatmap (Across Vectorizers)',\n",
        "             fontsize=18, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Vectorizer', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Model', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "heatmap_path = OUTPUT_DIR / \"plots\" / \"model_vectorizer_heatmap.png\"\n",
        "plt.savefig(heatmap_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "wandb.log({\"comparison/model_vectorizer_heatmap\": wandb.Image(str(heatmap_path))})\n",
        "plt.close()\n",
        "\n",
        "# =========================\n",
        "# 9Ô∏è‚É£ Generate Detailed Visualizations for Best Config\n",
        "# =========================\n",
        "print(f\"\\nüé® Generating detailed visualizations for best configuration...\")\n",
        "\n",
        "# Extract best configuration details\n",
        "best_vec_type = VECTORIZERS[best_config['Vectorizer']]\n",
        "best_ngram = eval(best_config['N-gram'])\n",
        "best_model_name = best_config['Model']\n",
        "best_full_name = best_config['Full_Name']\n",
        "\n",
        "# Recreate best vectorizer and model\n",
        "best_vectorizer = get_vectorizer(best_vec_type, best_ngram, VECTORIZER_CONFIGS['tfidf_max_features'][0])\n",
        "X_train_best = best_vectorizer.fit_transform(X_train)\n",
        "X_val_best = best_vectorizer.transform(X_val)\n",
        "\n",
        "# Train best model\n",
        "best_clf = CLASSIFIERS[best_model_name]\n",
        "best_model = OneVsRestClassifier(best_clf, n_jobs=-1)\n",
        "best_model.fit(X_train_best, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_best = best_model.predict(X_val_best)\n",
        "\n",
        "if hasattr(best_model, \"predict_proba\"):\n",
        "    y_pred_proba_best = best_model.predict_proba(X_val_best)\n",
        "elif hasattr(best_model, \"decision_function\"):\n",
        "    y_pred_proba_best = best_model.decision_function(X_val_best)\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    y_pred_proba_best = scaler.fit_transform(y_pred_proba_best)\n",
        "else:\n",
        "    y_pred_proba_best = y_pred_best\n",
        "\n",
        "# Generate all visualizations for best model\n",
        "class_report_best = classification_report(\n",
        "    y_val, y_pred_best, target_names=emotions,\n",
        "    output_dict=True, zero_division=0\n",
        ")\n",
        "\n",
        "plot_roc_curves(y_val, y_pred_proba_best, emotions, best_model_name, f\"BEST_{best_full_name}\")\n",
        "plot_classification_report(class_report_best, emotions, best_model_name, f\"BEST_{best_full_name}\")\n",
        "\n",
        "# =========================\n",
        "# üîü Retrain Best Model on Full Data\n",
        "# =========================\n",
        "print(f\"\\nüîÑ Retraining best model on full dataset...\")\n",
        "\n",
        "X_full_best = best_vectorizer.fit_transform(X)\n",
        "best_model_full = OneVsRestClassifier(best_clf, n_jobs=-1)\n",
        "best_model_full.fit(X_full_best, y)\n",
        "\n",
        "print(\"   ‚úÖ Best model retrained on full dataset\")\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Test Set Prediction\n",
        "# =========================\n",
        "print(\"\\nüìù Generating predictions on test set...\")\n",
        "clean_test['final_text'] = clean_test['final_text'].fillna('')\n",
        "X_test_best = best_vectorizer.transform(clean_test['final_text'])\n",
        "y_test_pred = best_model_full.predict(X_test_best)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame(y_test_pred, columns=y.columns)\n",
        "submission['id'] = clean_test['id']\n",
        "submission = submission[['id'] + list(y.columns)]\n",
        "\n",
        "submission_path = OUTPUT_DIR / \"submission.csv\"\n",
        "submission.to_csv(submission_path, index=False)\n",
        "wandb.save(str(submission_path))\n",
        "\n",
        "print(f\"   ‚úÖ Submission saved: {submission_path}\")\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Final Summary Report\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary = {\n",
        "    \"timestamp\": EXPERIMENT_CONFIG['timestamp'],\n",
        "    \"total_experiments\": total_experiments,\n",
        "    \"best_vectorizer\": best_config['Vectorizer'],\n",
        "    \"best_ngram\": best_config['N-gram'],\n",
        "    \"best_model\": best_config['Model'],\n",
        "    \"best_f1_macro\": float(best_config['F1_Macro']),\n",
        "    \"best_hamming_loss\": float(best_config['Hamming_Loss']),\n",
        "    \"best_jaccard_score\": float(best_config['Jaccard_Score']),\n",
        "    \"dataset_size\": len(X),\n",
        "    \"validation_size\": len(X_val),\n",
        "    \"num_emotions\": len(emotions),\n",
        "    \"emotions\": emotions,\n",
        "    \"vectorizers_tested\": list(VECTORIZERS.keys()),\n",
        "    \"ngram_ranges_tested\": VECTORIZER_CONFIGS['ngram_ranges'],\n",
        "    \"models_tested\": list(CLASSIFIERS.keys())\n",
        "}\n",
        "\n",
        "summary_path = OUTPUT_DIR / \"reports\" / \"experiment_summary.json\"\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=4)\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(f\"\\n‚úÖ Summary saved: {summary_path}\")\n",
        "\n",
        "wandb.log({\"experiment/summary\": summary})\n",
        "\n",
        "# Create final comparison table\n",
        "wandb.log({\"experiment/all_results\": wandb.Table(dataframe=results_df)})\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ Create Executive Summary Visualization\n",
        "# =========================\n",
        "print(\"\\nüìä Creating executive summary dashboard...\")\n",
        "\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Best Configuration Box\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.axis('off')\n",
        "summary_text = f\"\"\"\n",
        "BEST CONFIGURATION\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Vectorizer: {best_config['Vectorizer']}\n",
        "N-gram: {best_config['N-gram']}\n",
        "Model: {best_config['Model']}\n",
        "\n",
        "PERFORMANCE METRICS\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "F1-Macro: {best_config['F1_Macro']:.4f}\n",
        "F1-Micro: {best_config['F1_Micro']:.4f}\n",
        "F1-Weighted: {best_config['F1_Weighted']:.4f}\n",
        "Hamming Loss: {best_config['Hamming_Loss']:.4f}\n",
        "Jaccard Score: {best_config['Jaccard_Score']:.4f}\n",
        "Subset Accuracy: {best_config['Subset_Accuracy']:.4f}\n",
        "\"\"\"\n",
        "ax1.text(0.1, 0.5, summary_text, fontsize=11, fontfamily='monospace',\n",
        "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "# 2. Top 5 Configurations\n",
        "ax2 = fig.add_subplot(gs[0, 1:])\n",
        "top5 = results_df.head(5)\n",
        "ax2.axis('tight')\n",
        "ax2.axis('off')\n",
        "table_data = []\n",
        "for idx, row in top5.iterrows():\n",
        "    table_data.append([\n",
        "        row['Vectorizer'],\n",
        "        row['N-gram'],\n",
        "        row['Model'],\n",
        "        f\"{row['F1_Macro']:.4f}\"\n",
        "    ])\n",
        "table = ax2.table(cellText=table_data,\n",
        "                  colLabels=['Vectorizer', 'N-gram', 'Model', 'F1-Macro'],\n",
        "                  cellLoc='center',\n",
        "                  loc='center',\n",
        "                  colWidths=[0.25, 0.2, 0.3, 0.15])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "for i in range(len(top5) + 1):\n",
        "    if i == 0:\n",
        "        table[(i, 0)].set_facecolor('#4CAF50')\n",
        "        table[(i, 1)].set_facecolor('#4CAF50')\n",
        "        table[(i, 2)].set_facecolor('#4CAF50')\n",
        "        table[(i, 3)].set_facecolor('#4CAF50')\n",
        "    else:\n",
        "        table[(i, 0)].set_facecolor('#E8F5E9')\n",
        "        table[(i, 1)].set_facecolor('#E8F5E9')\n",
        "        table[(i, 2)].set_facecolor('#E8F5E9')\n",
        "        table[(i, 3)].set_facecolor('#E8F5E9')\n",
        "ax2.set_title('TOP 5 CONFIGURATIONS', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# 3. Vectorizer Performance\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "vec_means = results_df.groupby('Vectorizer')['F1_Macro'].mean().sort_values(ascending=True)\n",
        "ax3.barh(vec_means.index, vec_means.values, color=plt.cm.viridis(np.linspace(0, 1, len(vec_means))))\n",
        "ax3.set_xlabel('Mean F1-Macro', fontsize=11, fontweight='bold')\n",
        "ax3.set_title('Vectorizer Performance', fontsize=12, fontweight='bold')\n",
        "ax3.grid(axis='x', alpha=0.3)\n",
        "for i, v in enumerate(vec_means.values):\n",
        "    ax3.text(v, i, f' {v:.4f}', va='center', fontweight='bold')\n",
        "\n",
        "# 4. N-gram Performance\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "ngram_means = results_df.groupby('N-gram')['F1_Macro'].mean().sort_values(ascending=True)\n",
        "ax4.barh(ngram_means.index, ngram_means.values, color=plt.cm.plasma(np.linspace(0, 1, len(ngram_means))))\n",
        "ax4.set_xlabel('Mean F1-Macro', fontsize=11, fontweight='bold')\n",
        "ax4.set_title('N-gram Range Performance', fontsize=12, fontweight='bold')\n",
        "ax4.grid(axis='x', alpha=0.3)\n",
        "for i, v in enumerate(ngram_means.values):\n",
        "    ax4.text(v, i, f' {v:.4f}', va='center', fontweight='bold')\n",
        "\n",
        "# 5. Model Performance\n",
        "ax5 = fig.add_subplot(gs[1, 2])\n",
        "model_means = results_df.groupby('Model')['F1_Macro'].mean().sort_values(ascending=False).head(8)\n",
        "ax5.bar(range(len(model_means)), model_means.values, color=plt.cm.Set3(np.linspace(0, 1, len(model_means))))\n",
        "ax5.set_xticks(range(len(model_means)))\n",
        "ax5.set_xticklabels(model_means.index, rotation=45, ha='right', fontsize=9)\n",
        "ax5.set_ylabel('Mean F1-Macro', fontsize=11, fontweight='bold')\n",
        "ax5.set_title('Model Performance', fontsize=12, fontweight='bold')\n",
        "ax5.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(model_means.values):\n",
        "    ax5.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "# 6. F1-Macro Distribution\n",
        "ax6 = fig.add_subplot(gs[2, 0])\n",
        "ax6.hist(results_df['F1_Macro'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "ax6.axvline(best_config['F1_Macro'], color='red', linestyle='--', linewidth=2, label=f\"Best: {best_config['F1_Macro']:.4f}\")\n",
        "ax6.set_xlabel('F1-Macro Score', fontsize=11, fontweight='bold')\n",
        "ax6.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "ax6.set_title('F1-Macro Distribution', fontsize=12, fontweight='bold')\n",
        "ax6.legend()\n",
        "ax6.grid(alpha=0.3)\n",
        "\n",
        "# 7. Hamming Loss Distribution\n",
        "ax7 = fig.add_subplot(gs[2, 1])\n",
        "ax7.hist(results_df['Hamming_Loss'], bins=30, color='salmon', edgecolor='black', alpha=0.7)\n",
        "ax7.axvline(best_config['Hamming_Loss'], color='green', linestyle='--', linewidth=2, label=f\"Best: {best_config['Hamming_Loss']:.4f}\")\n",
        "ax7.set_xlabel('Hamming Loss', fontsize=11, fontweight='bold')\n",
        "ax7.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "ax7.set_title('Hamming Loss Distribution', fontsize=12, fontweight='bold')\n",
        "ax7.legend()\n",
        "ax7.grid(alpha=0.3)\n",
        "\n",
        "# 8. Experiment Statistics\n",
        "ax8 = fig.add_subplot(gs[2, 2])\n",
        "ax8.axis('off')\n",
        "stats_text = f\"\"\"\n",
        "EXPERIMENT STATISTICS\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Total Experiments: {total_experiments}\n",
        "Vectorizers: {len(VECTORIZERS)}\n",
        "N-gram Ranges: {len(VECTORIZER_CONFIGS['ngram_ranges'])}\n",
        "Models: {len(CLASSIFIERS)}\n",
        "\n",
        "BEST SCORES\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Highest F1-Macro: {results_df['F1_Macro'].max():.4f}\n",
        "Lowest Hamming Loss: {results_df['Hamming_Loss'].min():.4f}\n",
        "Highest Jaccard: {results_df['Jaccard_Score'].max():.4f}\n",
        "\n",
        "AVERAGE SCORES\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Mean F1-Macro: {results_df['F1_Macro'].mean():.4f}\n",
        "Std F1-Macro: {results_df['F1_Macro'].std():.4f}\n",
        "\"\"\"\n",
        "ax8.text(0.1, 0.5, stats_text, fontsize=10, fontfamily='monospace',\n",
        "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
        "\n",
        "plt.suptitle('üöÄ EXPERIMENT EXECUTIVE SUMMARY DASHBOARD', fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "dashboard_path = OUTPUT_DIR / \"plots\" / \"executive_summary_dashboard.png\"\n",
        "plt.savefig(dashboard_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "wandb.log({\"summary/executive_dashboard\": wandb.Image(str(dashboard_path))})\n",
        "plt.close()\n",
        "\n",
        "print(f\"   ‚úÖ Executive dashboard created: {dashboard_path}\")\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ Performance Comparison by Vectorizer and Model\n",
        "# =========================\n",
        "print(\"\\nüìä Creating detailed performance comparison charts...\")\n",
        "\n",
        "# Create a comprehensive comparison figure\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "# Plot 1: F1-Macro by Vectorizer and Model\n",
        "ax1 = axes[0, 0]\n",
        "for vectorizer in results_df['Vectorizer'].unique():\n",
        "    data = results_df[results_df['Vectorizer'] == vectorizer].groupby('Model')['F1_Macro'].mean()\n",
        "    ax1.plot(data.index, data.values, marker='o', linewidth=2, markersize=8, label=vectorizer)\n",
        "ax1.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Mean F1-Macro', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('F1-Macro: Models vs Vectorizers', fontsize=14, fontweight='bold')\n",
        "ax1.legend(title='Vectorizer', fontsize=10)\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 2: Hamming Loss by Vectorizer and Model\n",
        "ax2 = axes[0, 1]\n",
        "for vectorizer in results_df['Vectorizer'].unique():\n",
        "    data = results_df[results_df['Vectorizer'] == vectorizer].groupby('Model')['Hamming_Loss'].mean()\n",
        "    ax2.plot(data.index, data.values, marker='s', linewidth=2, markersize=8, label=vectorizer)\n",
        "ax2.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Mean Hamming Loss', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Hamming Loss: Models vs Vectorizers', fontsize=14, fontweight='bold')\n",
        "ax2.legend(title='Vectorizer', fontsize=10)\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 3: Performance by N-gram\n",
        "ax3 = axes[1, 0]\n",
        "ngram_model_perf = results_df.groupby(['N-gram', 'Model'])['F1_Macro'].mean().unstack()\n",
        "ngram_model_perf.plot(kind='bar', ax=ax3, width=0.8, colormap='viridis')\n",
        "ax3.set_xlabel('N-gram Range', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Mean F1-Macro', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('Performance by N-gram Range', fontsize=14, fontweight='bold')\n",
        "ax3.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "ax3.tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Plot 4: Box plot of F1-Macro by Vectorizer\n",
        "ax4 = axes[1, 1]\n",
        "results_df.boxplot(column='F1_Macro', by='Vectorizer', ax=ax4, patch_artist=True)\n",
        "ax4.set_xlabel('Vectorizer', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('F1-Macro Distribution', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('F1-Macro Distribution by Vectorizer', fontsize=14, fontweight='bold')\n",
        "plt.sca(ax4)\n",
        "plt.xticks(rotation=0)\n",
        "ax4.get_figure().suptitle('')  # Remove default title\n",
        "\n",
        "plt.tight_layout()\n",
        "comparison_path = OUTPUT_DIR / \"plots\" / \"detailed_performance_comparison.png\"\n",
        "plt.savefig(comparison_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "wandb.log({\"comparison/detailed_performance\": wandb.Image(str(comparison_path))})\n",
        "plt.close()\n",
        "\n",
        "print(f\"   ‚úÖ Detailed comparison charts created\")\n",
        "\n",
        "# =========================\n",
        "# üéØ Finish Experiment\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ COMPREHENSIVE EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nüìä FINAL RESULTS:\")\n",
        "print(f\"   ‚Ä¢ Total Experiments Run: {total_experiments}\")\n",
        "print(f\"   ‚Ä¢ Best Vectorizer: {best_config['Vectorizer']}\")\n",
        "print(f\"   ‚Ä¢ Best N-gram: {best_config['N-gram']}\")\n",
        "print(f\"   ‚Ä¢ Best Model: {best_config['Model']}\")\n",
        "print(f\"   ‚Ä¢ Best F1-Macro: {best_config['F1_Macro']:.4f}\")\n",
        "print(f\"\\nüìÅ All outputs saved in: {OUTPUT_DIR}\")\n",
        "print(f\"üîó View comprehensive results in W&B: {wandb.run.get_url()}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lq9vbqxFzlpM",
        "outputId": "fd9f51dc-d21f-4301-a7ad-453efc3bcbf4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maiwithajay\u001b[0m (\u001b[33maiwithajay-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251010_070211-720dp750</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/720dp750' target=\"_blank\">D02-multi-vectorizer-classification-20251010_070017</a></strong> to <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/720dp750' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/720dp750</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ STARTING COMPREHENSIVE VECTORIZER + MODEL EXPERIMENT\n",
            "================================================================================\n",
            "\n",
            "üìä Analyzing Label Distribution...\n",
            "‚úÖ Train set size: 3992 | Validation set size: 999\n",
            "\n",
            "================================================================================\n",
            "ü§ñ TRAINING ALL VECTORIZER + MODEL COMBINATIONS\n",
            "================================================================================\n",
            "\n",
            "üìä Total Experiments to Run: 45\n",
            "   - Vectorizers: 3\n",
            "   - N-gram ranges: 3\n",
            "   - Models: 5\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: TfidfVectorizer | N-gram: (1, 1)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [1/45] üîÑ Training: RandomForest\n",
            "      ‚úÖ F1-Macro: 0.4364 | Hamming: 0.2392\n",
            "\n",
            "   [2/45] üîÑ Training: LinearSVC\n",
            "      ‚úÖ F1-Macro: 0.4784 | Hamming: 0.2436\n",
            "\n",
            "   [3/45] üîÑ Training: XGBoost\n",
            "      ‚úÖ F1-Macro: 0.3881 | Hamming: 0.2364\n",
            "\n",
            "   [4/45] üîÑ Training: LightGBM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ F1-Macro: 0.3615 | Hamming: 0.2599\n",
            "\n",
            "   [5/45] üîÑ Training: CatBoost\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ F1-Macro: 0.4036 | Hamming: 0.2366\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: TfidfVectorizer | N-gram: (1, 2)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [6/45] üîÑ Training: RandomForest\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ F1-Macro: 0.4426 | Hamming: 0.2384\n",
            "\n",
            "   [7/45] üîÑ Training: LinearSVC\n",
            "      ‚úÖ F1-Macro: 0.4697 | Hamming: 0.2460\n",
            "\n",
            "   [8/45] üîÑ Training: XGBoost\n",
            "      ‚úÖ F1-Macro: 0.3938 | Hamming: 0.2410\n",
            "\n",
            "   [9/45] üîÑ Training: LightGBM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ F1-Macro: 0.3721 | Hamming: 0.2555\n",
            "\n",
            "   [10/45] üîÑ Training: CatBoost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      ‚úÖ F1-Macro: 0.4097 | Hamming: 0.2360\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: TfidfVectorizer | N-gram: (1, 3)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [11/45] üîÑ Training: RandomForest\n",
            "      ‚úÖ F1-Macro: 0.4580 | Hamming: 0.2362\n",
            "\n",
            "   [12/45] üîÑ Training: LinearSVC\n",
            "      ‚úÖ F1-Macro: 0.4614 | Hamming: 0.2496\n",
            "\n",
            "   [13/45] üîÑ Training: XGBoost\n",
            "      ‚úÖ F1-Macro: 0.4030 | Hamming: 0.2388\n",
            "\n",
            "   [14/45] üîÑ Training: LightGBM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      ‚úÖ F1-Macro: 0.3751 | Hamming: 0.2545\n",
            "\n",
            "   [15/45] üîÑ Training: CatBoost\n",
            "      ‚úÖ F1-Macro: 0.4293 | Hamming: 0.2318\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: CountVectorizer | N-gram: (1, 1)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [16/45] üîÑ Training: RandomForest\n",
            "      ‚úÖ F1-Macro: 0.4768 | Hamming: 0.2428\n",
            "\n",
            "   [17/45] üîÑ Training: LinearSVC\n",
            "      ‚úÖ F1-Macro: 0.4835 | Hamming: 0.2611\n",
            "\n",
            "   [18/45] üîÑ Training: XGBoost\n",
            "      ‚úÖ F1-Macro: 0.3987 | Hamming: 0.2344\n",
            "\n",
            "   [19/45] üîÑ Training: LightGBM\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Expected np.float32 or np.float64, met type(int64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py\", line 96, in _fit_binary\n    estimator.fit(X, y, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/sklearn.py\", line 1560, in fit\n    super().fit(\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/sklearn.py\", line 1049, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py\", line 297, in train\n    booster = Booster(params=params, train_set=train_set)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\", line 3656, in __init__\n    train_set.construct()\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\", line 2590, in construct\n    self._lazy_init(\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\", line 2183, in _lazy_init\n    self.__init_from_csr(data, params_str, ref_dataset)\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\", line 2398, in __init_from_csr\n    ptr_data, type_ptr_data, _ = _c_float_array(csr.data)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\", line 766, in _c_float_array\n    raise TypeError(f\"Expected np.float32 or np.float64, met type({data.dtype})\")\nTypeError: Expected np.float32 or np.float64, met type(int64)\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2028446353.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# of spawning threads.  See joblib issue #112.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n\u001b[0m\u001b[1;32m    377\u001b[0m             delayed(_fit_binary)(\n\u001b[1;32m    378\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected np.float32 or np.float64, met type(int64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "üöÄ Industry-Grade Multi-Label Text Classification Pipeline\n",
        "Features: Advanced Experiment Tracking with Multiple Vectorizers\n",
        "Including: TF-IDF, Count, Hashing, Word2Vec, GloVe, FastText\n",
        "Comprehensive Metrics, Production-Ready Visualizations\n",
        "WITH CHECKPOINT SUPPORT AND ERROR RECOVERY\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wandb\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "from itertools import product\n",
        "import pickle\n",
        "import os\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score, classification_report, confusion_matrix,\n",
        "    roc_curve, auc, roc_auc_score, precision_recall_curve,\n",
        "    average_precision_score, hamming_loss, jaccard_score,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from scipy import sparse\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# =========================\n",
        "# üé® Configuration & Setup\n",
        "# =========================\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Set style for professional visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"plots\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"reports\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"checkpoints\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"embeddings\").mkdir(exist_ok=True)\n",
        "\n",
        "# Checkpoint file path\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoints\" / \"experiment_checkpoint.pkl\"\n",
        "RESULTS_CHECKPOINT = OUTPUT_DIR / \"checkpoints\" / \"results_checkpoint.csv\"\n",
        "\n",
        "# Experiment configuration\n",
        "EXPERIMENT_CONFIG = {\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42,\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "    \"embedding_dim\": 100,\n",
        "    \"word2vec_min_count\": 2,\n",
        "    \"word2vec_window\": 5,\n",
        "    \"word2vec_workers\": 4\n",
        "}\n",
        "\n",
        "# Vectorizer configurations\n",
        "VECTORIZER_CONFIGS = {\n",
        "    'tfidf_max_features': [5000],\n",
        "    'ngram_ranges': [(1, 1), (1, 2), (1, 3)]\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# üîß Checkpoint Functions\n",
        "# =========================\n",
        "\n",
        "def save_checkpoint(experiment_state):\n",
        "    \"\"\"Save experiment checkpoint\"\"\"\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'wb') as f:\n",
        "            pickle.dump(experiment_state, f)\n",
        "        print(f\"   üíæ Checkpoint saved: {CHECKPOINT_FILE}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Warning: Could not save checkpoint: {e}\")\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load experiment checkpoint if exists\"\"\"\n",
        "    if CHECKPOINT_FILE.exists():\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'rb') as f:\n",
        "                state = pickle.load(f)\n",
        "            print(f\"   ‚úÖ Checkpoint loaded: {CHECKPOINT_FILE}\")\n",
        "            return state\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Warning: Could not load checkpoint: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def save_results_checkpoint(results_df):\n",
        "    \"\"\"Save results as CSV checkpoint\"\"\"\n",
        "    try:\n",
        "        results_df.to_csv(RESULTS_CHECKPOINT, index=False)\n",
        "        print(f\"   üíæ Results checkpoint saved: {RESULTS_CHECKPOINT}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Warning: Could not save results checkpoint: {e}\")\n",
        "\n",
        "def load_results_checkpoint():\n",
        "    \"\"\"Load results checkpoint if exists\"\"\"\n",
        "    if RESULTS_CHECKPOINT.exists():\n",
        "        try:\n",
        "            results_df = pd.read_csv(RESULTS_CHECKPOINT)\n",
        "            print(f\"   ‚úÖ Results checkpoint loaded: {len(results_df)} experiments\")\n",
        "            return results_df\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Warning: Could not load results checkpoint: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# Initialize W&B with rich config\n",
        "wandb.init(\n",
        "    project=\"23f3003030-t32025\",\n",
        "    name=f\"D02-multi-vectorizer-classification-20251010_070017\",\n",
        "    config=EXPERIMENT_CONFIG,\n",
        "    tags=[\"multi-label\", \"emotion-detection\", \"vectorizer-comparison\", \"embeddings\", \"production\"],\n",
        "    notes=\"Comprehensive experiment with TF-IDF, Count, Hashing, Word2Vec, GloVe, FastText\",\n",
        "    resume=\"allow\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# üåê Embedding Helper Functions\n",
        "# =========================\n",
        "\n",
        "class EmbeddingVectorizer:\n",
        "    \"\"\"Custom vectorizer for word embeddings (Word2Vec, GloVe, FastText)\"\"\"\n",
        "\n",
        "    def __init__(self, model, embedding_type='word2vec'):\n",
        "        self.model = model\n",
        "        self.embedding_type = embedding_type\n",
        "        self.dim = model.vector_size if hasattr(model, 'vector_size') else len(model['the'])\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text\"\"\"\n",
        "        try:\n",
        "            return word_tokenize(str(text).lower())\n",
        "        except:\n",
        "            return str(text).lower().split()\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        \"\"\"Get vector for a word\"\"\"\n",
        "        try:\n",
        "            if self.embedding_type in ['word2vec', 'fasttext']:\n",
        "                return self.model.wv[word]\n",
        "            else:  # glove\n",
        "                return self.model[word]\n",
        "        except:\n",
        "            return np.zeros(self.dim)\n",
        "\n",
        "    def transform_text(self, text):\n",
        "        \"\"\"Transform single text to average embedding vector\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        vectors = [self.get_vector(word) for word in tokens]\n",
        "        valid_vectors = [v for v in vectors if np.any(v)]\n",
        "\n",
        "        if valid_vectors:\n",
        "            return np.mean(valid_vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(self.dim)\n",
        "\n",
        "    def fit_transform(self, texts):\n",
        "        \"\"\"Fit and transform texts\"\"\"\n",
        "        return self.transform(texts)\n",
        "\n",
        "    def transform(self, texts):\n",
        "        \"\"\"Transform texts to embedding matrix\"\"\"\n",
        "        embeddings = np.array([self.transform_text(text) for text in texts])\n",
        "        return embeddings.astype(np.float32)\n",
        "\n",
        "\n",
        "def load_glove_embeddings(dim=100):\n",
        "    \"\"\"Load pretrained GloVe embeddings\"\"\"\n",
        "    print(f\"\\nüîÑ Loading GloVe embeddings (dim={dim})...\")\n",
        "\n",
        "    glove_path = OUTPUT_DIR / \"embeddings\" / f\"glove_{dim}d.pkl\"\n",
        "\n",
        "    if glove_path.exists():\n",
        "        print(\"   ‚úÖ Loading cached GloVe model...\")\n",
        "        with open(glove_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    try:\n",
        "        # Download from gensim\n",
        "        model_name = f'glove-twitter-{dim}'\n",
        "        print(f\"   üåê Downloading {model_name} from gensim...\")\n",
        "        glove_model = api.load(model_name)\n",
        "\n",
        "        # Save for future use\n",
        "        with open(glove_path, 'wb') as f:\n",
        "            pickle.dump(glove_model, f)\n",
        "        print(f\"   ‚úÖ GloVe model saved to cache\")\n",
        "\n",
        "        return glove_model\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error loading GloVe: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_word2vec(texts, **params):\n",
        "    \"\"\"Train Word2Vec model\"\"\"\n",
        "    print(f\"\\nüîÑ Training Word2Vec model...\")\n",
        "\n",
        "    # Tokenize all texts\n",
        "    tokenized_texts = [word_tokenize(str(text).lower()) for text in texts]\n",
        "\n",
        "    # Train model\n",
        "    model = Word2Vec(\n",
        "        sentences=tokenized_texts,\n",
        "        vector_size=params.get('embedding_dim', 100),\n",
        "        window=params.get('window', 5),\n",
        "        min_count=params.get('min_count', 2),\n",
        "        workers=params.get('workers', 4),\n",
        "        epochs=10,\n",
        "        seed=params.get('random_state', 42)\n",
        "    )\n",
        "\n",
        "    print(f\"   ‚úÖ Word2Vec trained: vocab_size={len(model.wv)}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_fasttext(texts, **params):\n",
        "    \"\"\"Train FastText model\"\"\"\n",
        "    print(f\"\\nüîÑ Training FastText model...\")\n",
        "\n",
        "    # Tokenize all texts\n",
        "    tokenized_texts = [word_tokenize(str(text).lower()) for text in texts]\n",
        "\n",
        "    # Train model\n",
        "    model = FastText(\n",
        "        sentences=tokenized_texts,\n",
        "        vector_size=params.get('embedding_dim', 100),\n",
        "        window=params.get('window', 5),\n",
        "        min_count=params.get('min_count', 2),\n",
        "        workers=params.get('workers', 4),\n",
        "        epochs=10,\n",
        "        seed=params.get('random_state', 42)\n",
        "    )\n",
        "\n",
        "    print(f\"   ‚úÖ FastText trained: vocab_size={len(model.wv)}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# =========================\n",
        "# üìä Advanced Visualization Functions\n",
        "# =========================\n",
        "\n",
        "def plot_combined_confusion_matrix(y_true, y_pred, emotions, model_name, vec_name):\n",
        "    \"\"\"Create professional confusion matrix visualization\"\"\"\n",
        "    n_emotions = len(emotions)\n",
        "    fig, axes = plt.subplots(1, n_emotions, figsize=(4*n_emotions, 3.5))\n",
        "\n",
        "    if n_emotions == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        cm = confusion_matrix(y_true.iloc[:, i], y_pred[:, i])\n",
        "\n",
        "        # Calculate percentages\n",
        "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "        # Create annotations with counts and percentages\n",
        "        annot = np.array([[f'{count}\\n({percent:.1f}%)'\n",
        "                          for count, percent in zip(row_counts, row_percents)]\n",
        "                         for row_counts, row_percents in zip(cm, cm_percent)])\n",
        "\n",
        "        sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', ax=axes[i],\n",
        "                    cbar=False, square=True, linewidths=1, linecolor='gray')\n",
        "        axes[i].set_title(f'{emotion.upper()}', fontsize=12, fontweight='bold', pad=10)\n",
        "        axes[i].set_xlabel('Predicted', fontsize=10)\n",
        "        axes[i].set_ylabel('Actual' if i == 0 else '', fontsize=10)\n",
        "        axes[i].set_xticklabels(['No', 'Yes'])\n",
        "        axes[i].set_yticklabels(['No', 'Yes'])\n",
        "\n",
        "    fig.suptitle(f'{model_name} ({vec_name}) - Confusion Matrices',\n",
        "                 fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{vec_name}_{model_name}_confusion_matrix.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{vec_name}/{model_name}/confusion_matrix\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_label_distribution(y_data, title, filename):\n",
        "    \"\"\"Plot label distribution\"\"\"\n",
        "    label_counts = y_data.sum().sort_values(ascending=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.bar(range(len(label_counts)), label_counts.values,\n",
        "                  color=plt.cm.viridis(np.linspace(0, 1, len(label_counts))))\n",
        "    ax.set_xticks(range(len(label_counts)))\n",
        "    ax.set_xticklabels([label.capitalize() for label in label_counts.index], fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = OUTPUT_DIR / \"plots\" / filename\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"data_analysis/{filename.replace('.png', '')}\": wandb.Image(str(save_path))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compute_advanced_metrics(y_true, y_pred, y_pred_proba, emotions):\n",
        "    \"\"\"Compute comprehensive metrics\"\"\"\n",
        "    metrics = {\n",
        "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
        "        'jaccard_score': jaccard_score(y_true, y_pred, average='samples', zero_division=0),\n",
        "        'subset_accuracy': accuracy_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "    # Per-emotion metrics\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        metrics[f'{emotion}_f1'] = f1_score(y_true.iloc[:, i], y_pred[:, i], zero_division=0)\n",
        "        try:\n",
        "            metrics[f'{emotion}_auc'] = roc_auc_score(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        except:\n",
        "            metrics[f'{emotion}_auc'] = 0.0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_vectorizer(vec_type, ngram_range, max_features, embedding_models=None):\n",
        "    \"\"\"Factory function to create vectorizers\"\"\"\n",
        "\n",
        "    if vec_type == 'count':\n",
        "        return CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "    elif vec_type == 'hashing':\n",
        "        return HashingVectorizer(n_features=max_features, ngram_range=ngram_range)\n",
        "    elif vec_type == 'word2vec' and embedding_models and 'word2vec' in embedding_models:\n",
        "        return EmbeddingVectorizer(embedding_models['word2vec'], 'word2vec')\n",
        "    elif vec_type == 'glove' and embedding_models and 'glove' in embedding_models:\n",
        "        return EmbeddingVectorizer(embedding_models['glove'], 'glove')\n",
        "    elif vec_type == 'fasttext' and embedding_models and 'fasttext' in embedding_models:\n",
        "        return EmbeddingVectorizer(embedding_models['fasttext'], 'fasttext')\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown vectorizer type: {vec_type}\")\n",
        "\n",
        "\n",
        "def convert_to_float32(X):\n",
        "    \"\"\"Convert sparse matrix to float32 (required for LightGBM)\"\"\"\n",
        "    if sparse.issparse(X):\n",
        "        return X.astype(np.float32)\n",
        "    else:\n",
        "        return X.astype(np.float32)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£ Data Preparation & EDA\n",
        "# =========================\n",
        "print(\"=\" * 80)\n",
        "print(\"üöÄ STARTING COMPREHENSIVE VECTORIZER + MODEL EXPERIMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "X = final_df['final_text'].fillna('')\n",
        "y = final_df[['anger', 'fear', 'joy', 'sadness', 'surprise']]\n",
        "emotions = y.columns.tolist()\n",
        "\n",
        "# Log dataset info\n",
        "wandb.log({\n",
        "    \"dataset/total_samples\": len(X),\n",
        "    \"dataset/num_emotions\": len(emotions),\n",
        "    \"dataset/feature_name\": \"final_text\"\n",
        "})\n",
        "\n",
        "# Analyze and log label distribution\n",
        "print(\"\\nüìä Analyzing Label Distribution...\")\n",
        "plot_label_distribution(y, \"Training Data - Emotion Distribution\", \"train_label_distribution.png\")\n",
        "\n",
        "# Log label statistics\n",
        "label_stats = {}\n",
        "for emotion in emotions:\n",
        "    label_stats[f\"dataset/{emotion}_count\"] = int(y[emotion].sum())\n",
        "    label_stats[f\"dataset/{emotion}_percentage\"] = float(y[emotion].sum() / len(y) * 100)\n",
        "\n",
        "wandb.log(label_stats)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=EXPERIMENT_CONFIG['test_size'],\n",
        "    random_state=EXPERIMENT_CONFIG['random_state']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train set size: {len(X_train)} | Validation set size: {len(X_val)}\")\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ Prepare Embedding Models\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üåê PREPARING EMBEDDING MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "embedding_models = {}\n",
        "\n",
        "# Train Word2Vec\n",
        "try:\n",
        "    word2vec_model = train_word2vec(\n",
        "        X_train,\n",
        "        embedding_dim=EXPERIMENT_CONFIG['embedding_dim'],\n",
        "        window=EXPERIMENT_CONFIG['word2vec_window'],\n",
        "        min_count=EXPERIMENT_CONFIG['word2vec_min_count'],\n",
        "        workers=EXPERIMENT_CONFIG['word2vec_workers'],\n",
        "        random_state=EXPERIMENT_CONFIG['random_state']\n",
        "    )\n",
        "    embedding_models['word2vec'] = word2vec_model\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error training Word2Vec: {e}\")\n",
        "\n",
        "# Load GloVe\n",
        "try:\n",
        "    glove_model = load_glove_embeddings(dim=EXPERIMENT_CONFIG['embedding_dim'])\n",
        "    if glove_model:\n",
        "        embedding_models['glove'] = glove_model\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error loading GloVe: {e}\")\n",
        "\n",
        "# Train FastText\n",
        "try:\n",
        "    fasttext_model = train_fasttext(\n",
        "        X_train,\n",
        "        embedding_dim=EXPERIMENT_CONFIG['embedding_dim'],\n",
        "        window=EXPERIMENT_CONFIG['word2vec_window'],\n",
        "        min_count=EXPERIMENT_CONFIG['word2vec_min_count'],\n",
        "        workers=EXPERIMENT_CONFIG['word2vec_workers'],\n",
        "        random_state=EXPERIMENT_CONFIG['random_state']\n",
        "    )\n",
        "    embedding_models['fasttext'] = fasttext_model\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error training FastText: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Embedding models ready: {list(embedding_models.keys())}\")\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ Define All Vectorizers\n",
        "# =========================\n",
        "VECTORIZERS = {\n",
        "    'CountVectorizer': 'count',\n",
        "    'HashingVectorizer': 'hashing',\n",
        "}\n",
        "\n",
        "# Add embedding vectorizers if available\n",
        "if 'word2vec' in embedding_models:\n",
        "    VECTORIZERS['Word2Vec'] = 'word2vec'\n",
        "if 'glove' in embedding_models:\n",
        "    VECTORIZERS['GloVe'] = 'glove'\n",
        "if 'fasttext' in embedding_models:\n",
        "    VECTORIZERS['FastText'] = 'fasttext'\n",
        "\n",
        "print(f\"\\nüìê Total Vectorizers Available: {len(VECTORIZERS)}\")\n",
        "for vec_name, vec_type in VECTORIZERS.items():\n",
        "    print(f\"   ‚Ä¢ {vec_name} ({vec_type})\")\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ Model Configuration\n",
        "# =========================\n",
        "CLASSIFIERS = {\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "    'LinearSVC': LinearSVC(max_iter=5000, random_state=42),\n",
        "    'XGBoost': XGBClassifier(\n",
        "        eval_metric='logloss', n_estimators=500, max_depth=6,\n",
        "        learning_rate=0.05, random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    'LightGBM': LGBMClassifier(\n",
        "        n_estimators=500, max_depth=8, learning_rate=0.05,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    ),\n",
        "    'CatBoost': CatBoostClassifier(\n",
        "        iterations=500, depth=7, learning_rate=0.05,\n",
        "        verbose=0, random_state=42, thread_count=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 5Ô∏è‚É£ Load or Initialize Checkpoint\n",
        "# =========================\n",
        "checkpoint_state = load_checkpoint()\n",
        "all_results_df = load_results_checkpoint()\n",
        "\n",
        "if checkpoint_state:\n",
        "    print(\"\\nüîÑ RESUMING FROM CHECKPOINT\")\n",
        "    print(f\"   ‚úÖ Completed experiments: {len(all_results_df)}\")\n",
        "    completed_experiments = set(\n",
        "        (row['Vectorizer'], row['N-gram'], row['Model'])\n",
        "        for _, row in all_results_df.iterrows()\n",
        "    )\n",
        "    experiment_counter = len(all_results_df)\n",
        "else:\n",
        "    print(\"\\nüÜï STARTING NEW EXPERIMENT\")\n",
        "    completed_experiments = set()\n",
        "    experiment_counter = 0\n",
        "    all_results_df = pd.DataFrame()\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Comprehensive Training Loop\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ü§ñ TRAINING ALL VECTORIZER + MODEL COMBINATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# Calculate total experiments\n",
        "# For traditional vectorizers: use n-gram ranges\n",
        "# For embedding vectorizers: no n-gram (just one config per embedding)\n",
        "traditional_vectorizers = ['tfidf', 'count', 'hashing']\n",
        "embedding_vectorizers = ['word2vec', 'glove', 'fasttext']\n",
        "\n",
        "traditional_count = sum(1 for v in VECTORIZERS.values() if v in traditional_vectorizers)\n",
        "embedding_count = sum(1 for v in VECTORIZERS.values() if v in embedding_vectorizers)\n",
        "\n",
        "total_experiments = (\n",
        "    traditional_count * len(VECTORIZER_CONFIGS['ngram_ranges']) * len(CLASSIFIERS) +\n",
        "    embedding_count * len(CLASSIFIERS)\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Total Experiments to Run: {total_experiments}\")\n",
        "print(f\"   - Already Completed: {len(completed_experiments)}\")\n",
        "print(f\"   - Remaining: {total_experiments - len(completed_experiments)}\")\n",
        "print(f\"   - Vectorizers: {len(VECTORIZERS)}\")\n",
        "print(f\"   - Traditional (with n-grams): {traditional_count}\")\n",
        "print(f\"   - Embeddings (no n-grams): {embedding_count}\")\n",
        "print(f\"   - Models: {len(CLASSIFIERS)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Iterate over all combinations\n",
        "for vec_name, vec_type in VECTORIZERS.items():\n",
        "\n",
        "    # Determine if this is an embedding vectorizer\n",
        "    is_embedding = vec_type in embedding_vectorizers\n",
        "\n",
        "    # For embeddings, use single configuration (no n-grams)\n",
        "    # For traditional, use n-gram configurations\n",
        "    ngram_configs = [(None, None)] if is_embedding else VECTORIZER_CONFIGS['ngram_ranges']\n",
        "\n",
        "    for ngram_range in ngram_configs:\n",
        "\n",
        "        # Create vectorizer name\n",
        "        if is_embedding:\n",
        "            ngram_str = \"embedding\"\n",
        "            full_vec_name = f\"{vec_name}\"\n",
        "        else:\n",
        "            ngram_str = f\"ngram_{ngram_range[0]}_{ngram_range[1]}\"\n",
        "            full_vec_name = f\"{vec_name}_{ngram_str}\"\n",
        "\n",
        "        # Check if we need to process this vectorizer configuration\n",
        "        skip_vectorizer = all(\n",
        "            (vec_name, str(ngram_range), model_name) in completed_experiments\n",
        "            for model_name in CLASSIFIERS.keys()\n",
        "        )\n",
        "\n",
        "        if skip_vectorizer:\n",
        "            print(f\"\\n‚è≠Ô∏è  SKIPPING: {vec_name} | Config: {ngram_str} (Already completed)\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        if is_embedding:\n",
        "            print(f\"üìê VECTORIZER: {vec_name} (Embedding)\")\n",
        "        else:\n",
        "            print(f\"üìê VECTORIZER: {vec_name} | N-gram: {ngram_range}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        try:\n",
        "            # Create and fit vectorizer\n",
        "            if is_embedding:\n",
        "                vectorizer = get_vectorizer(vec_type, None, None, embedding_models)\n",
        "            else:\n",
        "                vectorizer = get_vectorizer(vec_type, ngram_range,\n",
        "                                          VECTORIZER_CONFIGS['tfidf_max_features'][0],\n",
        "                                          embedding_models)\n",
        "\n",
        "            # Transform data\n",
        "            if is_embedding:\n",
        "                print(f\"   üîÑ Transforming texts with {vec_name}...\")\n",
        "                X_train_vec = vectorizer.transform(X_train)\n",
        "                X_val_vec = vectorizer.transform(X_val)\n",
        "            else:\n",
        "                X_train_vec = vectorizer.fit_transform(X_train)\n",
        "                X_val_vec = vectorizer.transform(X_val)\n",
        "\n",
        "            # Convert to float32 for compatibility with all models (especially LightGBM)\n",
        "            X_train_vec = convert_to_float32(X_train_vec)\n",
        "            X_val_vec = convert_to_float32(X_val_vec)\n",
        "\n",
        "            # Log vectorizer info\n",
        "            vec_shape = X_train_vec.shape[1]\n",
        "            wandb.log({f\"{full_vec_name}/feature_dimension\": vec_shape})\n",
        "\n",
        "            print(f\"   ‚úÖ Features extracted: {vec_shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå ERROR in vectorization: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "        # Train all models with this vectorizer\n",
        "        for model_name, clf in CLASSIFIERS.items():\n",
        "\n",
        "            # Check if this specific experiment was already completed\n",
        "            if (vec_name, str(ngram_range), model_name) in completed_experiments:\n",
        "                print(f\"\\n   ‚è≠Ô∏è  SKIPPING: {model_name} (Already completed)\")\n",
        "                continue\n",
        "\n",
        "            experiment_counter += 1\n",
        "\n",
        "            print(f\"\\n   [{experiment_counter}/{total_experiments}] üîÑ Training: {model_name}\")\n",
        "\n",
        "            try:\n",
        "                # Train model\n",
        "                model = OneVsRestClassifier(clf, n_jobs=-1)\n",
        "                model.fit(X_train_vec, y_train)\n",
        "\n",
        "                # Predictions\n",
        "                y_pred = model.predict(X_val_vec)\n",
        "\n",
        "                # Get probabilities\n",
        "                if hasattr(model, \"predict_proba\"):\n",
        "                    y_pred_proba = model.predict_proba(X_val_vec)\n",
        "                elif hasattr(model, \"decision_function\"):\n",
        "                    y_pred_proba = model.decision_function(X_val_vec)\n",
        "                    # Normalize to [0, 1]\n",
        "                    from sklearn.preprocessing import MinMaxScaler\n",
        "                    scaler = MinMaxScaler()\n",
        "                    y_pred_proba = scaler.fit_transform(y_pred_proba)\n",
        "                else:\n",
        "                    y_pred_proba = y_pred\n",
        "\n",
        "                # Compute metrics\n",
        "                metrics = compute_advanced_metrics(y_val, y_pred, y_pred_proba, emotions)\n",
        "\n",
        "                # Log all metrics to W&B\n",
        "                for metric_name, value in metrics.items():\n",
        "                    wandb.log({f\"{full_vec_name}/{model_name}/{metric_name}\": value})\n",
        "\n",
        "                # Classification report\n",
        "                class_report = classification_report(\n",
        "                    y_val, y_pred, target_names=emotions,\n",
        "                    output_dict=True, zero_division=0\n",
        "                )\n",
        "\n",
        "                # Confusion matrix\n",
        "                plot_combined_confusion_matrix(y_val, y_pred, emotions, model_name, full_vec_name)\n",
        "\n",
        "                # Store results\n",
        "                result_entry = {\n",
        "                    'Vectorizer': vec_name,\n",
        "                    'N-gram': str(ngram_range),\n",
        "                    'Model': model_name,\n",
        "                    'F1_Macro': metrics['f1_macro'],\n",
        "                    'F1_Micro': metrics['f1_micro'],\n",
        "                    'F1_Weighted': metrics['f1_weighted'],\n",
        "                    'Hamming_Loss': metrics['hamming_loss'],\n",
        "                    'Jaccard_Score': metrics['jaccard_score'],\n",
        "                    'Subset_Accuracy': metrics['subset_accuracy'],\n",
        "                    'Full_Name': full_vec_name,\n",
        "                    'Is_Embedding': is_embedding\n",
        "                }\n",
        "\n",
        "                all_results.append(result_entry)\n",
        "\n",
        "                # Update results dataframe and save checkpoint\n",
        "                all_results_df = pd.concat([all_results_df, pd.DataFrame([result_entry])], ignore_index=True)\n",
        "                save_results_checkpoint(all_results_df)\n",
        "\n",
        "                # Save checkpoint state\n",
        "                checkpoint_state = {\n",
        "                    'completed_experiments': completed_experiments.union({(vec_name, str(ngram_range), model_name)}),\n",
        "                    'experiment_counter': experiment_counter\n",
        "                }\n",
        "                save_checkpoint(checkpoint_state)\n",
        "\n",
        "                # Add to completed set\n",
        "                completed_experiments.add((vec_name, str(ngram_range), model_name))\n",
        "\n",
        "                print(f\"      ‚úÖ F1-Macro: {metrics['f1_macro']:.4f} | Hamming: {metrics['hamming_loss']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      ‚ùå ERROR training {model_name}: {e}\")\n",
        "                print(f\"      üìù Error details: {type(e).__name__}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                # Save checkpoint even on error\n",
        "                save_results_checkpoint(all_results_df)\n",
        "                continue\n",
        "\n",
        "# =========================\n",
        "# 7Ô∏è‚É£ Comprehensive Results Analysis\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà COMPREHENSIVE RESULTS ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results_df = all_results_df.sort_values(by='F1_Macro', ascending=False)\n",
        "\n",
        "# Save full results\n",
        "results_path = OUTPUT_DIR / \"reports\" / \"full_experiment_results.csv\"\n",
        "results_df.to_csv(results_path, index=False)\n",
        "wandb.save(str(results_path))\n",
        "\n",
        "# Display top 10 results\n",
        "print(\"\\nüèÜ TOP 10 CONFIGURATIONS:\")\n",
        "print(results_df.head(10).to_string(index=False))\n",
        "\n",
        "if len(results_df) > 0:\n",
        "    # Best configuration\n",
        "    best_config = results_df.iloc[0]\n",
        "    print(f\"\\nü•á BEST CONFIGURATION:\")\n",
        "    print(f\"   Vectorizer: {best_config['Vectorizer']}\")\n",
        "    print(f\"   N-gram: {best_config['N-gram']}\")\n",
        "    print(f\"   Model: {best_config['Model']}\")\n",
        "    print(f\"   F1-Macro: {best_config['F1_Macro']:.4f}\")\n",
        "    print(f\"   Is Embedding: {best_config.get('Is_Embedding', False)}\")\n",
        "\n",
        "    # Log best config\n",
        "    wandb.log({\n",
        "        \"best/vectorizer\": best_config['Vectorizer'],\n",
        "        \"best/ngram\": best_config['N-gram'],\n",
        "        \"best/model\": best_config['Model'],\n",
        "        \"best/f1_macro\": best_config['F1_Macro'],\n",
        "        \"best/is_embedding\": best_config.get('Is_Embedding', False)\n",
        "    })\n",
        "\n",
        "    # =========================\n",
        "    # 8Ô∏è‚É£ Vectorizer Comparison\n",
        "    # =========================\n",
        "    if len(results_df.groupby('Vectorizer')) > 0:\n",
        "        print(\"\\nüìä VECTORIZER PERFORMANCE COMPARISON:\")\n",
        "        vectorizer_perf = results_df.groupby('Vectorizer')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "        print(vectorizer_perf.sort_values('mean', ascending=False))\n",
        "\n",
        "        # Plot vectorizer comparison\n",
        "        fig, ax = plt.subplots(figsize=(14, 7))\n",
        "        vec_comparison = results_df.groupby('Vectorizer')['F1_Macro'].mean().sort_values(ascending=False)\n",
        "        bars = ax.bar(vec_comparison.index, vec_comparison.values,\n",
        "                      color=plt.cm.viridis(np.linspace(0, 1, len(vec_comparison))))\n",
        "        ax.set_xlabel('Vectorizer', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Mean F1-Macro Score', fontsize=14, fontweight='bold')\n",
        "        ax.set_title('Vectorizer Performance Comparison (Averaged Across All Models)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "        ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        vec_comp_path = OUTPUT_DIR / \"plots\" / \"vectorizer_comparison.png\"\n",
        "        plt.savefig(vec_comp_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "        wandb.log({\"comparison/vectorizer_performance\": wandb.Image(str(vec_comp_path))})\n",
        "        plt.close()\n",
        "\n",
        "    # =========================\n",
        "    # 9Ô∏è‚É£ Embedding vs Traditional Comparison\n",
        "    # =========================\n",
        "    if 'Is_Embedding' in results_df.columns:\n",
        "        print(\"\\nüìä EMBEDDING VS TRADITIONAL VECTORIZERS:\")\n",
        "        embed_comparison = results_df.groupby('Is_Embedding')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "        embed_comparison.index = ['Traditional', 'Embedding']\n",
        "        print(embed_comparison)\n",
        "\n",
        "        # Plot comparison\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        embed_means = results_df.groupby('Is_Embedding')['F1_Macro'].mean()\n",
        "        embed_labels = ['Traditional\\nVectorizers', 'Embedding\\nVectorizers']\n",
        "        bars = ax.bar(embed_labels, embed_means.values,\n",
        "                     color=['#3498db', '#e74c3c'], width=0.6)\n",
        "        ax.set_ylabel('Mean F1-Macro Score', fontsize=14, fontweight='bold')\n",
        "        ax.set_title('Embedding vs Traditional Vectorizers Performance',\n",
        "                    fontsize=16, fontweight='bold', pad=20)\n",
        "        ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.4f}', ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        embed_comp_path = OUTPUT_DIR / \"plots\" / \"embedding_vs_traditional.png\"\n",
        "        plt.savefig(embed_comp_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "        wandb.log({\"comparison/embedding_vs_traditional\": wandb.Image(str(embed_comp_path))})\n",
        "        plt.close()\n",
        "\n",
        "    # =========================\n",
        "    # üîü N-gram Range Comparison (Traditional only)\n",
        "    # =========================\n",
        "    traditional_results = results_df[results_df['Is_Embedding'] == False]\n",
        "    if len(traditional_results) > 0:\n",
        "        print(\"\\nüìä N-GRAM RANGE PERFORMANCE (Traditional Vectorizers):\")\n",
        "        ngram_perf = traditional_results.groupby('N-gram')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "        print(ngram_perf.sort_values('mean', ascending=False))\n",
        "\n",
        "        # Plot n-gram comparison\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ngram_comparison = traditional_results.groupby('N-gram')['F1_Macro'].mean().sort_values(ascending=False)\n",
        "        bars = ax.bar(range(len(ngram_comparison)), ngram_comparison.values,\n",
        "                     color=plt.cm.plasma(np.linspace(0, 1, len(ngram_comparison))))\n",
        "        ax.set_xticks(range(len(ngram_comparison)))\n",
        "        ax.set_xticklabels(ngram_comparison.index, rotation=0)\n",
        "        ax.set_xlabel('N-gram Range', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Mean F1-Macro Score', fontsize=14, fontweight='bold')\n",
        "        ax.set_title('N-gram Range Performance Comparison', fontsize=16, fontweight='bold', pad=20)\n",
        "        ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "        for i, v in enumerate(ngram_comparison.values):\n",
        "            ax.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        ngram_comp_path = OUTPUT_DIR / \"plots\" / \"ngram_comparison.png\"\n",
        "        plt.savefig(ngram_comp_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "        wandb.log({\"comparison/ngram_performance\": wandb.Image(str(ngram_comp_path))})\n",
        "        plt.close()\n",
        "\n",
        "    # =========================\n",
        "    # 1Ô∏è‚É£1Ô∏è‚É£ Model Performance Across Vectorizers\n",
        "    # =========================\n",
        "    print(\"\\nüìä MODEL PERFORMANCE ACROSS VECTORIZERS:\")\n",
        "    model_perf = results_df.groupby('Model')['F1_Macro'].agg(['mean', 'max', 'std']).round(4)\n",
        "    print(model_perf.sort_values('mean', ascending=False))\n",
        "\n",
        "    # Heatmap: Models vs Vectorizers\n",
        "    pivot_table = results_df.pivot_table(\n",
        "        values='F1_Macro',\n",
        "        index='Model',\n",
        "        columns='Vectorizer',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd',\n",
        "                cbar_kws={'label': 'F1-Macro Score'}, ax=ax,\n",
        "                linewidths=1, linecolor='white')\n",
        "    ax.set_title('Model Performance Heatmap (Across Vectorizers)',\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Vectorizer', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Model', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    heatmap_path = OUTPUT_DIR / \"plots\" / \"model_vectorizer_heatmap.png\"\n",
        "    plt.savefig(heatmap_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({\"comparison/model_vectorizer_heatmap\": wandb.Image(str(heatmap_path))})\n",
        "    plt.close()\n",
        "\n",
        "    # =========================\n",
        "    # 1Ô∏è‚É£2Ô∏è‚É£ Generate Best Model Submission\n",
        "    # =========================\n",
        "    print(f\"\\nüîÑ Retraining best model on full dataset...\")\n",
        "\n",
        "    best_vec_name = best_config['Vectorizer']\n",
        "    best_vec_type = VECTORIZERS[best_vec_name]\n",
        "    best_is_embedding = best_config.get('Is_Embedding', False)\n",
        "    best_model_name = best_config['Model']\n",
        "\n",
        "    try:\n",
        "        # Recreate best vectorizer and model\n",
        "        if best_is_embedding:\n",
        "            best_vectorizer = get_vectorizer(best_vec_type, None, None, embedding_models)\n",
        "            print(f\"   üîÑ Using {best_vec_name} embeddings...\")\n",
        "            X_full_best = best_vectorizer.transform(X)\n",
        "        else:\n",
        "            best_ngram = eval(best_config['N-gram'])\n",
        "            best_vectorizer = get_vectorizer(best_vec_type, best_ngram,\n",
        "                                            VECTORIZER_CONFIGS['tfidf_max_features'][0])\n",
        "            X_full_best = best_vectorizer.fit_transform(X)\n",
        "\n",
        "        X_full_best = convert_to_float32(X_full_best)\n",
        "\n",
        "        best_clf = CLASSIFIERS[best_model_name]\n",
        "        best_model_full = OneVsRestClassifier(best_clf, n_jobs=-1)\n",
        "        best_model_full.fit(X_full_best, y)\n",
        "\n",
        "        print(\"   ‚úÖ Best model retrained on full dataset\")\n",
        "\n",
        "        # Test Set Prediction\n",
        "        print(\"\\nüìù Generating predictions on test set...\")\n",
        "        clean_test['final_text'] = clean_test['final_text'].fillna('')\n",
        "\n",
        "        if best_is_embedding:\n",
        "            X_test_best = best_vectorizer.transform(clean_test['final_text'])\n",
        "        else:\n",
        "            X_test_best = best_vectorizer.transform(clean_test['final_text'])\n",
        "\n",
        "        X_test_best = convert_to_float32(X_test_best)\n",
        "        y_test_pred = best_model_full.predict(X_test_best)\n",
        "\n",
        "        # Create submission\n",
        "        submission = pd.DataFrame(y_test_pred, columns=y.columns)\n",
        "        submission['id'] = clean_test['id']\n",
        "        submission = submission[['id'] + list(y.columns)]\n",
        "\n",
        "        submission_path = OUTPUT_DIR / \"submission.csv\"\n",
        "        submission.to_csv(submission_path, index=False)\n",
        "        wandb.save(str(submission_path))\n",
        "\n",
        "        print(f\"   ‚úÖ Submission saved: {submission_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error generating submission: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # =========================\n",
        "    # 1Ô∏è‚É£3Ô∏è‚É£ Executive Summary Dashboard\n",
        "    # =========================\n",
        "    print(\"\\nüìä Creating executive summary dashboard...\")\n",
        "\n",
        "    fig = plt.figure(figsize=(22, 14))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
        "\n",
        "    # 1. Best Configuration Box\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.axis('off')\n",
        "    summary_text = f\"\"\"\n",
        "BEST CONFIGURATION\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Vectorizer: {best_config['Vectorizer']}\n",
        "Type: {'Embedding' if best_is_embedding else 'Traditional'}\n",
        "N-gram: {best_config['N-gram']}\n",
        "Model: {best_config['Model']}\n",
        "\n",
        "PERFORMANCE METRICS\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "F1-Macro: {best_config['F1_Macro']:.4f}\n",
        "F1-Micro: {best_config['F1_Micro']:.4f}\n",
        "F1-Weighted: {best_config['F1_Weighted']:.4f}\n",
        "Hamming Loss: {best_config['Hamming_Loss']:.4f}\n",
        "Jaccard Score: {best_config['Jaccard_Score']:.4f}\n",
        "Subset Accuracy: {best_config['Subset_Accuracy']:.4f}\n",
        "\"\"\"\n",
        "    ax1.text(0.1, 0.5, summary_text, fontsize=10, fontfamily='monospace',\n",
        "             verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    # 2. Top 5 Configurations\n",
        "    ax2 = fig.add_subplot(gs[0, 1:])\n",
        "    top5 = results_df.head(5)\n",
        "    ax2.axis('tight')\n",
        "    ax2.axis('off')\n",
        "    table_data = []\n",
        "    for idx, row in top5.iterrows():\n",
        "        table_data.append([\n",
        "            row['Vectorizer'],\n",
        "            row['N-gram'][:15],\n",
        "            row['Model'],\n",
        "            f\"{row['F1_Macro']:.4f}\"\n",
        "        ])\n",
        "    table = ax2.table(cellText=table_data,\n",
        "                      colLabels=['Vectorizer', 'N-gram', 'Model', 'F1-Macro'],\n",
        "                      cellLoc='center',\n",
        "                      loc='center',\n",
        "                      colWidths=[0.25, 0.2, 0.3, 0.15])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1, 2)\n",
        "    for i in range(len(top5) + 1):\n",
        "        if i == 0:\n",
        "            for j in range(4):\n",
        "                table[(i, j)].set_facecolor('#4CAF50')\n",
        "        else:\n",
        "            for j in range(4):\n",
        "                table[(i, j)].set_facecolor('#E8F5E9')\n",
        "    ax2.set_title('TOP 5 CONFIGURATIONS', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    # 3. Vectorizer Performance\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    vec_means = results_df.groupby('Vectorizer')['F1_Macro'].mean().sort_values(ascending=True)\n",
        "    colors = ['#e74c3c' if 'Word2Vec' in idx or 'GloVe' in idx or 'FastText' in idx\n",
        "              else '#3498db' for idx in vec_means.index]\n",
        "    ax3.barh(vec_means.index, vec_means.values, color=colors)\n",
        "    ax3.set_xlabel('Mean F1-Macro', fontsize=11, fontweight='bold')\n",
        "    ax3.set_title('Vectorizer Performance', fontsize=12, fontweight='bold')\n",
        "    ax3.grid(axis='x', alpha=0.3)\n",
        "    for i, v in enumerate(vec_means.values):\n",
        "        ax3.text(v, i, f' {v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "    # 4. Model Performance\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    model_means = results_df.groupby('Model')['F1_Macro'].mean().sort_values(ascending=False)\n",
        "    ax4.bar(range(len(model_means)), model_means.values,\n",
        "            color=plt.cm.Set3(np.linspace(0, 1, len(model_means))))\n",
        "    ax4.set_xticks(range(len(model_means)))\n",
        "    ax4.set_xticklabels(model_means.index, rotation=45, ha='right', fontsize=9)\n",
        "    ax4.set_ylabel('Mean F1-Macro', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Model Performance', fontsize=12, fontweight='bold')\n",
        "    ax4.grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(model_means.values):\n",
        "        ax4.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "    # 5. Embedding vs Traditional\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    if 'Is_Embedding' in results_df.columns:\n",
        "        embed_means = results_df.groupby('Is_Embedding')['F1_Macro'].mean()\n",
        "        labels = ['Traditional', 'Embedding']\n",
        "        ax5.bar(labels, embed_means.values, color=['#3498db', '#e74c3c'], width=0.5)\n",
        "        ax5.set_ylabel('Mean F1-Macro', fontsize=11, fontweight='bold')\n",
        "        ax5.set_title('Embedding vs Traditional', fontsize=12, fontweight='bold')\n",
        "        ax5.grid(axis='y', alpha=0.3)\n",
        "        for i, v in enumerate(embed_means.values):\n",
        "            ax5.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # 6. F1-Macro Distribution\n",
        "    ax6 = fig.add_subplot(gs[2, 0])\n",
        "    ax6.hist(results_df['F1_Macro'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    ax6.axvline(best_config['F1_Macro'], color='red', linestyle='--', linewidth=2,\n",
        "                label=f\"Best: {best_config['F1_Macro']:.4f}\")\n",
        "    ax6.set_xlabel('F1-Macro Score', fontsize=11, fontweight='bold')\n",
        "    ax6.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax6.set_title('F1-Macro Distribution', fontsize=12, fontweight='bold')\n",
        "    ax6.legend(fontsize=9)\n",
        "    ax6.grid(alpha=0.3)\n",
        "\n",
        "    # 7. Hamming Loss Distribution\n",
        "    ax7 = fig.add_subplot(gs[2, 1])\n",
        "    ax7.hist(results_df['Hamming_Loss'], bins=30, color='salmon', edgecolor='black', alpha=0.7)\n",
        "    ax7.axvline(best_config['Hamming_Loss'], color='green', linestyle='--', linewidth=2,\n",
        "                label=f\"Best: {best_config['Hamming_Loss']:.4f}\")\n",
        "    ax7.set_xlabel('Hamming Loss', fontsize=11, fontweight='bold')\n",
        "    ax7.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    ax7.set_title('Hamming Loss Distribution', fontsize=12, fontweight='bold')\n",
        "    ax7.legend(fontsize=9)\n",
        "    ax7.grid(alpha=0.3)\n",
        "\n",
        "    # 8. Experiment Statistics\n",
        "    ax8 = fig.add_subplot(gs[2, 2])\n",
        "    ax8.axis('off')\n",
        "    stats_text = f\"\"\"\n",
        "EXPERIMENT STATISTICS\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Total Experiments: {len(results_df)}\n",
        "Vectorizers: {len(VECTORIZERS)}\n",
        "  - Traditional: {traditional_count}\n",
        "  - Embeddings: {embedding_count}\n",
        "Models: {len(CLASSIFIERS)}\n",
        "\n",
        "BEST SCORES\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Highest F1-Macro: {results_df['F1_Macro'].max():.4f}\n",
        "Lowest Hamming Loss: {results_df['Hamming_Loss'].min():.4f}\n",
        "Highest Jaccard: {results_df['Jaccard_Score'].max():.4f}\n",
        "\n",
        "AVERAGE SCORES\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "Mean F1-Macro: {results_df['F1_Macro'].mean():.4f}\n",
        "Std F1-Macro: {results_df['F1_Macro'].std():.4f}\n",
        "\"\"\"\n",
        "    ax8.text(0.1, 0.5, stats_text, fontsize=9, fontfamily='monospace',\n",
        "             verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('üöÄ EXPERIMENT EXECUTIVE SUMMARY DASHBOARD\\nAll Vectorizers + Embeddings',\n",
        "                 fontsize=18, fontweight='bold', y=0.98)\n",
        "\n",
        "    dashboard_path = OUTPUT_DIR / \"plots\" / \"executive_summary_dashboard.png\"\n",
        "    plt.savefig(dashboard_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({\"summary/executive_dashboard\": wandb.Image(str(dashboard_path))})\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"   ‚úÖ Executive dashboard created: {dashboard_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No results to analyze. All experiments may have failed.\")\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£4Ô∏è‚É£ Clean up checkpoints\n",
        "# =========================\n",
        "print(\"\\nüßπ Experiment completed. Checkpoint files are preserved for recovery.\")\n",
        "print(f\"   Checkpoint location: {OUTPUT_DIR / 'checkpoints'}\")\n",
        "\n",
        "# =========================\n",
        "# üéØ Finish Experiment\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ COMPREHENSIVE EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "if len(results_df) > 0:\n",
        "    print(f\"\\nüìä FINAL RESULTS:\")\n",
        "    print(f\"   ‚Ä¢ Total Experiments Run: {len(results_df)}\")\n",
        "    print(f\"   ‚Ä¢ Best Vectorizer: {best_config['Vectorizer']}\")\n",
        "    print(f\"   ‚Ä¢ Best Type: {'Embedding' if best_is_embedding else 'Traditional'}\")\n",
        "    print(f\"   ‚Ä¢ Best N-gram: {best_config['N-gram']}\")\n",
        "    print(f\"   ‚Ä¢ Best Model: {best_config['Model']}\")\n",
        "    print(f\"   ‚Ä¢ Best F1-Macro: {best_config['F1_Macro']:.4f}\")\n",
        "    print(f\"\\nüåü VECTORIZERS TESTED:\")\n",
        "    for vec_name in results_df['Vectorizer'].unique():\n",
        "        mean_score = results_df[results_df['Vectorizer'] == vec_name]['F1_Macro'].mean()\n",
        "        print(f\"   ‚Ä¢ {vec_name}: {mean_score:.4f} (avg)\")\n",
        "print(f\"\\nüìÅ All outputs saved in: {OUTPUT_DIR}\")\n",
        "print(f\"üîó View comprehensive results in W&B: {wandb.run.get_url()}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d_xC0bkP59U7",
        "outputId": "e241afba-9ef7-4590-ab6d-1d411c4a4935"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CountVectorizer_ngram_1_1/LinearSVC/anger_auc</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/anger_f1</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/f1_macro</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/f1_micro</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/f1_weighted</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/fear_auc</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/fear_f1</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/hamming_loss</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/jaccard_score</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/joy_auc</td><td>‚ñÅ</td></tr><tr><td>+294</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CountVectorizer_ngram_1_1/LinearSVC/anger_auc</td><td>0.71266</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/anger_f1</td><td>0.30526</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/f1_macro</td><td>0.48346</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/f1_micro</td><td>0.54941</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/f1_weighted</td><td>0.54608</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/fear_auc</td><td>0.69605</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/fear_f1</td><td>0.69156</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/hamming_loss</td><td>0.26106</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/jaccard_score</td><td>0.39286</td></tr><tr><td>CountVectorizer_ngram_1_1/LinearSVC/joy_auc</td><td>0.68938</td></tr><tr><td>+295</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">D02-multi-vectorizer-classification-20251010_070017</strong> at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/720dp750' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/720dp750</a><br> View project at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a><br>Synced 5 W&B file(s), 19 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251010_070211-720dp750/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251010_080354-f89a5igy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/f89a5igy' target=\"_blank\">D02-multi-vectorizer-classification-20251010_070017</a></strong> to <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/f89a5igy' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/f89a5igy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üöÄ STARTING COMPREHENSIVE VECTORIZER + MODEL EXPERIMENT\n",
            "================================================================================\n",
            "\n",
            "üìä Analyzing Label Distribution...\n",
            "‚úÖ Train set size: 3992 | Validation set size: 999\n",
            "\n",
            "================================================================================\n",
            "üåê PREPARING EMBEDDING MODELS\n",
            "================================================================================\n",
            "\n",
            "üîÑ Training Word2Vec model...\n",
            "   ‚úÖ Word2Vec trained: vocab_size=2874\n",
            "\n",
            "üîÑ Loading GloVe embeddings (dim=100)...\n",
            "   üåê Downloading glove-twitter-100 from gensim...\n",
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n",
            "   ‚úÖ GloVe model saved to cache\n",
            "\n",
            "üîÑ Training FastText model...\n",
            "   ‚úÖ FastText trained: vocab_size=2874\n",
            "\n",
            "‚úÖ Embedding models ready: ['word2vec', 'glove', 'fasttext']\n",
            "\n",
            "üìê Total Vectorizers Available: 5\n",
            "   ‚Ä¢ CountVectorizer (count)\n",
            "   ‚Ä¢ HashingVectorizer (hashing)\n",
            "   ‚Ä¢ Word2Vec (word2vec)\n",
            "   ‚Ä¢ GloVe (glove)\n",
            "   ‚Ä¢ FastText (fasttext)\n",
            "\n",
            "üÜï STARTING NEW EXPERIMENT\n",
            "\n",
            "================================================================================\n",
            "ü§ñ TRAINING ALL VECTORIZER + MODEL COMBINATIONS\n",
            "================================================================================\n",
            "\n",
            "üìä Total Experiments to Run: 45\n",
            "   - Already Completed: 0\n",
            "   - Remaining: 45\n",
            "   - Vectorizers: 5\n",
            "   - Traditional (with n-grams): 2\n",
            "   - Embeddings (no n-grams): 3\n",
            "   - Models: 5\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: CountVectorizer | N-gram: (1, 1)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [1/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4768 | Hamming: 0.2428\n",
            "\n",
            "   [2/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4835 | Hamming: 0.2611\n",
            "\n",
            "   [3/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3987 | Hamming: 0.2344\n",
            "\n",
            "   [4/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3614 | Hamming: 0.2561\n",
            "\n",
            "   [5/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4123 | Hamming: 0.2328\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: CountVectorizer | N-gram: (1, 2)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [6/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4745 | Hamming: 0.2460\n",
            "\n",
            "   [7/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4813 | Hamming: 0.2653\n",
            "\n",
            "   [8/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3901 | Hamming: 0.2352\n",
            "\n",
            "   [9/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3616 | Hamming: 0.2563\n",
            "\n",
            "   [10/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4131 | Hamming: 0.2336\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: CountVectorizer | N-gram: (1, 3)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [11/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4867 | Hamming: 0.2446\n",
            "\n",
            "   [12/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4852 | Hamming: 0.2629\n",
            "\n",
            "   [13/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3901 | Hamming: 0.2352\n",
            "\n",
            "   [14/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3616 | Hamming: 0.2563\n",
            "\n",
            "   [15/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4020 | Hamming: 0.2338\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: HashingVectorizer | N-gram: (1, 1)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [16/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3757 | Hamming: 0.2408\n",
            "\n",
            "   [17/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4462 | Hamming: 0.2486\n",
            "\n",
            "   [18/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3850 | Hamming: 0.2432\n",
            "\n",
            "   [19/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3596 | Hamming: 0.2619\n",
            "\n",
            "   [20/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4072 | Hamming: 0.2366\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: HashingVectorizer | N-gram: (1, 2)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [21/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3369 | Hamming: 0.2478\n",
            "\n",
            "   [22/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3993 | Hamming: 0.2687\n",
            "\n",
            "   [23/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3533 | Hamming: 0.2494\n",
            "\n",
            "   [24/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3513 | Hamming: 0.2613\n",
            "\n",
            "   [25/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3391 | Hamming: 0.2430\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: HashingVectorizer | N-gram: (1, 3)\n",
            "================================================================================\n",
            "   ‚úÖ Features extracted: 5000\n",
            "\n",
            "   [26/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3141 | Hamming: 0.2557\n",
            "\n",
            "   [27/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3712 | Hamming: 0.2783\n",
            "\n",
            "   [28/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3447 | Hamming: 0.2549\n",
            "\n",
            "   [29/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3564 | Hamming: 0.2599\n",
            "\n",
            "   [30/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3008 | Hamming: 0.2527\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: Word2Vec (Embedding)\n",
            "================================================================================\n",
            "   üîÑ Transforming texts with Word2Vec...\n",
            "   ‚úÖ Features extracted: 100\n",
            "\n",
            "   [31/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.1846 | Hamming: 0.2779\n",
            "\n",
            "   [32/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.1471 | Hamming: 0.2725\n",
            "\n",
            "   [33/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.2411 | Hamming: 0.2907\n",
            "\n",
            "   [34/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.2495 | Hamming: 0.2865\n",
            "\n",
            "   [35/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.2022 | Hamming: 0.2833\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: GloVe (Embedding)\n",
            "================================================================================\n",
            "   üîÑ Transforming texts with GloVe...\n",
            "   ‚úÖ Features extracted: 100\n",
            "\n",
            "   [36/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.3302 | Hamming: 0.2380\n",
            "\n",
            "   [37/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4753 | Hamming: 0.2206\n",
            "\n",
            "   [38/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4785 | Hamming: 0.2220\n",
            "\n",
            "   [39/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4722 | Hamming: 0.2250\n",
            "\n",
            "   [40/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.4541 | Hamming: 0.2238\n",
            "\n",
            "================================================================================\n",
            "üìê VECTORIZER: FastText (Embedding)\n",
            "================================================================================\n",
            "   üîÑ Transforming texts with FastText...\n",
            "   ‚úÖ Features extracted: 100\n",
            "\n",
            "   [41/45] üîÑ Training: RandomForest\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.1797 | Hamming: 0.2865\n",
            "\n",
            "   [42/45] üîÑ Training: LinearSVC\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.1468 | Hamming: 0.2733\n",
            "\n",
            "   [43/45] üîÑ Training: XGBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.2171 | Hamming: 0.2943\n",
            "\n",
            "   [44/45] üîÑ Training: LightGBM\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.2243 | Hamming: 0.2971\n",
            "\n",
            "   [45/45] üîÑ Training: CatBoost\n",
            "   üíæ Results checkpoint saved: outputs/checkpoints/results_checkpoint.csv\n",
            "   üíæ Checkpoint saved: outputs/checkpoints/experiment_checkpoint.pkl\n",
            "      ‚úÖ F1-Macro: 0.1789 | Hamming: 0.2845\n",
            "\n",
            "================================================================================\n",
            "üìà COMPREHENSIVE RESULTS ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "üèÜ TOP 10 CONFIGURATIONS:\n",
            "     Vectorizer       N-gram        Model  F1_Macro  F1_Micro  F1_Weighted  Hamming_Loss  Jaccard_Score  Subset_Accuracy                 Full_Name  Is_Embedding\n",
            "CountVectorizer       (1, 3) RandomForest  0.486679  0.566359     0.552311      0.244645       0.405873         0.263263 CountVectorizer_ngram_1_3         False\n",
            "CountVectorizer       (1, 3)    LinearSVC  0.485174  0.548642     0.545726      0.262863       0.392042         0.246246 CountVectorizer_ngram_1_3         False\n",
            "CountVectorizer       (1, 1)    LinearSVC  0.483455  0.549413     0.546078      0.261061       0.392860         0.249249 CountVectorizer_ngram_1_1         False\n",
            "CountVectorizer       (1, 2)    LinearSVC  0.481273  0.543576     0.540781      0.265265       0.384718         0.239239 CountVectorizer_ngram_1_2         False\n",
            "          GloVe (None, None)      XGBoost  0.478517  0.589107     0.559566      0.222022       0.417835         0.289289                     GloVe          True\n",
            "CountVectorizer       (1, 1) RandomForest  0.476832  0.560984     0.544732      0.242843       0.399399         0.264264 CountVectorizer_ngram_1_1         False\n",
            "          GloVe (None, None)    LinearSVC  0.475299  0.584151     0.558384      0.220621       0.415832         0.294294                     GloVe          True\n",
            "CountVectorizer       (1, 2) RandomForest  0.474495  0.560601     0.544311      0.246046       0.401818         0.264264 CountVectorizer_ngram_1_2         False\n",
            "          GloVe (None, None)     LightGBM  0.472203  0.581845     0.553033      0.225025       0.411078         0.285285                     GloVe          True\n",
            "          GloVe (None, None)     CatBoost  0.454064  0.579699     0.544725      0.223824       0.421672         0.291291                     GloVe          True\n",
            "\n",
            "ü•á BEST CONFIGURATION:\n",
            "   Vectorizer: CountVectorizer\n",
            "   N-gram: (1, 3)\n",
            "   Model: RandomForest\n",
            "   F1-Macro: 0.4867\n",
            "   Is Embedding: False\n",
            "\n",
            "üìä VECTORIZER PERFORMANCE COMPARISON:\n",
            "                     mean     max     std\n",
            "Vectorizer                               \n",
            "GloVe              0.4421  0.4785  0.0633\n",
            "CountVectorizer    0.4253  0.4867  0.0501\n",
            "HashingVectorizer  0.3627  0.4462  0.0369\n",
            "Word2Vec           0.2049  0.2495  0.0420\n",
            "FastText           0.1894  0.2243  0.0316\n",
            "\n",
            "üìä EMBEDDING VS TRADITIONAL VECTORIZERS:\n",
            "               mean     max     std\n",
            "Traditional  0.3940  0.4867  0.0537\n",
            "Embedding    0.2788  0.4785  0.1275\n",
            "\n",
            "üìä N-GRAM RANGE PERFORMANCE (Traditional Vectorizers):\n",
            "          mean     max     std\n",
            "N-gram                        \n",
            "(1, 1)  0.4106  0.4835  0.0447\n",
            "(1, 2)  0.3900  0.4813  0.0529\n",
            "(1, 3)  0.3813  0.4867  0.0631\n",
            "\n",
            "üìä MODEL PERFORMANCE ACROSS VECTORIZERS:\n",
            "                mean     max     std\n",
            "Model                               \n",
            "LinearSVC     0.3817  0.4852  0.1390\n",
            "XGBoost       0.3554  0.4785  0.0811\n",
            "RandomForest  0.3510  0.4867  0.1168\n",
            "CatBoost      0.3455  0.4541  0.0989\n",
            "LightGBM      0.3442  0.4722  0.0717\n",
            "\n",
            "üîÑ Retraining best model on full dataset...\n",
            "   ‚úÖ Best model retrained on full dataset\n",
            "\n",
            "üìù Generating predictions on test set...\n",
            "   ‚úÖ Submission saved: outputs/submission.csv\n",
            "\n",
            "üìä Creating executive summary dashboard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ Executive dashboard created: outputs/plots/executive_summary_dashboard.png\n",
            "\n",
            "üßπ Experiment completed. Checkpoint files are preserved for recovery.\n",
            "   Checkpoint location: outputs/checkpoints\n",
            "\n",
            "================================================================================\n",
            "‚úÖ COMPREHENSIVE EXPERIMENT COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            "üìä FINAL RESULTS:\n",
            "   ‚Ä¢ Total Experiments Run: 45\n",
            "   ‚Ä¢ Best Vectorizer: CountVectorizer\n",
            "   ‚Ä¢ Best Type: Traditional\n",
            "   ‚Ä¢ Best N-gram: (1, 3)\n",
            "   ‚Ä¢ Best Model: RandomForest\n",
            "   ‚Ä¢ Best F1-Macro: 0.4867\n",
            "\n",
            "üåü VECTORIZERS TESTED:\n",
            "   ‚Ä¢ CountVectorizer: 0.4253 (avg)\n",
            "   ‚Ä¢ GloVe: 0.4421 (avg)\n",
            "   ‚Ä¢ HashingVectorizer: 0.3627 (avg)\n",
            "   ‚Ä¢ Word2Vec: 0.2049 (avg)\n",
            "   ‚Ä¢ FastText: 0.1894 (avg)\n",
            "\n",
            "üìÅ All outputs saved in: outputs\n",
            "üîó View comprehensive results in W&B: https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/f89a5igy\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CountVectorizer_ngram_1_1/CatBoost/anger_auc</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/anger_f1</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/f1_macro</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/f1_micro</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/f1_weighted</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/fear_auc</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/fear_f1</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/hamming_loss</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/jaccard_score</td><td>‚ñÅ</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/joy_auc</td><td>‚ñÅ</td></tr><tr><td>+732</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CountVectorizer_ngram_1_1/CatBoost/anger_auc</td><td>0.75741</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/anger_f1</td><td>0.224</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/f1_macro</td><td>0.41234</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/f1_micro</td><td>0.53977</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/f1_weighted</td><td>0.49461</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/fear_auc</td><td>0.6893</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/fear_f1</td><td>0.71621</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/hamming_loss</td><td>0.23283</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/jaccard_score</td><td>0.38639</td></tr><tr><td>CountVectorizer_ngram_1_1/CatBoost/joy_auc</td><td>0.69843</td></tr><tr><td>+737</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">D02-multi-vectorizer-classification-20251010_070017</strong> at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/f89a5igy' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/f89a5igy</a><br> View project at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a><br>Synced 5 W&B file(s), 51 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251010_080354-f89a5igy/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, hamming_loss, jaccard_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from lightgbm import LGBMClassifier\n",
        "from gensim import downloader as api\n",
        "import wandb\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£ Initialize W&B\n",
        "# =========================\n",
        "# wandb.init(project=\"23f3003030-t32025\", name=\"glove200_2models\", reinit=True)\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ Load Data\n",
        "# =========================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "yl-oc1gTJb6A",
        "outputId": "3af3f3e1-f0d1-44c2-9a4a-34c40f3189c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251010_095108-lqnds20u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/lqnds20u' target=\"_blank\">glove200_2models</a></strong> to <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/lqnds20u' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/lqnds20u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading glove-twitter-200 embeddings...\n",
            "GloVe model loaded!\n",
            "\n",
            "Training LinearSVC...\n",
            "LinearSVC F1-Macro: 0.5067 | Hamming Loss: 0.2186 | Jaccard: 0.4312\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM F1-Macro: 0.4734 | Hamming Loss: 0.2136 | Jaccard: 0.4184\n",
            "\n",
            "Model comparison:\n",
            "        Model  F1_Macro\n",
            "0  LinearSVC  0.506681\n",
            "1   LightGBM  0.473423\n",
            "Submission saved as submission.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LightGBM_F1_Macro</td><td>‚ñÅ</td></tr><tr><td>LightGBM_Hamming_Loss</td><td>‚ñÅ</td></tr><tr><td>LightGBM_Jaccard</td><td>‚ñÅ</td></tr><tr><td>LinearSVC_F1_Macro</td><td>‚ñÅ</td></tr><tr><td>LinearSVC_Hamming_Loss</td><td>‚ñÅ</td></tr><tr><td>LinearSVC_Jaccard</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LightGBM_F1_Macro</td><td>0.47342</td></tr><tr><td>LightGBM_Hamming_Loss</td><td>0.21361</td></tr><tr><td>LightGBM_Jaccard</td><td>0.41842</td></tr><tr><td>LinearSVC_F1_Macro</td><td>0.50668</td></tr><tr><td>LinearSVC_Hamming_Loss</td><td>0.21862</td></tr><tr><td>LinearSVC_Jaccard</td><td>0.43121</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glove200_2models</strong> at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/lqnds20u' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/lqnds20u</a><br> View project at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251010_095108-lqnds20u/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmfMQEXba-xy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = final_df['final_text'].fillna('')\n",
        "y = final_df[['anger','fear','joy','sadness','surprise']]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ Load GloVe Twitter 200d embeddings\n",
        "# =========================\n",
        "dim = 200\n",
        "glove_model_name = f'glove-twitter-{dim}'\n",
        "print(f\"Loading {glove_model_name} embeddings...\")\n",
        "glove_model = api.load(glove_model_name)\n",
        "print(\"GloVe model loaded!\")\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ Convert text to vectors\n",
        "# =========================\n",
        "def text_to_vector(text, model, dim=200):\n",
        "    words = text.split()\n",
        "    vectors = [model[word] for word in words if word in model]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_vec = np.vstack([text_to_vector(t, glove_model, dim) for t in X_train])\n",
        "X_val_vec   = np.vstack([text_to_vector(t, glove_model, dim) for t in X_val])\n",
        "\n",
        "# =========================\n",
        "# 5Ô∏è‚É£ Define classifiers\n",
        "# =========================\n",
        "classifiers = {\n",
        "    'LinearSVC': LinearSVC(max_iter=5000, random_state=42),\n",
        "    'LightGBM': LGBMClassifier(n_estimators=500, max_depth=8, learning_rate=0.05,\n",
        "                               random_state=42, verbose=-1, n_jobs=-1)\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Train & log metrics\n",
        "# =========================\n",
        "results = []\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model = OneVsRestClassifier(clf)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    y_pred = model.predict(X_val_vec)\n",
        "\n",
        "    f1_macro = f1_score(y_val, y_pred, average='macro')\n",
        "    h_loss = hamming_loss(y_val, y_pred)\n",
        "    jaccard = jaccard_score(y_val, y_pred, average='samples')\n",
        "\n",
        "    print(f\"{name} F1-Macro: {f1_macro:.4f} | Hamming Loss: {h_loss:.4f} | Jaccard: {jaccard:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    class_report = classification_report(y_val, y_pred, target_names=y.columns, output_dict=True)\n",
        "\n",
        "    # Log metrics to W&B\n",
        "    wandb.log({\n",
        "        f\"{name}_F1_Macro\": f1_macro,\n",
        "        f\"{name}_Hamming_Loss\": h_loss,\n",
        "        f\"{name}_Jaccard\": jaccard,\n",
        "        f\"{name}_Classification_Report\": class_report\n",
        "    })\n",
        "\n",
        "    results.append({'Model': name, 'F1_Macro': f1_macro})\n",
        "\n",
        "# =========================\n",
        "# 7Ô∏è‚É£ Summary\n",
        "# =========================\n",
        "results_df = pd.DataFrame(results).sort_values(by='F1_Macro', ascending=False)\n",
        "print(\"\\nModel comparison:\\n\", results_df)\n",
        "wandb.log({\"Results_Table\": results_df})\n",
        "\n",
        "# =========================\n",
        "# 8Ô∏è‚É£ Generate submission\n",
        "# =========================\n",
        "clean_test['final_text'] = clean_test['final_text'].fillna('')\n",
        "X_test_vec = np.vstack([text_to_vector(t, glove_model, dim) for t in clean_test['final_text']])\n",
        "\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model = OneVsRestClassifier(classifiers[best_model_name])\n",
        "best_model.fit(np.vstack([X_train_vec, X_val_vec]), pd.concat([y_train, y_val]))\n",
        "y_test_pred = best_model.predict(X_test_vec)\n",
        "\n",
        "submission = pd.DataFrame(y_test_pred, columns=y.columns)\n",
        "submission['id'] = clean_test['id']\n",
        "submission = submission[['id'] + list(y.columns)]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved as submission.csv\")\n"
      ],
      "metadata": {
        "id": "cOTMtdnXfGM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "ZfRf9ehmkZSe",
        "outputId": "37116c47-7955-4beb-cd95-9a9405bfaef7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LinearSVC(max_iter=5000, random_state=42))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LinearSVC(max_iter=5000, random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneVsRestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\">?<span>Documentation for OneVsRestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>OneVsRestClassifier(estimator=LinearSVC(max_iter=5000, random_state=42))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: LinearSVC</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(max_iter=5000, random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearSVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(max_iter=5000, random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "696EnJqQkbPz",
        "outputId": "2fbc2755-229c-485a-8946-1cb31be75ba5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  anger  fear  joy  sadness  surprise\n",
              "0        0      0     1    0        0         0\n",
              "1        1      0     0    0        0         0\n",
              "2        2      0     0    0        0         0\n",
              "3        3      0     1    0        0         0\n",
              "4        4      0     1    0        0         1\n",
              "...    ...    ...   ...  ...      ...       ...\n",
              "1702  1702      0     1    0        0         0\n",
              "1703  1703      0     0    0        0         0\n",
              "1704  1704      0     1    0        0         0\n",
              "1705  1705      0     0    0        1         0\n",
              "1706  1706      0     1    0        0         0\n",
              "\n",
              "[1707 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef870679-6b9c-4375-ada6-deabc47a571a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>anger</th>\n",
              "      <th>fear</th>\n",
              "      <th>joy</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1702</th>\n",
              "      <td>1702</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1703</th>\n",
              "      <td>1703</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1704</th>\n",
              "      <td>1704</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1705</th>\n",
              "      <td>1705</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1706</th>\n",
              "      <td>1706</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1707 rows √ó 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef870679-6b9c-4375-ada6-deabc47a571a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ef870679-6b9c-4375-ada6-deabc47a571a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ef870679-6b9c-4375-ada6-deabc47a571a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e4d94bba-84ed-438e-8418-e50b04940bd8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4d94bba-84ed-438e-8418-e50b04940bd8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e4d94bba-84ed-438e-8418-e50b04940bd8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_49a827be-c35e-414c-8f9f-059e3363e7c8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('submission')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_49a827be-c35e-414c-8f9f-059e3363e7c8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('submission');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "submission",
              "summary": "{\n  \"name\": \"submission\",\n  \"rows\": 1707,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 492,\n        \"min\": 0,\n        \"max\": 1706,\n        \"num_unique_values\": 1707,\n        \"samples\": [\n          567,\n          1325,\n          1350\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"anger\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fear\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"joy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sadness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"surprise\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "pRIflY2ikphr",
        "outputId": "7fe7e5d8-224d-4d9c-f8eb-3d3d0d5f1834"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3056494562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'final_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDv9TBR5ktTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}