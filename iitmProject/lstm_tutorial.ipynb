{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 17777,
          "databundleVersionId": 869809,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T13:41:40.321286Z",
          "iopub.execute_input": "2025-10-16T13:41:40.321578Z",
          "iopub.status.idle": "2025-10-16T13:41:40.717343Z",
          "shell.execute_reply.started": "2025-10-16T13:41:40.321555Z",
          "shell.execute_reply": "2025-10-16T13:41:40.716259Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "id": "tqokSKRNcMLw",
        "outputId": "313bf6b4-224d-492e-8e2f-854f1ec5cf1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SECTION 1: SETUP AND DATA LOADING\n",
        "# -----------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Deep Learning Imports\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Evaluation Imports (using scikit-learn for metrics)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Initialize NLTK components\n",
        "lemmatizer =\n",
        "STOPWORDS =\n",
        "PUNCTUATION =\n",
        "\n",
        "# --- File Paths ---\n",
        "TRAIN_PATH =\n",
        "TEST_PATH =\n",
        "SUB_PATH =\n",
        "\n",
        "print(\"1. Loading datasets...\")\n",
        "try:\n",
        "    df_train = pd.read_csv(TRAIN_PATH)\n",
        "    df_test = pd.read_csv(TEST_PATH)\n",
        "    df_submission = pd.read_csv(SUB_PATH)\n",
        "    print(\"   Data loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"   Error loading data: {e}. Please ensure files are in the correct path.\")\n",
        "    exit()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T13:41:42.851563Z",
          "iopub.execute_input": "2025-10-16T13:41:42.852438Z",
          "iopub.status.idle": "2025-10-16T13:42:04.927376Z",
          "shell.execute_reply.started": "2025-10-16T13:41:42.852403Z",
          "shell.execute_reply": "2025-10-16T13:42:04.925441Z"
        },
        "id": "6Vyc_SWhcML0",
        "outputId": "054a4188-0075-4d4a-bf5e-696329ab0874"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-16 13:41:46.772065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760622107.072744      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760622107.156546      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1. Loading datasets...\n   Data loaded successfully.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SECTION 2: EXPLORATORY DATA ANALYSIS\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n2. Initial Data Exploration:\")\n",
        "print(f\"   Training Data Shape: {}\")\n",
        "print(f\"   Test Data Shape: {}\")\n",
        "print(\"\\n--- Training Data Head ---\")\n",
        "print()\n",
        "\n",
        "target_counts =\n",
        "print(\"\\n--- Target Distribution (0=Not Disaster, 1=Disaster) ---\")\n",
        "print(target_counts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T13:43:12.203683Z",
          "iopub.execute_input": "2025-10-16T13:43:12.204077Z",
          "iopub.status.idle": "2025-10-16T13:43:12.214804Z",
          "shell.execute_reply.started": "2025-10-16T13:43:12.204040Z",
          "shell.execute_reply": "2025-10-16T13:43:12.212989Z"
        },
        "id": "PoOPlqP7cML2",
        "outputId": "73873c9c-d9e4-4c21-9137-141b6e948555"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n2. Initial Data Exploration:\n   Training Data Shape: (7613, 5)\n   Test Data Shape: (3263, 4)\n\n--- Training Data Head ---\n   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n\n--- Target Distribution (0=Not Disaster, 1=Disaster) ---\ntarget\n0    4342\n1    3271\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SECTION 3: TEXT PREPROCESSING & CLEANING (Slightly modified comments)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n3. Cleaning and Preprocessing Text Data...\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Performs comprehensive text cleaning for tweets.\"\"\"\n",
        "    # Normalize\n",
        "    text =\n",
        "    # Remove URLs ( Hint, use : 'https?://\\S+|www\\.\\S+')\n",
        "    text =\n",
        "    # Remove HTML special characters ( Hint, use : '&[a-z]+;')\n",
        "    text =\n",
        "    # Remove user mentions ( Hint, use : '@\\w+')\n",
        "    text =\n",
        "    # Remove punctuation\n",
        "    text =\n",
        "\n",
        "    # Tokenization, Stopword Removal, and Lemmatization\n",
        "    tokens =\n",
        "\n",
        "    return\n",
        "\n",
        "# Apply cleaning\n",
        "df_train['clean_text'] =\n",
        "df_test['clean_text'] =\n",
        "\n",
        "print(\"   Sample of cleaned text:\")\n",
        "print(df_train['clean_text'].head().to_string())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-10-16T13:27:58.848Z"
        },
        "id": "9aTX8KnNcML3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SECTION 4: DEEP LEARNING DATA PREPARATION (Tokenization & Padding)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n4. Deep Learning Data Preparation (Tokenization & Padding)...\")\n",
        "\n",
        "# Define our features (X) and target (y)\n",
        "X = df_train['clean_text']\n",
        "y = df_train['target'].values # Convert to numpy array for Keras\n",
        "\n",
        "# --- Hyperparameters for Tokenization ---\n",
        "VOCAB_SIZE =                    # Max number of unique words to keep. Try 10000, 20000, 30000\n",
        "MAX_LENGTH =                    # Max sequence length (words per tweet). Try 20, 30, 40\n",
        "OOV_TOKEN = \"<oov>\"             # Token for Out-Of-Vocabulary words\n",
        "\n",
        "# 1. Initialize and Fit Tokenizer\n",
        "# The tokenizer learns the vocabulary from the training text\n",
        "tokenizer =\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# 2. Convert text to sequences of integers\n",
        "train_sequences =\n",
        "test_sequences =\n",
        "\n",
        "# 3. Padding sequences\n",
        "# Pad all sequences to MAX_LENGTH, padding = 'post', truncating = 'post'\n",
        "X_padded =\n",
        "X_test_padded =\n",
        "\n",
        "print(f\"   Padded Sequence Shape (Train): {}\")\n",
        "print(f\"   Padded Sequence Shape (Test): {}\")\n",
        "print(f\"   Vocabulary Size used: {}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-10-16T13:27:58.848Z"
        },
        "id": "moSjthsPcML4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SECTION 5: DEEP LEARNING MODEL DEFINITION, TRAINING, AND EVALUATION\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n5. Deep Learning Model Training and Evaluation...\")\n",
        "\n",
        "# --- Splitting Data for Validation ---\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_padded, y,\n",
        "    test_size= , #  Try the split sizes 0.1, 0.2, 0.3\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"   Training on {X_train.shape[0]} samples, Validating on {X_val.shape[0]} samples.\")\n",
        "\n",
        "# --- Hyperparameters for DL Model ---\n",
        "EMBEDDING_DIM =     # Size of the word vector representation. Try 50, 100, 200\n",
        "LSTM_UNITS =        # Number of hidden units in the LSTM layer. Try 32, 64, 128\n",
        "EPOCHS =            # Number of full passes over the training data. Start low (5), try higher (20)\n",
        "BATCH_SIZE =        # Number of samples per gradient update. Try 16, 32, 64\n",
        "\n",
        "# --- Define the Sequential Model ---\n",
        "def create_lstm_model(vocab_size, embedding_dim, max_length, lstm_units):\n",
        "    model = Sequential([\n",
        "        # Layer 1: Embedding Layer\n",
        "        # Converts integer sequences to dense vectors (word embeddings)\n",
        "\n",
        "\n",
        "        # Layer 2: LSTM Layer\n",
        "        # Processes sequences, capturing long-term dependencies in the text\n",
        "\n",
        "\n",
        "        # Layer 3: Output Layer\n",
        "        # Single neuron with sigmoid activation for binary classification (0 or 1)\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss= , # Appropriate loss for binary classification\n",
        "        optimizer= ,           # Use a default optimizer\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Initialize and summarize the model\n",
        "dl_model = create_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH, LSTM_UNITS)\n",
        "print(\"\\n--- Deep Learning Model Summary ---\")\n",
        "dl_model.summary()\n",
        "#\n",
        "\n",
        "# --- Train the Model ---\n",
        "print(\"\\n   Starting model training...\")\n",
        "history = dl_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1 # Display training progress\n",
        ")\n",
        "print(\"   Model training complete.\")\n",
        "\n",
        "# --- Evaluate on Validation Set ---\n",
        "# Keras predictions output probabilities (0.0 to 1.0)\n",
        "y_val_pred_proba =\n",
        "# Convert probabilities to binary classes (0 or 1) using a threshold of 0.5\n",
        "y_val_pred =\n",
        "\n",
        "# Calculate key metrics\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"\\n--- Validation Results ---\")\n",
        "print(f\"   Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"   F1 Score (Target Metric): {val_f1:.4f}\")\n",
        "print(\"\\n--- Detailed Classification Report ---\")\n",
        "# Flatten y_val_pred for compatibility with classification_report\n",
        "print(classification_report(y_val, y_val_pred.flatten(), target_names=['Not Disaster (0)', 'Disaster (1)']))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-10-16T13:27:58.848Z"
        },
        "id": "8AemxB1FcML5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# SECTION 6: GENERATE FINAL PREDICTIONS FOR SUBMISSION\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n6. Generating Final Predictions...\")\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "final_predictions_proba =\n",
        "# Convert probabilities to binary (0 or 1)\n",
        "final_predictions =\n",
        "# Flatten the 2D array to a 1D series\n",
        "final_predictions = final_predictions.flatten()\n",
        "\n",
        "# Create the submission dataframe\n",
        "df_submission['target'] = final_predictions\n",
        "\n",
        "# Ensure the submission file only has the required columns: id and target\n",
        "submission_output = df_submission[['id', 'target']]\n",
        "\n",
        "# Save the submission file\n",
        "submission_output.to_csv('submission_lstm.csv', index=False)\n",
        "print(\"   Submission file 'submission_lstm.csv' created successfully.\")\n",
        "print(\"\\n--- Sample Submission Head ---\")\n",
        "print(submission_output.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-10-16T13:27:58.848Z"
        },
        "id": "i9gP7Tk7cML6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CLASS WRAP-UP: EXPERIMENTS TO TRY\n",
        "# =============================================================================\n",
        "# 1. MODEL ARCHITECTURE (Section 5):\n",
        "#    - Change the recurrent layer: Replace LSTM(64) with GRU(64) (GRU is generally faster).\n",
        "#    - Add Dropout: Insert a Dropout layer (e.g., Dropout(0.5)) after the Embedding layer to prevent overfitting.\n",
        "#    - Try Bidirectional: Wrap the LSTM layer in a Bidirectional layer (tf.keras.layers.Bidirectional(LSTM(64))).\n",
        "#\n",
        "# 2. HYPERPARAMETER TUNING (Section 4 & 5):\n",
        "#    - Adjust embedding_dim: Try EMBEDDING_DIM = 50 or 200.\n",
        "#    - Adjust LSTM_UNITS: Try LSTM_UNITS = 32 or 128.\n",
        "#    - Adjust EPOCHS: See how the training and validation loss/accuracy change after 5, 10, or 20 epochs.\n",
        "#\n",
        "# 3. DATA PREPARATION (Section 4):\n",
        "#    - Adjust sequence length: Try MAX_LENGTH = 20 or 40.\n",
        "#\n",
        "# This Deep Learning pipeline should yield a competitive score!\n",
        "# Happy coding!"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-10-16T13:27:58.848Z"
        },
        "id": "Kf1peKQDcML7"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}