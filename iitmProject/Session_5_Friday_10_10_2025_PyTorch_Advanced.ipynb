{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67ac477b",
      "metadata": {
        "id": "67ac477b"
      },
      "source": [
        "### Architecture Flow\n",
        "\n",
        "## **What Does This Architecture Do?**\n",
        "\n",
        "This architecture diagram illustrates the complete pipeline of a multi-label emotion classification system using BERT (Bidirectional Encoder Representations from Transformers). It processes raw text input and outputs predictions for multiple emotions simultaneously.\n",
        "\n",
        "**Key Components:**\n",
        "- **Input Processing**: Raw text → Tokenized sequences\n",
        "- **Feature Extraction**: BERT model extracts contextual embeddings\n",
        "- **Classification**: Custom head maps embeddings to emotion probabilities\n",
        "- **Output**: Binary predictions for 5 emotions (anger, fear, joy, love, sadness)\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Is This Architecture Necessary?**\n",
        "\n",
        "### **1. Multi-Label Classification Challenge**\n",
        "Unlike single-label classification (where text belongs to ONE category), emotions can co-exist. A sentence like *\"I'm excited but nervous about the interview\"* contains both **joy** and **fear**. This architecture handles such complexity.\n",
        "\n",
        "### **2. Contextual Understanding**\n",
        "Traditional models treat words independently, missing context. BERT's transformer layers capture:\n",
        "- **Bidirectional context**: Words understand both left and right neighbors\n",
        "- **Semantic relationships**: Distinguishes \"I love this!\" (positive) from \"I'd love to leave\" (sarcasm/negative)\n",
        "\n",
        "### **3. Transfer Learning Efficiency**\n",
        "BERT is pre-trained on massive text corpora, giving it:\n",
        "- **Language understanding** out-of-the-box\n",
        "- **Reduced training time** (we only fine-tune, not train from scratch)\n",
        "- **Better performance** with less data\n",
        "\n",
        "### **4. Regularization & Overfitting Prevention**\n",
        "- **Dropout layers** prevent the model from memorizing training data\n",
        "- **Sigmoid activation** allows independent emotion predictions (not mutually exclusive)\n",
        "\n",
        "---\n",
        "\n",
        "## **How Should This Architecture Be Implemented?**\n",
        "\n",
        "### **Step-by-Step Implementation Guide**\n",
        "\n",
        "#### **1. Input Processing (Tokenization)**\n",
        "```python\n",
        "# Convert text to BERT-compatible format\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "# Returns: input_ids (token IDs) + attention_mask (padding indicators)\n",
        "```\n",
        "\n",
        "**Why?** BERT requires fixed-length numerical inputs with special tokens ([CLS], [SEP]).\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Embedding Layer**\n",
        "```python\n",
        "# BERT's first layer converts token IDs to 768-dimensional vectors\n",
        "embeddings = bert_model.embeddings(input_ids)\n",
        "```\n",
        "\n",
        "**Why?** Neural networks process numbers, not words. Embeddings capture semantic meaning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Transformer Layers (12 in BERT-base)**\n",
        "```python\n",
        "# Each layer applies self-attention + feed-forward networks\n",
        "for layer in bert_model.encoder.layer:\n",
        "    hidden_states = layer(hidden_states, attention_mask)\n",
        "```\n",
        "\n",
        "**Why?** Multi-head attention learns which words relate to each other (e.g., \"not happy\" → focus on \"not\").\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Extract [CLS] Token**\n",
        "```python\n",
        "# [CLS] token (first position) aggregates sentence-level information\n",
        "cls_output = hidden_states[:, 0, :]  # Shape: (batch_size, 768)\n",
        "```\n",
        "\n",
        "**Why?** This special token is designed to represent the entire sentence for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Dropout for Regularization**\n",
        "```python\n",
        "# Randomly drop 30% of neurons during training\n",
        "dropout_output = nn.Dropout(0.3)(cls_output)\n",
        "```\n",
        "\n",
        "**Why?** Prevents overfitting by forcing the model to not rely on specific neurons.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Classification Head**\n",
        "```python\n",
        "# Linear layer maps 768 features to 5 emotion scores\n",
        "logits = nn.Linear(768, 5)(dropout_output)\n",
        "```\n",
        "\n",
        "**Why?** Reduces dimensionality to match the number of target emotions.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Sigmoid Activation**\n",
        "```python\n",
        "# Convert logits to probabilities (0-1 range)\n",
        "probabilities = torch.sigmoid(logits)\n",
        "```\n",
        "\n",
        "**Why?** Unlike softmax (used for single-label), sigmoid treats each emotion independently.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Thresholding for Predictions**\n",
        "```python\n",
        "# Predict emotion if probability > 0.5\n",
        "predictions = (probabilities > 0.5).int()\n",
        "```\n",
        "\n",
        "**Why?** Converts probabilities to binary decisions. Threshold (0.5) can be tuned for precision/recall trade-off.\n",
        "\n",
        "---\n",
        "\n",
        "### **Complete Architecture Diagram**\n",
        "\n",
        "```\n",
        "Raw Text Input\n",
        "    ↓\n",
        "Tokenization (BERT Tokenizer)\n",
        "    ↓\n",
        "Token IDs + Attention Mask (Tensors)\n",
        "    ↓\n",
        "Embedding Layer (BERT)\n",
        "    ↓\n",
        "12 Transformer Layers (Self-Attention + FFN)\n",
        "    ↓\n",
        "[CLS] Token Representation (768-dim)\n",
        "    ↓\n",
        "Dropout Layer (Regularization)\n",
        "    ↓\n",
        "Linear Classification Head (768 → 5)\n",
        "    ↓\n",
        "Logits (Raw Scores)\n",
        "    ↓\n",
        "Sigmoid Activation\n",
        "    ↓\n",
        "Probabilities (0-1 per emotion)\n",
        "    ↓\n",
        "Thresholding (> 0.5)\n",
        "    ↓\n",
        "Binary Predictions\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f58102",
      "metadata": {
        "id": "95f58102"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292b2104",
      "metadata": {
        "id": "292b2104"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `import torch`**\n",
        "\n",
        "**What**: Imports the main PyTorch library.\n",
        "\n",
        "**Why**: PyTorch is the foundation for all tensor operations, automatic differentiation, and neural network training. We need it for creating and manipulating tensors, which are the fundamental data structures in deep learning.\n",
        "\n",
        "**How**: This makes the entire `torch` namespace available, allowing access to functions like `torch.tensor()`, `torch.device()`, and `torch.no_grad()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: `import torch.nn as nn`**\n",
        "\n",
        "**What**: Imports PyTorch's neural network module with the alias `nn`.\n",
        "\n",
        "**Why**: The `torch.nn` module contains all building blocks for neural networks including layers, loss functions, and activation functions. The alias `nn` is a standard convention that makes code more concise and readable.\n",
        "\n",
        "**How**: By importing as `nn`, we can write `nn.Linear()` instead of `torch.nn.Linear()`, which is cleaner and follows community standards.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `from torch.utils.data import Dataset, DataLoader`**\n",
        "\n",
        "**What**: Imports two specific classes from PyTorch's data utilities module.\n",
        "\n",
        "**Why**:\n",
        "- `Dataset`: Abstract base class for creating custom datasets. We will inherit from this to build our emotion dataset.\n",
        "- `DataLoader`: Utility for batching, shuffling, and loading data efficiently during training.\n",
        "\n",
        "**How**: Using `from ... import` allows us to directly use `Dataset` and `DataLoader` without the `torch.utils.data.` prefix, making the code cleaner.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `from transformers import AutoTokenizer, AutoModel`**\n",
        "\n",
        "**What**: Imports automatic model and tokenizer classes from Hugging Face transformers library.\n",
        "\n",
        "**Why**:\n",
        "- `AutoTokenizer`: Automatically selects the correct tokenizer for a given model name, handling the complexity of different tokenization schemes.\n",
        "- `AutoModel`: Automatically loads the appropriate transformer architecture based on the model name.\n",
        "\n",
        "**How**: These \"Auto\" classes use model configuration files to determine which specific tokenizer or model class to instantiate, providing a uniform interface across different transformer models.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `import pandas as pd`**\n",
        "\n",
        "**What**: Imports the pandas library with the standard alias `pd`.\n",
        "\n",
        "**Why**: Pandas provides DataFrame structures that are ideal for handling tabular data. We use it to organize our text and labels in a structured format that is easy to manipulate and pass to our dataset class.\n",
        "\n",
        "**How**: The alias `pd` is the universally accepted convention, allowing us to write `pd.DataFrame()` instead of `pandas.DataFrame()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `import numpy as np`**\n",
        "\n",
        "**What**: Imports the NumPy library with the standard alias `np`.\n",
        "\n",
        "**Why**: NumPy provides efficient array operations and is the foundation for numerical computing in Python. We use it for stacking predictions and performing array manipulations during evaluation.\n",
        "\n",
        "**How**: The alias `np` is the standard convention, making code more concise when using functions like `np.vstack()` or `np.array()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score`**\n",
        "\n",
        "**What**: Imports four specific evaluation metric functions from scikit-learn.\n",
        "\n",
        "**Why**:\n",
        "- These metrics are essential for evaluating multi-label classification performance\n",
        "- `accuracy_score`: Measures the proportion of correct predictions\n",
        "- `f1_score`: Harmonic mean of precision and recall, balancing both metrics\n",
        "- `precision_score`: Measures how many predicted positives are actually positive\n",
        "- `recall_score`: Measures how many actual positives were correctly predicted\n",
        "\n",
        "**How**: These functions accept true labels and predictions as arrays and compute the respective metrics, handling both binary and multi-label cases.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: `from typing import Dict, List, Optional`**\n",
        "\n",
        "**What**: Imports type hint classes from Python's typing module.\n",
        "\n",
        "**Why**: Type hints improve code readability and enable static type checking. They document what types of data functions expect and return, making the code self-documenting and catching potential bugs early.\n",
        "\n",
        "**How**:\n",
        "- `Dict`: Type hint for dictionaries, used as `Dict[str, int]` to specify key and value types\n",
        "- `List`: Type hint for lists, used as `List[str]` to specify element types\n",
        "- `Optional`: Indicates a value can be of a specified type or None\n",
        "\n",
        "---\n",
        "\n",
        "**Line 9: `from dataclasses import dataclass`**\n",
        "\n",
        "**What**: Imports the dataclass decorator from Python's dataclasses module.\n",
        "\n",
        "**Why**: Dataclasses automatically generate special methods like `__init__()` for classes that primarily store data. This reduces boilerplate code and makes our DataCollator class cleaner.\n",
        "\n",
        "**How**: When we decorate a class with `@dataclass`, Python automatically creates an initializer and other methods based on the class attributes we define.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 10: `import warnings`**\n",
        "\n",
        "**What**: Imports Python's warnings module for managing warning messages.\n",
        "\n",
        "**Why**: We import this to control warning outputs that might clutter our notebook, especially deprecation warnings from libraries.\n",
        "\n",
        "**How**: The warnings module provides functions to filter, suppress, or customize how warnings are displayed.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 11: (blank line)**\n",
        "\n",
        "**What**: Empty line for code readability.\n",
        "\n",
        "**Why**: Following PEP 8 style guidelines, blank lines separate import statements from subsequent code, improving visual organization.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 12: `warnings.filterwarnings('ignore')`**\n",
        "\n",
        "**What**: Configures the warnings module to suppress all warning messages.\n",
        "\n",
        "**Why**: During training, various libraries may emit warnings about deprecations or optimizations. While these are sometimes useful, they can clutter notebook output and distract from learning objectives.\n",
        "\n",
        "**How**: The `filterwarnings()` function accepts an action parameter. The `'ignore'` action tells Python to suppress all warnings. In production code, you would typically want to be more selective about which warnings to ignore."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab705ba",
      "metadata": {
        "id": "5ab705ba"
      },
      "source": [
        "## Section 3: Data Synthesis\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4592be28",
      "metadata": {
        "id": "4592be28"
      },
      "outputs": [],
      "source": [
        "synthetic_data = {\n",
        "    'id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    'text': [\n",
        "        \"I was extremely disappointed with the customer service at the restaurant.\",\n",
        "        \"Walking alone at night in an unfamiliar neighborhood made me nervous.\",\n",
        "        \"Receiving the promotion I worked hard for filled me with happiness.\",\n",
        "        \"The news about my friend's illness left me feeling devastated and concerned.\",\n",
        "        \"I never expected to see my childhood friend at the conference today.\",\n",
        "        \"The project deadline is approaching and I am worried about finishing on time.\",\n",
        "        \"Celebrating my graduation with family brought immense satisfaction.\",\n",
        "        \"The unexpected cancellation of the event frustrated me greatly.\",\n",
        "        \"Reading the final chapter of my favorite book series was bittersweet.\",\n",
        "        \"The presentation went smoothly and I felt confident throughout.\"\n",
        "    ],\n",
        "    'anger': [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
        "    'fear': [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
        "    'joy': [0, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
        "    'sadness': [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "    'surprise': [0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(synthetic_data)\n",
        "print(\"Dataset Overview:\")\n",
        "print(df.head())\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nEmotion distribution:\")\n",
        "print(df[['anger', 'fear', 'joy', 'sadness', 'surprise']].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9057e921",
      "metadata": {
        "id": "9057e921"
      },
      "source": [
        "## Section 4: Train-Test Split\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f05e107",
      "metadata": {
        "id": "8f05e107"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(df))\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:]\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5c1c53",
      "metadata": {
        "id": "ea5c1c53"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `train_size = int(0.8 * len(df))`**\n",
        "\n",
        "**What**: Calculates the number of samples to use for training (80% of total).\n",
        "\n",
        "**Why**: We need to split data into training and validation sets to evaluate model performance on unseen data. Using 80% for training and 20% for validation is a common convention that balances having enough training data while maintaining a meaningful validation set.\n",
        "\n",
        "**How**:\n",
        "- `len(df)` returns the number of rows in the DataFrame (10)\n",
        "- `0.8 * len(df)` calculates 80% of rows (10 * 0.8 = 8.0)\n",
        "- `int()` converts the float to an integer (8.0 → 8)\n",
        "- The result is stored in `train_size`\n",
        "\n",
        "**Why int() is necessary**: Array slicing requires integer indices. Without `int()`, we would have a float, which cannot be used for slicing.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: `train_df = df[:train_size]`**\n",
        "\n",
        "**What**: Creates the training DataFrame by selecting the first `train_size` rows.\n",
        "\n",
        "**Why**: This separates our training data from validation data. The training set is what the model learns from during the training process.\n",
        "\n",
        "**How**:\n",
        "- `df[:train_size]` uses slice notation to select rows from index 0 up to (but not including) index `train_size`\n",
        "- With `train_size = 8`, this selects rows 0-7 (8 total rows)\n",
        "- The resulting DataFrame is assigned to `train_df`\n",
        "\n",
        "**Important Note**: This is a simple split without shuffling. In production, you would typically shuffle before splitting to avoid any ordering bias.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `val_df = df[train_size:]`**\n",
        "\n",
        "**What**: Creates the validation DataFrame by selecting all rows from `train_size` onwards.\n",
        "\n",
        "**Why**: The validation set is used to evaluate model performance on data it has not seen during training, helping detect overfitting and assess generalization.\n",
        "\n",
        "**How**:\n",
        "- `df[train_size:]` selects from index `train_size` to the end of the DataFrame\n",
        "- With `train_size = 8`, this selects rows 8-9 (2 total rows)\n",
        "- The resulting DataFrame is assigned to `val_df`\n",
        "\n",
        "**Split Visualization**:\n",
        "```\n",
        "Original df (10 rows): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "                        |__________________|  |______|\n",
        "                           train_df (8)      val_df (2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: (blank line)**\n",
        "\n",
        "**What**: Empty line for code organization.\n",
        "\n",
        "**Why**: Separates the data splitting logic from the output display logic, improving readability.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `print(f\"Training samples: {len(train_df)}\")`**\n",
        "\n",
        "**What**: Prints the number of samples in the training set.\n",
        "\n",
        "**Why**: Verifying the split worked correctly is important. This also documents the dataset sizes for anyone reading the notebook.\n",
        "\n",
        "**How**:\n",
        "- `len(train_df)` counts the rows in the training DataFrame (8)\n",
        "- The f-string formats and displays this count\n",
        "- Expected output: \"Training samples: 8\"\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `print(f\"Validation samples: {len(val_df)}\")`**\n",
        "\n",
        "**What**: Prints the number of samples in the validation set.\n",
        "\n",
        "**Why**: Completes the verification of the data split, confirming we have the expected 2 validation samples (20% of 10).\n",
        "\n",
        "**How**:\n",
        "- `len(val_df)` counts the rows in the validation DataFrame (2)\n",
        "- Expected output: \"Validation samples: 2\"\n",
        "\n",
        "**Educational Discussion Points**:\n",
        "1. **Why 80-20 split?**: Common ratios are 80-20, 70-30, or 60-20-20 (train-val-test). Smaller datasets might use 70-30 to have more validation data.\n",
        "2. **Why not use test set here?**: With only 10 samples, adding a test set would leave too little data for training and validation. In real projects, you would have train-val-test splits.\n",
        "3. **Stratification**: For imbalanced datasets, you would use stratified splitting to maintain class distribution across splits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8417ee",
      "metadata": {
        "id": "6b8417ee"
      },
      "source": [
        "## Section 5: Tokenization\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c33ca8e",
      "metadata": {
        "id": "6c33ca8e"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "sample_text = df['text'].iloc[0]\n",
        "sample_encoding = tokenizer(\n",
        "    sample_text,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(\"Sample tokenization:\")\n",
        "print(f\"Original text: {sample_text}\")\n",
        "print(f\"\\nToken IDs shape: {sample_encoding['input_ids'].shape}\")\n",
        "print(f\"Token IDs: {sample_encoding['input_ids'][0][:20]}...\")\n",
        "print(f\"\\nDecoded tokens: {tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'][0][:20])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0139b800",
      "metadata": {
        "id": "0139b800"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `MODEL_NAME = 'bert-base-uncased'`**\n",
        "\n",
        "**What**: Defines a constant string specifying which pretrained model to use.\n",
        "\n",
        "**Why**:\n",
        "- BERT (Bidirectional Encoder Representations from Transformers) is a powerful language model trained on massive text corpora\n",
        "- 'base' indicates the standard size (12 layers, 768 hidden units)\n",
        "- 'uncased' means the model does not distinguish between uppercase and lowercase letters\n",
        "- Using a constant makes it easy to switch models by changing one line\n",
        "\n",
        "**How**: By convention, constants are written in UPPERCASE. This string will be used to download both the tokenizer and model from Hugging Face's model hub.\n",
        "\n",
        "**Model Choice Considerations**:\n",
        "- **bert-base-uncased**: Good for general English text, faster than large models\n",
        "- **bert-base-cased**: Use when capitalization matters (e.g., names, acronyms)\n",
        "- **bert-large-uncased**: Better performance but requires more memory and compute\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: `tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)`**\n",
        "\n",
        "**What**: Downloads and initializes the tokenizer associated with BERT-base-uncased.\n",
        "\n",
        "**Why**: Neural networks cannot process raw text directly. Tokenizers convert text into numerical tokens that models can understand. Using the pretrained tokenizer ensures compatibility with the pretrained model's vocabulary.\n",
        "\n",
        "**How**:\n",
        "- `AutoTokenizer.from_pretrained()` automatically determines the correct tokenizer class for the specified model\n",
        "- It downloads the tokenizer configuration and vocabulary file from Hugging Face Hub\n",
        "- These files are cached locally to avoid re-downloading\n",
        "- The tokenizer instance is stored in the `tokenizer` variable\n",
        "\n",
        "**What gets downloaded**:\n",
        "1. `tokenizer_config.json`: Configuration parameters\n",
        "2. `vocab.txt`: Vocabulary mapping words to IDs (30,522 tokens for BERT-base)\n",
        "3. Special token definitions ([CLS], [SEP], [PAD], etc.)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `sample_text = df['text'].iloc[0]`**\n",
        "\n",
        "**What**: Extracts the first text sample from the DataFrame for demonstration.\n",
        "\n",
        "**Why**: We want to show how tokenization works on a concrete example before applying it to the entire dataset.\n",
        "\n",
        "**How**:\n",
        "- `df['text']` selects the 'text' column, returning a Series\n",
        "- `.iloc[0]` uses integer-location indexing to get the first element\n",
        "- The result is the string: \"I was extremely disappointed with the customer service at the restaurant.\"\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `sample_encoding = tokenizer(`**\n",
        "\n",
        "**What**: Begins a function call to tokenize the sample text.\n",
        "\n",
        "**Why**: The tokenizer is callable like a function, providing a convenient interface for text processing.\n",
        "\n",
        "**How**: The parentheses indicate the start of a multi-line function call with multiple arguments.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `    sample_text,`**\n",
        "\n",
        "**What**: First positional argument - the text to tokenize.\n",
        "\n",
        "**Why**: This is the required input that will be converted to tokens.\n",
        "\n",
        "**How**: The tokenizer will split this text into subword tokens using BERT's WordPiece algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `    padding='max_length',`**\n",
        "\n",
        "**What**: Specifies padding strategy to pad sequences to maximum length.\n",
        "\n",
        "**Why**: Neural networks require fixed-size inputs for batch processing. Padding ensures all sequences have the same length by adding special [PAD] tokens.\n",
        "\n",
        "**How**:\n",
        "- If the sequence is shorter than `max_length`, [PAD] tokens are appended\n",
        "- If longer, it will be truncated (controlled by the truncation parameter)\n",
        "\n",
        "**Alternative values**:\n",
        "- `'longest'`: Pad to longest sequence in the batch\n",
        "- `'max_length'`: Pad to specified max_length\n",
        "- `False`: No padding (sequences remain different lengths)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: `    truncation=True,`**\n",
        "\n",
        "**What**: Enables truncation of sequences exceeding maximum length.\n",
        "\n",
        "**Why**: BERT has a maximum position embedding of 512 tokens. Processing longer sequences would cause errors. Truncation prevents this by cutting off excess tokens.\n",
        "\n",
        "**How**: If a tokenized sequence exceeds `max_length`, it is trimmed to fit, keeping the first `max_length` tokens.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 9: `    max_length=128,`**\n",
        "\n",
        "**What**: Sets the maximum sequence length to 128 tokens.\n",
        "\n",
        "**Why**:\n",
        "- Our texts are relatively short, so 128 is sufficient\n",
        "- Shorter sequences train faster and use less memory\n",
        "- BERT's maximum is 512, but using smaller lengths when possible improves efficiency\n",
        "\n",
        "**How**: This works in conjunction with `padding` and `truncation` to ensure all outputs are exactly 128 tokens long.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 10: `    return_tensors='pt'`**\n",
        "\n",
        "**What**: Specifies that outputs should be PyTorch tensors.\n",
        "\n",
        "**Why**: PyTorch models require tensor inputs, not lists or NumPy arrays. This parameter ensures the output is in the correct format.\n",
        "\n",
        "**How**:\n",
        "- `'pt'`: Returns PyTorch tensors\n",
        "- `'tf'`: Would return TensorFlow tensors\n",
        "- `'np'`: Would return NumPy arrays\n",
        "- `None` (default): Returns Python lists\n",
        "\n",
        "---\n",
        "\n",
        "**Line 11: `)`**\n",
        "\n",
        "**What**: Closes the tokenizer function call.\n",
        "\n",
        "**Why**: Completes the multi-line function invocation started on line 5.\n",
        "\n",
        "**How**: The tokenizer processes the input and returns a dictionary containing:\n",
        "- `input_ids`: Token IDs\n",
        "- `attention_mask`: Mask indicating real tokens (1) vs padding (0)\n",
        "- `token_type_ids`: Segment IDs (for BERT's dual-sequence input)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 12: (blank line)**\n",
        "\n",
        "**What**: Empty line for readability.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 13: `print(\"Sample tokenization:\")`**\n",
        "\n",
        "**What**: Prints a header for the tokenization demonstration.\n",
        "\n",
        "**Why**: Provides context for the output that follows.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 14: `print(f\"Original text: {sample_text}\")`**\n",
        "\n",
        "**What**: Displays the original text before tokenization.\n",
        "\n",
        "**Why**: Allows comparison between input text and tokenized output, helping students understand the transformation.\n",
        "\n",
        "**How**: F-string substitutes the `sample_text` variable into the output string.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 15: `print(f\"\\nToken IDs shape: {sample_encoding['input_ids'].shape}\")`**\n",
        "\n",
        "**What**: Prints the shape of the token ID tensor.\n",
        "\n",
        "**Why**: Understanding tensor shapes is crucial in deep learning. This shows the batch dimension and sequence length.\n",
        "\n",
        "**How**:\n",
        "- `sample_encoding['input_ids']` accesses the token IDs tensor\n",
        "- `.shape` returns the dimensions as a `torch.Size` object\n",
        "- Expected output: `torch.Size([1, 128])` meaning 1 sample with 128 tokens\n",
        "\n",
        "---\n",
        "\n",
        "**Line 16: `print(f\"Token IDs: {sample_encoding['input_ids'][0][:20]}...\")`**\n",
        "\n",
        "**What**: Displays the first 20 token IDs.\n",
        "\n",
        "**Why**: Showing all 128 tokens would clutter the output. The first 20 tokens demonstrate how text converts to numbers.\n",
        "\n",
        "**How**:\n",
        "- `sample_encoding['input_ids'][0]` gets the first (and only) sequence\n",
        "- `[:20]` slices to get the first 20 token IDs\n",
        "- `...` indicates truncation for display purposes\n",
        "\n",
        "**Example output**: `tensor([  101,  1045,  2001,  3533, ...])`\n",
        "- 101 is [CLS] (classification token)\n",
        "- Subsequent numbers represent words/subwords\n",
        "\n",
        "---\n",
        "\n",
        "**Line 17: `print(f\"\\nDecoded tokens: {tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'][0][:20])}\")`**\n",
        "\n",
        "**What**: Converts the first 20 token IDs back to their string representations.\n",
        "\n",
        "**Why**: This bridges the gap between numbers (what the model sees) and text (what humans understand), making tokenization concrete and understandable.\n",
        "\n",
        "**How**:\n",
        "- `tokenizer.convert_ids_to_tokens()` reverses the tokenization process\n",
        "- Takes a list/tensor of IDs and returns corresponding token strings\n",
        "- Expected output includes special tokens like [CLS] and subword tokens like '##ed'\n",
        "\n",
        "**Example output**: `['[CLS]', 'i', 'was', 'extremely', 'disappointed', 'with', 'the', 'customer', 'service', ...]`\n",
        "\n",
        "**Key Concepts to Highlight**:\n",
        "1. **WordPiece tokenization**: Splits unknown words into known subwords (e.g., \"disappointment\" → \"disappoint\", \"##ment\")\n",
        "2. **Special tokens**: [CLS] at start, [SEP] at end, [PAD] for padding\n",
        "3. **Attention mask**: Tells model which tokens are real (1) vs padding (0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c06beaa",
      "metadata": {
        "id": "9c06beaa"
      },
      "source": [
        "## Section 6: Custom Dataset Class\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08681b95",
      "metadata": {
        "id": "08681b95"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 128):\n",
        "        self.texts = dataframe['text'].values\n",
        "        self.labels = dataframe[['anger', 'fear', 'joy', 'sadness', 'surprise']].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        text = str(self.texts[idx])\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "train_dataset = EmotionDataset(train_df, tokenizer)\n",
        "val_dataset = EmotionDataset(val_df, tokenizer)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"\\nSample from dataset:\")\n",
        "sample = train_dataset[0]\n",
        "for key, value in sample.items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5974dd",
      "metadata": {
        "id": "7d5974dd"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `class EmotionDataset(Dataset):`**\n",
        "\n",
        "**What**: Defines a new class that inherits from PyTorch's Dataset base class.\n",
        "\n",
        "**Why**: PyTorch's DataLoader requires datasets to implement specific methods (`__len__` and `__getitem__`). By inheriting from Dataset, we get a standardized interface and access to PyTorch's data loading utilities.\n",
        "\n",
        "**How**:\n",
        "- `class EmotionDataset` names our custom class\n",
        "- `(Dataset)` specifies inheritance from the imported Dataset class\n",
        "- The colon `:` begins the class definition\n",
        "\n",
        "**Inheritance Benefits**:\n",
        "1. Standardized interface expected by DataLoader\n",
        "2. Type checking and IDE support\n",
        "3. Integration with PyTorch's ecosystem\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: (blank line)**\n",
        "\n",
        "**What**: Empty line after class declaration.\n",
        "\n",
        "**Why**: PEP 8 style guideline for class readability.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 128):`**\n",
        "\n",
        "**What**: Defines the constructor method with type hints.\n",
        "\n",
        "**Why**: The constructor initializes the dataset with necessary components. It runs once when creating a dataset instance, setting up everything needed for data access.\n",
        "\n",
        "**How**:\n",
        "- `def __init__` is Python's special method for object initialization\n",
        "- `self` is the instance reference (automatically passed)\n",
        "- `dataframe: pd.DataFrame` expects a pandas DataFrame with type hint\n",
        "- `tokenizer` is the pretrained tokenizer (no type hint as it could be various types)\n",
        "- `max_length: int = 128` has a default value of 128\n",
        "\n",
        "**Parameter Purposes**:\n",
        "- `dataframe`: Contains text and labels\n",
        "- `tokenizer`: Converts text to tokens\n",
        "- `max_length`: Controls sequence padding/truncation\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `        self.texts = dataframe['text'].values`**\n",
        "\n",
        "**What**: Extracts text column as a NumPy array and stores as instance variable.\n",
        "\n",
        "**Why**:\n",
        "- Converting to `.values` (NumPy array) is faster than accessing DataFrame rows repeatedly\n",
        "- Storing in `self.texts` makes it accessible in other methods\n",
        "- NumPy arrays support efficient integer indexing\n",
        "\n",
        "**How**:\n",
        "- `dataframe['text']` selects the text column\n",
        "- `.values` converts the Series to a NumPy array\n",
        "- `self.texts =` stores it as an instance attribute\n",
        "\n",
        "**Performance consideration**: Pre-extracting data prevents DataFrame overhead during training iterations.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `        self.labels = dataframe[['anger', 'fear', 'joy', 'sadness', 'surprise']].values`**\n",
        "\n",
        "**What**: Extracts emotion columns as a 2D NumPy array.\n",
        "\n",
        "**Why**: Labels must be in numerical form for loss calculation. Extracting as an array creates a matrix where each row is a sample and each column is an emotion.\n",
        "\n",
        "**How**:\n",
        "- Double brackets `[[...]]` select multiple columns, returning a DataFrame\n",
        "- `.values` converts to a 2D NumPy array of shape (n_samples, 5)\n",
        "- Each row contains binary values for the 5 emotions\n",
        "\n",
        "**Shape**: For 8 training samples, this creates an (8, 5) array.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `        self.tokenizer = tokenizer`**\n",
        "\n",
        "**What**: Stores the tokenizer as an instance variable.\n",
        "\n",
        "**Why**: We need the tokenizer in `__getitem__` to process text on-the-fly. Storing it avoids passing it repeatedly.\n",
        "\n",
        "**How**: Simple assignment makes the tokenizer accessible throughout the class.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `        self.max_length = max_length`**\n",
        "\n",
        "**What**: Stores the maximum sequence length.\n",
        "\n",
        "**Why**: This parameter is needed when tokenizing in `__getitem__`. Storing it as an attribute makes it reusable.\n",
        "\n",
        "**How**: The value (default 128) is saved for use in tokenization calls.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: (blank line)**\n",
        "\n",
        "**Why**: Separates constructor from other methods.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 9: `    def __len__(self) -> int:`**\n",
        "\n",
        "**What**: Defines the special method that returns dataset length.\n",
        "\n",
        "**Why**: DataLoader calls this to determine how many samples exist. Required method for PyTorch Dataset interface.\n",
        "\n",
        "**How**:\n",
        "- `__len__` is a Python magic method that enables `len(dataset)`\n",
        "- `-> int` is a type hint indicating it returns an integer\n",
        "- Must return the total number of samples\n",
        "\n",
        "---\n",
        "\n",
        "**Line 10: `        return len(self.texts)`**\n",
        "\n",
        "**What**: Returns the number of text samples.\n",
        "\n",
        "**Why**: This tells PyTorch how many items can be retrieved via `__getitem__`.\n",
        "\n",
        "**How**: `len(self.texts)` counts elements in the texts array, which equals the number of samples.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 11: (blank line)**\n",
        "\n",
        "**Why**: Separates methods for readability.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 12: `    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:`**\n",
        "\n",
        "**What**: Defines the special method for retrieving a single sample by index.\n",
        "\n",
        "**Why**:\n",
        "- This is the core of the Dataset class\n",
        "- DataLoader calls this repeatedly to fetch batches\n",
        "- Enables indexing like `dataset[0]` to get the first sample\n",
        "\n",
        "**How**:\n",
        "- `__getitem__` is Python's indexing magic method\n",
        "- `idx: int` is the sample index to retrieve\n",
        "- `-> Dict[str, torch.Tensor]` indicates it returns a dictionary of tensors\n",
        "\n",
        "---\n",
        "\n",
        "**Line 13: `        text = str(self.texts[idx])`**\n",
        "\n",
        "**What**: Retrieves the text at position idx and converts to string.\n",
        "\n",
        "**Why**:\n",
        "- We need the text for tokenization\n",
        "- `str()` ensures the value is a string (defensive programming)\n",
        "- Some DataFrame operations might return non-string types\n",
        "\n",
        "**How**:\n",
        "- `self.texts[idx]` uses NumPy array indexing to get element at position idx\n",
        "- `str()` wraps it for type safety\n",
        "\n",
        "---\n",
        "\n",
        "**Line 14: `        labels = self.labels[idx]`**\n",
        "\n",
        "**What**: Retrieves the label array for this sample.\n",
        "\n",
        "**Why**: We need the corresponding emotion labels for supervised learning.\n",
        "\n",
        "**How**:\n",
        "- `self.labels[idx]` indexes into the 2D array\n",
        "- Returns a 1D array of 5 binary values\n",
        "- Example: `[1, 0, 1, 0, 0]` for anger and joy\n",
        "\n",
        "---\n",
        "\n",
        "**Line 15: (blank line)**\n",
        "\n",
        "**Why**: Separates data retrieval from tokenization logic.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 16-22: Tokenization block**\n",
        "\n",
        "**What**: Tokenizes the text with specific parameters.\n",
        "\n",
        "**Why**: Must be done here (not in __init__) to handle dynamic batching and maintain flexibility.\n",
        "\n",
        "**How**: Same as earlier tokenization, but now applied to individual samples during iteration.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 24: `        return {`**\n",
        "\n",
        "**What**: Begins returning a dictionary of processed data.\n",
        "\n",
        "**Why**: Returning a dictionary makes it easy to access different components by name in training loops.\n",
        "\n",
        "**How**: Dictionary keys will be used to access specific tensors.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 25: `            'input_ids': encoding['input_ids'].flatten(),`**\n",
        "\n",
        "**What**: Includes token IDs, flattened to 1D.\n",
        "\n",
        "**Why**:\n",
        "- `encoding['input_ids']` has shape (1, 128) from `return_tensors='pt'`\n",
        "- `.flatten()` converts to shape (128,) which is cleaner for batching\n",
        "- DataLoader will stack these into (batch_size, 128)\n",
        "\n",
        "**How**: `.flatten()` removes the extra batch dimension added by the tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 26: `            'attention_mask': encoding['attention_mask'].flatten(),`**\n",
        "\n",
        "**What**: Includes attention mask, also flattened.\n",
        "\n",
        "**Why**: The attention mask tells the model which tokens are real (1) vs padding (0). Essential for transformer models.\n",
        "\n",
        "**How**: Same flattening operation as input_ids.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 27: `            'labels': torch.tensor(labels, dtype=torch.float)`**\n",
        "\n",
        "**What**: Converts NumPy label array to PyTorch float tensor.\n",
        "\n",
        "**Why**:\n",
        "- PyTorch models require tensor inputs, not NumPy arrays\n",
        "- `dtype=torch.float` is necessary for BCEWithLogitsLoss\n",
        "- Binary Cross Entropy expects float targets, not integers\n",
        "\n",
        "**How**: `torch.tensor()` creates a new tensor from the NumPy array with specified data type.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 28: `        }`**\n",
        "\n",
        "**What**: Closes the return dictionary.\n",
        "\n",
        "**Why**: Completes the data structure being returned.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 30: `train_dataset = EmotionDataset(train_df, tokenizer)`**\n",
        "\n",
        "**What**: Creates a dataset instance for training data.\n",
        "\n",
        "**Why**: Wraps our training DataFrame in the custom dataset class, making it compatible with PyTorch DataLoader.\n",
        "\n",
        "**How**:\n",
        "- Calls `__init__` with `train_df` and `tokenizer`\n",
        "- Uses default `max_length=128`\n",
        "- Creates an instance stored in `train_dataset`\n",
        "\n",
        "---\n",
        "\n",
        "**Line 31: `val_dataset = EmotionDataset(val_df, tokenizer)`**\n",
        "\n",
        "**What**: Creates a dataset instance for validation data.\n",
        "\n",
        "**Why**: We need a separate dataset for validation to evaluate on unseen data.\n",
        "\n",
        "**How**: Same as training dataset but uses `val_df` instead.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 33-38: Verification output**\n",
        "\n",
        "**What**: Prints dataset information and a sample to verify correct setup.\n",
        "\n",
        "**Why**: Sanity checking is crucial - we verify the dataset size and shape of returned tensors before training.\n",
        "\n",
        "**How**:\n",
        "- `len(train_dataset)` calls `__len__`, should return 8\n",
        "- `train_dataset[0]` calls `__getitem__(0)`, returning a dictionary\n",
        "- Iterating over dictionary items shows the shape of each component\n",
        "\n",
        "**Expected output**:\n",
        "```\n",
        "Training dataset size: 8\n",
        "Validation dataset size: 2\n",
        "\n",
        "Sample from dataset:\n",
        "input_ids: torch.Size([128])\n",
        "attention_mask: torch.Size([128])\n",
        "labels: torch.Size([5])\n",
        "```\n",
        "\n",
        "**Teaching Points**:\n",
        "1. **Lazy loading**: Tokenization happens in `__getitem__`, not `__init__`, saving memory\n",
        "2. **Indexing**: Dataset supports `dataset[i]` through `__getitem__`\n",
        "3. **Iteration**: Can loop over dataset, though DataLoader is preferred\n",
        "4. **Consistency**: Each sample returns the same structure (dictionary with same keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083dfbad",
      "metadata": {
        "id": "083dfbad"
      },
      "source": [
        "## Section 7: Data Collator\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a11b63",
      "metadata": {
        "id": "12a11b63"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollator:\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "        labels = torch.stack([item['labels'] for item in batch])\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "collator = DataCollator()\n",
        "print(\"Data collator initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62f74f4",
      "metadata": {
        "id": "b62f74f4"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `@dataclass`**\n",
        "\n",
        "**What**: Decorator that transforms the following class into a dataclass.\n",
        "\n",
        "**Why**: Dataclasses automatically generate `__init__`, `__repr__`, and other methods, reducing boilerplate. Since our collator has no instance attributes to initialize, this makes the class definition cleaner.\n",
        "\n",
        "**How**: The `@dataclass` decorator modifies the class definition at creation time, adding special methods. Even with no attributes defined, it creates a valid `__init__` method.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: `class DataCollator:`**\n",
        "\n",
        "**What**: Defines a callable class for batching dataset samples.\n",
        "\n",
        "**Why**: DataLoader needs a collation function to combine individual samples into batches. Making it a class (rather than a function) provides better organization and makes it callable like a function through `__call__`.\n",
        "\n",
        "**How**: The class name follows convention (PascalCase). No inheritance is needed as we implement `__call__` directly.\n",
        "\n",
        "**Collator Purpose**: Transforms a list of individual samples into a single batch by stacking tensors.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: (blank line)**\n",
        "\n",
        "**Why**: PEP 8 style guideline.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:`**\n",
        "\n",
        "**What**: Defines the special method that makes instances callable like functions.\n",
        "\n",
        "**Why**:\n",
        "- DataLoader calls `collator(batch)` where batch is a list of samples\n",
        "- `__call__` allows `collator_instance(batch)` syntax\n",
        "- Makes the class behave like a function\n",
        "\n",
        "**How**:\n",
        "- `__call__` is a Python magic method\n",
        "- `batch: List[Dict[str, torch.Tensor]]` indicates batch is a list of dictionaries\n",
        "  - Each dictionary has string keys and tensor values\n",
        "  - Example: `[{'input_ids': tensor(...), ...}, {'input_ids': tensor(...), ...}]`\n",
        "- `-> Dict[str, torch.Tensor]` shows it returns a single dictionary of tensors\n",
        "\n",
        "**Type Hint Breakdown**:\n",
        "- `List[...]`: Python list\n",
        "- `Dict[str, torch.Tensor]`: Dictionary with string keys and tensor values\n",
        "- Input: List of dictionaries (one per sample)\n",
        "- Output: Single dictionary (batched data)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `        input_ids = torch.stack([item['input_ids'] for item in batch])`**\n",
        "\n",
        "**What**: Stacks all input_ids tensors from individual samples into a single 2D tensor.\n",
        "\n",
        "**Why**: Neural networks process batches, not individual samples. Stacking combines multiple 1D tensors (128,) into a 2D tensor (batch_size, 128).\n",
        "\n",
        "**How**:\n",
        "- List comprehension: `[item['input_ids'] for item in batch]` creates a list of tensors\n",
        "  - If batch has 4 samples, creates list of 4 tensors each of shape (128,)\n",
        "- `torch.stack()` combines them into a single tensor\n",
        "  - Stacks along a new dimension (dimension 0)\n",
        "  - Result shape: (4, 128) for batch size 4\n",
        "\n",
        "**torch.stack vs torch.cat**:\n",
        "- `stack`: Creates new dimension, requires same-shaped tensors\n",
        "- `cat`: Concatenates along existing dimension, allows different shapes\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `        attention_mask = torch.stack([item['attention_mask'] for item in batch])`**\n",
        "\n",
        "**What**: Stacks attention masks into a batch.\n",
        "\n",
        "**Why**: Each sample's attention mask must be batched alongside its input_ids to maintain alignment.\n",
        "\n",
        "**How**: Identical process to input_ids stacking, resulting in shape (batch_size, 128).\n",
        "\n",
        "**Attention Mask Content**:\n",
        "- 1: Real token (attend to this)\n",
        "- 0: Padding token (ignore this)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `        labels = torch.stack([item['labels'] for item in batch])`**\n",
        "\n",
        "**What**: Stacks label tensors into a batch.\n",
        "\n",
        "**Why**: Batching labels is necessary for batch loss calculation during training.\n",
        "\n",
        "**How**:\n",
        "- Each label tensor has shape (5,) representing 5 emotions\n",
        "- Stacking creates shape (batch_size, 5)\n",
        "- Example: For batch size 4, result is (4, 5)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: (blank line)**\n",
        "\n",
        "**Why**: Separates processing from return statement.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 9: `        return {`**\n",
        "\n",
        "**What**: Begins return dictionary construction.\n",
        "\n",
        "**Why**: Maintaining dictionary structure makes it easy to access batch components by name in training loops.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 10-12: Dictionary entries**\n",
        "\n",
        "**What**: Returns the batched tensors in a dictionary.\n",
        "\n",
        "**Why**: This structure matches what training loops expect, providing named access to batch components.\n",
        "\n",
        "**How**: Each key maps to a batched tensor:\n",
        "- `'input_ids'`: Shape (batch_size, 128)\n",
        "- `'attention_mask'`: Shape (batch_size, 128)\n",
        "- `'labels'`: Shape (batch_size, 5)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 13: `        }`**\n",
        "\n",
        "**What**: Closes the return dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 15: `collator = DataCollator()`**\n",
        "\n",
        "**What**: Creates an instance of the DataCollator class.\n",
        "\n",
        "**Why**: We need an instance to pass to DataLoader's `collate_fn` parameter.\n",
        "\n",
        "**How**:\n",
        "- Calls the auto-generated `__init__` (from @dataclass)\n",
        "- Since there are no attributes, initialization is trivial\n",
        "- The instance can be called: `collator(batch)`\n",
        "\n",
        "---\n",
        "\n",
        "**Line 16: `print(\"Data collator initialized successfully\")`**\n",
        "\n",
        "**What**: Confirmation message.\n",
        "\n",
        "**Why**: Provides feedback that the collator was created without errors.\n",
        "\n",
        "**Teaching Points**:\n",
        "\n",
        "1. **Batching Process**:\n",
        "   ```\n",
        "   Input:  [{'input_ids': (128,)}, {'input_ids': (128,)}, ...]\n",
        "   Output: {'input_ids': (batch_size, 128), ...}\n",
        "   ```\n",
        "\n",
        "2. **Why custom collator**: Default collator doesn't handle dictionary outputs from our dataset.\n",
        "\n",
        "3. **Alternative**: Could use a function instead of a class, but classes provide better organization.\n",
        "\n",
        "4. **Memory efficiency**: Stacking creates views when possible, not copies, saving memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92581ba4",
      "metadata": {
        "id": "92581ba4"
      },
      "source": [
        "## Section 8: DataLoader Setup\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762fcfdc",
      "metadata": {
        "id": "762fcfdc"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "\n",
        "print(\"\\nSample batch from DataLoader:\")\n",
        "for batch in train_loader:\n",
        "    for key, value in batch.items():\n",
        "        print(f\"{key}: {value.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138e1006",
      "metadata": {
        "id": "138e1006"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `BATCH_SIZE = 4`**\n",
        "\n",
        "**What**: Defines a constant for the batch size.\n",
        "\n",
        "**Why**:\n",
        "- Batch size controls how many samples are processed together\n",
        "- Using a constant makes it easy to experiment with different values\n",
        "- Larger batches train faster but require more memory\n",
        "- Smaller batches provide more frequent updates and noisier gradients\n",
        "\n",
        "**How**: Uppercase naming convention indicates this is a constant that should not be changed during execution.\n",
        "\n",
        "**Batch Size Considerations**:\n",
        "- **Too small** (1-2): Slow training, noisy gradients, poor GPU utilization\n",
        "- **Too large** (128+): High memory usage, may not fit in GPU, can reduce generalization\n",
        "- **Sweet spot** (8-64): Good balance for most tasks\n",
        "- **Our choice** (4): Appropriate for small dataset and demonstration purposes\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `train_loader = DataLoader(`**\n",
        "\n",
        "**What**: Begins creation of a DataLoader for training data.\n",
        "\n",
        "**Why**: DataLoader handles batching, shuffling, and parallel loading, abstracting away complex iteration logic. It converts our dataset into an efficient iterator.\n",
        "\n",
        "**How**: `DataLoader` is PyTorch's built-in utility class that wraps datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `    train_dataset,`**\n",
        "\n",
        "**What**: First positional argument - the dataset to load from.\n",
        "\n",
        "**Why**: DataLoader needs to know which dataset to iterate over.\n",
        "\n",
        "**How**: Passes our `train_dataset` instance (EmotionDataset object with 8 samples).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `    batch_size=BATCH_SIZE,`**\n",
        "\n",
        "**What**: Specifies how many samples per batch.\n",
        "\n",
        "**Why**: Determines batch dimensions. With 8 samples and batch size 4, we get 2 batches per epoch.\n",
        "\n",
        "**How**: DataLoader will call `dataset[i]` for indices in each batch and pass the list to the collator.\n",
        "\n",
        "**Batch Calculation**:\n",
        "- Total samples: 8\n",
        "- Batch size: 4\n",
        "- Number of batches: 8 / 4 = 2 full batches\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `    shuffle=True,`**\n",
        "\n",
        "**What**: Enables random shuffling of data each epoch.\n",
        "\n",
        "**Why**:\n",
        "- Shuffling prevents the model from learning spurious patterns based on data order\n",
        "- Creates different batches each epoch, improving generalization\n",
        "- Reduces risk of overfitting to specific batch compositions\n",
        "\n",
        "**How**: Before each epoch, DataLoader randomly permutes the dataset indices, then creates batches from this shuffled order.\n",
        "\n",
        "**Important**: Always shuffle training data, never validation/test data.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `    collate_fn=collator`**\n",
        "\n",
        "**What**: Specifies our custom collation function.\n",
        "\n",
        "**Why**: Without this, DataLoader uses default collation which cannot handle our dictionary-based dataset output.\n",
        "\n",
        "**How**: DataLoader will call `collator(batch_samples)` where `batch_samples` is a list of dictionaries from our dataset.\n",
        "\n",
        "**Collation Flow**:\n",
        "1. DataLoader samples indices: `[2, 5, 1, 7]` (example for batch size 4)\n",
        "2. Calls `dataset[i]` for each index\n",
        "3. Collects results into list of dictionaries\n",
        "4. Passes list to `collator()`\n",
        "5. Receives batched dictionary back\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: `)`**\n",
        "\n",
        "**What**: Closes the DataLoader constructor.\n",
        "\n",
        "**How**: Creates an iterable `train_loader` object.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 10: `val_loader = DataLoader(`**\n",
        "\n",
        "**What**: Begins creation of validation DataLoader.\n",
        "\n",
        "**Why**: We need a separate loader for validation data with different settings.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 11-15: Validation DataLoader parameters**\n",
        "\n",
        "**What**: Similar to training loader but with `shuffle=False`.\n",
        "\n",
        "**Why shuffle=False**:\n",
        "- Validation is for evaluation, not training\n",
        "- Consistent order makes debugging easier\n",
        "- Shuffling does not improve validation performance\n",
        "- Deterministic order ensures reproducible results\n",
        "\n",
        "**Other parameters identical**: Same batch size and collator for consistency.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 17: `print(f\"Number of training batches: {len(train_loader)}\")`**\n",
        "\n",
        "**What**: Prints how many batches are in the training loader.\n",
        "\n",
        "**Why**: Understanding batch count helps predict training time and verify setup.\n",
        "\n",
        "**How**:\n",
        "- `len(train_loader)` calculates: ceil(dataset_size / batch_size)\n",
        "- For 8 samples with batch size 4: 8 / 4 = 2 batches\n",
        "\n",
        "**Formula**:\n",
        "```\n",
        "num_batches = ceil(num_samples / batch_size)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 18: `print(f\"Number of validation batches: {len(val_loader)}\")`**\n",
        "\n",
        "**What**: Prints validation batch count.\n",
        "\n",
        "**How**: For 2 validation samples with batch size 4: ceil(2 / 4) = 1 batch (partial batch).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 20: `print(\"\\nSample batch from DataLoader:\")`**\n",
        "\n",
        "**What**: Header for batch inspection.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 21: `for batch in train_loader:`**\n",
        "\n",
        "**What**: Begins iteration over the training DataLoader.\n",
        "\n",
        "**Why**: We want to inspect one batch to verify shapes and structure.\n",
        "\n",
        "**How**:\n",
        "- `for batch in train_loader` triggers DataLoader's iteration protocol\n",
        "- DataLoader calls dataset `__getitem__` for batch_size samples\n",
        "- Collates them using our collator\n",
        "- Returns batched dictionary as `batch`\n",
        "\n",
        "---\n",
        "\n",
        "**Line 22: `    for key, value in batch.items():`**\n",
        "\n",
        "**What**: Iterates over the batch dictionary entries.\n",
        "\n",
        "**Why**: We want to see each component and its shape.\n",
        "\n",
        "**How**: `batch.items()` yields (key, value) pairs from the dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 23: `        print(f\"{key}: {value.shape}\")`**\n",
        "\n",
        "**What**: Prints each tensor's name and shape.\n",
        "\n",
        "**Why**: Verifying tensor shapes is crucial before training to catch dimensionality errors.\n",
        "\n",
        "**Expected Output**:\n",
        "```\n",
        "input_ids: torch.Size([4, 128])\n",
        "attention_mask: torch.Size([4, 128])\n",
        "labels: torch.Size([4, 5])\n",
        "```\n",
        "\n",
        "**Shape Interpretation**:\n",
        "- `[4, 128]`: 4 samples, each with 128 tokens\n",
        "- `[4, 5]`: 4 samples, each with 5 emotion labels\n",
        "\n",
        "---\n",
        "\n",
        "**Line 24: `    break`**\n",
        "\n",
        "**What**: Exits the loop after processing one batch.\n",
        "\n",
        "**Why**: We only need to inspect one batch, not iterate through all of them.\n",
        "\n",
        "**How**: `break` immediately exits the `for` loop.\n",
        "\n",
        "**Teaching Points**:\n",
        "\n",
        "1. **DataLoader Benefits**:\n",
        "   - Automatic batching\n",
        "   - Background data loading (with `num_workers`)\n",
        "   - Memory-efficient iteration\n",
        "   - GPU transfer optimization\n",
        "\n",
        "2. **Batch vs Epoch**:\n",
        "   - **Batch**: One group of samples (e.g., 4 samples)\n",
        "   - **Epoch**: Full pass through entire dataset (e.g., 2 batches = 1 epoch)\n",
        "\n",
        "3. **Memory Considerations**:\n",
        "   - Larger batches use more GPU memory\n",
        "   - Formula: memory ≈ batch_size × sequence_length × model_params\n",
        "   - If out of memory, reduce batch_size\n",
        "\n",
        "4. **Drop Last**:\n",
        "   - `drop_last=True` would discard incomplete final batches\n",
        "   - We don't use it here because our data divides evenly\n",
        "   - Useful when batch normalization requires consistent batch sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd321170",
      "metadata": {
        "id": "cd321170"
      },
      "source": [
        "## Section 9: Model Definition\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5d906a",
      "metadata": {
        "id": "8f5d906a"
      },
      "outputs": [],
      "source": [
        "class EmotionClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name: str, num_labels: int = 5, dropout: float = 0.3):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        outputs = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EmotionClassifier(MODEL_NAME, num_labels=5)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd8c1d2",
      "metadata": {
        "id": "9cd8c1d2"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `class EmotionClassifier(nn.Module):`**\n",
        "\n",
        "**What**: Defines a neural network class inheriting from PyTorch's Module base class.\n",
        "\n",
        "**Why**:\n",
        "- All PyTorch models must inherit from `nn.Module`\n",
        "- This provides access to training methods, parameter management, and device handling\n",
        "- Enables automatic gradient computation and model serialization\n",
        "\n",
        "**How**:\n",
        "- `class EmotionClassifier` names our custom model\n",
        "- `(nn.Module)` specifies inheritance\n",
        "- `nn.Module` is PyTorch's base class for all neural network modules\n",
        "\n",
        "**nn.Module Benefits**:\n",
        "1. Automatic parameter tracking\n",
        "2. `.to(device)` for GPU/CPU transfer\n",
        "3. `.train()` and `.eval()` mode switching\n",
        "4. `.parameters()` for optimization\n",
        "5. State dictionary for saving/loading\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `    def __init__(self, model_name: str, num_labels: int = 5, dropout: float = 0.3):`**\n",
        "\n",
        "**What**: Constructor that initializes the model architecture.\n",
        "\n",
        "**Why**: Defines what layers the model contains. This runs once when creating a model instance.\n",
        "\n",
        "**How**:\n",
        "- `model_name: str`: Name of pretrained transformer (e.g., 'bert-base-uncased')\n",
        "- `num_labels: int = 5`: Number of output classes (our 5 emotions)\n",
        "- `dropout: float = 0.3`: Dropout probability for regularization\n",
        "\n",
        "**Parameter Purposes**:\n",
        "- `model_name`: Determines which transformer to load\n",
        "- `num_labels`: Size of output layer\n",
        "- `dropout`: Controls regularization strength (0.3 = 30% of neurons dropped)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `        super(EmotionClassifier, self).__init__()`**\n",
        "\n",
        "**What**: Calls the parent class (nn.Module) constructor.\n",
        "\n",
        "**Why**: **CRITICAL** - This initializes the nn.Module infrastructure. Without this, parameter registration, device handling, and other essential features will not work.\n",
        "\n",
        "**How**:\n",
        "- `super()` references the parent class\n",
        "- `EmotionClassifier, self` specifies which class we are in\n",
        "- `.__init__()` calls the parent constructor\n",
        "\n",
        "**Common Python3 Alternative**: `super().__init__()` (shorter, equivalent)\n",
        "\n",
        "**What happens inside nn.Module.__init__()**:\n",
        "1. Initializes parameter tracking system\n",
        "2. Sets up hooks for forward/backward passes\n",
        "3. Prepares device management\n",
        "4. Creates module registry\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `        self.transformer = AutoModel.from_pretrained(model_name)`**\n",
        "\n",
        "**What**: Loads a pretrained transformer model and stores it as a submodule.\n",
        "\n",
        "**Why**:\n",
        "- Transfer learning: We use knowledge from pretraining on massive datasets\n",
        "- Pretrained models understand language structure, grammar, and semantics\n",
        "- Fine-tuning is much faster than training from scratch\n",
        "- Better performance, especially with limited data\n",
        "\n",
        "**How**:\n",
        "- `AutoModel.from_pretrained()` downloads and initializes the model\n",
        "- Downloads weights (hundreds of MB) from Hugging Face Hub\n",
        "- Cached locally to avoid re-downloading\n",
        "- Returns a transformer model (BERT in our case) with pretrained weights\n",
        "\n",
        "**Model Components Loaded**:\n",
        "- 12 transformer layers (for bert-base)\n",
        "- 768-dimensional hidden states\n",
        "- Multi-head attention mechanisms\n",
        "- Position embeddings\n",
        "- Token embeddings\n",
        "- ~110 million parameters\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `        self.dropout = nn.Dropout(dropout)`**\n",
        "\n",
        "**What**: Creates a dropout layer for regularization.\n",
        "\n",
        "**Why**:\n",
        "- **Regularization**: Prevents overfitting by randomly zeroing neurons during training\n",
        "- **Ensemble effect**: Creates different network paths each forward pass\n",
        "- **Robustness**: Forces network to learn redundant representations\n",
        "\n",
        "**How**:\n",
        "- `nn.Dropout(dropout)` creates a dropout layer\n",
        "- `dropout=0.3` means 30% of inputs are randomly set to zero during training\n",
        "- Automatically disabled during evaluation (`.eval()` mode)\n",
        "- Remaining values are scaled by 1/(1-dropout) to maintain expected value\n",
        "\n",
        "**Dropout Behavior**:\n",
        "- **Training mode**: Randomly drops 30% of neurons\n",
        "- **Eval mode**: All neurons active (no dropout)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: `        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)`**\n",
        "\n",
        "**What**: Creates a linear (fully connected) classification layer.\n",
        "\n",
        "**Why**:\n",
        "- Transforms transformer outputs (768-dim) to label space (5-dim)\n",
        "- This is the \"classification head\" specific to our task\n",
        "- Only this layer is task-specific; transformer is general-purpose\n",
        "\n",
        "**How**:\n",
        "- `nn.Linear(in_features, out_features)` creates weight matrix W and bias b\n",
        "- `self.transformer.config.hidden_size` gets transformer output dimension (768)\n",
        "- `num_labels` is our output dimension (5)\n",
        "- Computes: output = input @ W.T + b\n",
        "\n",
        "**Layer Dimensions**:\n",
        "- Input: (batch_size, 768)\n",
        "- Weight matrix: (5, 768)\n",
        "- Bias: (5,)\n",
        "- Output: (batch_size, 5)\n",
        "\n",
        "**Parameters**: 768 × 5 + 5 = 3,845 parameters\n",
        "\n",
        "---\n",
        "\n",
        "**Line 9: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 10: `    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:`**\n",
        "\n",
        "**What**: Defines the forward pass - how data flows through the network.\n",
        "\n",
        "**Why**:\n",
        "- PyTorch calls this method automatically during model(input)\n",
        "- Defines the computational graph for backpropagation\n",
        "- Separates model architecture (init) from computation (forward)\n",
        "\n",
        "**How**:\n",
        "- `forward` is a special method name recognized by nn.Module\n",
        "- Takes input tensors (input_ids, attention_mask)\n",
        "- Returns output tensor (logits)\n",
        "- Type hints document tensor types\n",
        "\n",
        "---\n",
        "\n",
        "**Line 11: `        outputs = self.transformer(`**\n",
        "\n",
        "**What**: Begins passing inputs through the transformer model.\n",
        "\n",
        "**Why**: The transformer extracts contextual representations from token IDs.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 12-13: Transformer inputs**\n",
        "\n",
        "**What**: Passes token IDs and attention mask to transformer.\n",
        "\n",
        "**Why**:\n",
        "- `input_ids`: The actual tokens to process\n",
        "- `attention_mask`: Tells model which tokens to attend to (1) vs ignore (0)\n",
        "\n",
        "**How**: Transformer processes these through 12 layers of self-attention and feedforward networks.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 14: `        )`**\n",
        "\n",
        "**What**: Closes the transformer call.\n",
        "\n",
        "**How**: `outputs` is a special object containing:\n",
        "- `last_hidden_state`: Shape (batch_size, 128, 768) - all token representations\n",
        "- `pooler_output`: Shape (batch_size, 768) - [CLS] token representation\n",
        "- Additional attributes depending on model configuration\n",
        "\n",
        "---\n",
        "\n",
        "**Line 15: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 16: `        pooled_output = outputs.last_hidden_state[:, 0, :]`**\n",
        "\n",
        "**What**: Extracts the [CLS] token representation from the transformer output.\n",
        "\n",
        "**Why**:\n",
        "- In BERT, the first token ([CLS]) is trained to represent the entire sequence\n",
        "- This single 768-dimensional vector summarizes the input text\n",
        "- Commonly used for classification tasks\n",
        "\n",
        "**How**:\n",
        "- `outputs.last_hidden_state` has shape (batch_size, 128, 768)\n",
        "  - 128 positions (tokens)\n",
        "  - 768 dimensions per token\n",
        "- `[:, 0, :]` slices to get first token ([CLS]) for all samples\n",
        "  - `:` - all samples in batch\n",
        "  - `0` - first token position\n",
        "  - `:` - all 768 dimensions\n",
        "- Result shape: (batch_size, 768)\n",
        "\n",
        "**Indexing Breakdown**:\n",
        "```\n",
        "[batch_dimension, sequence_dimension, feature_dimension]\n",
        "[      :        ,         0         ,         :        ]\n",
        "   all samples     first token       all features\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 17: `        pooled_output = self.dropout(pooled_output)`**\n",
        "\n",
        "**What**: Applies dropout to the pooled representation.\n",
        "\n",
        "**Why**:\n",
        "- Regularization between transformer and classifier\n",
        "- Prevents overfitting to specific neuron patterns\n",
        "- Improves generalization\n",
        "\n",
        "**How**:\n",
        "- During training: Randomly zeros 30% of the 768 values\n",
        "- During evaluation: No-op (passes through unchanged)\n",
        "- In-place operation (same variable name)\n",
        "\n",
        "---\n",
        "\n",
        "**Line 18: `        logits = self.classifier(pooled_output)`**\n",
        "\n",
        "**What**: Passes the pooled representation through the classification layer.\n",
        "\n",
        "**Why**: Transforms from feature space (768-dim) to label space (5-dim), producing raw scores for each emotion.\n",
        "\n",
        "**How**:\n",
        "- Matrix multiplication: (batch_size, 768) @ (768, 5)\n",
        "- Add bias: + (5,)\n",
        "- Result: (batch_size, 5) - one score per emotion per sample\n",
        "\n",
        "**Logits vs Probabilities**:\n",
        "- **Logits**: Raw scores (can be any value)\n",
        "- **Probabilities**: After sigmoid/softmax (0 to 1)\n",
        "- We return logits because BCEWithLogitsLoss applies sigmoid internally\n",
        "\n",
        "---\n",
        "\n",
        "**Line 19: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 20: `        return logits`**\n",
        "\n",
        "**What**: Returns the model's predictions (logits).\n",
        "\n",
        "**Why**: These logits are passed to the loss function during training or converted to probabilities during inference.\n",
        "\n",
        "**How**: Returns tensor of shape (batch_size, 5).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 22: `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`**\n",
        "\n",
        "**What**: Determines whether to use GPU or CPU for computation.\n",
        "\n",
        "**Why**:\n",
        "- GPUs dramatically accelerate training (10-100x faster)\n",
        "- Code should run on both GPU and CPU for compatibility\n",
        "- Automatic device selection makes code portable\n",
        "\n",
        "**How**:\n",
        "- `torch.cuda.is_available()` checks if CUDA-capable GPU is present\n",
        "- Returns `torch.device('cuda')` if GPU available, else `torch.device('cpu')`\n",
        "- `torch.device` object is used for tensor and model placement\n",
        "\n",
        "---\n",
        "\n",
        "**Line 23: `model = EmotionClassifier(MODEL_NAME, num_labels=5)`**\n",
        "\n",
        "**What**: Instantiates our model class.\n",
        "\n",
        "**Why**: Creates the actual model object we will train.\n",
        "\n",
        "**How**:\n",
        "- Calls `__init__` with `model_name='bert-base-uncased'` and `num_labels=5`\n",
        "- Downloads BERT weights (if not cached)\n",
        "- Initializes all layers\n",
        "- Creates ~110M parameters\n",
        "\n",
        "---\n",
        "\n",
        "**Line 24: `model = model.to(device)`**\n",
        "\n",
        "**What**: Moves model parameters to the selected device (GPU or CPU).\n",
        "\n",
        "**Why**:\n",
        "- Model parameters must be on the same device as input data\n",
        "- `.to(device)` is PyTorch's method for device transfer\n",
        "- Returns the model for chaining\n",
        "\n",
        "**How**:\n",
        "- If device is 'cuda', moves all parameters to GPU memory\n",
        "- If device is 'cpu', keeps them in RAM\n",
        "- Also moves buffers (batch norm stats, etc.)\n",
        "\n",
        "**Performance Impact**:\n",
        "- CPU: Slow training, lower memory usage\n",
        "- GPU: Fast training, higher memory usage\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 26-30: Model inspection**\n",
        "\n",
        "**What**: Prints model information for verification.\n",
        "\n",
        "**Why**: Understanding model size and structure before training is crucial.\n",
        "\n",
        "**How**:\n",
        "\n",
        "**Line 26**: Prints device name\n",
        "- Expected: \"cuda\" or \"cpu\"\n",
        "\n",
        "**Line 28**: Prints model architecture\n",
        "- Shows all layers and their parameters\n",
        "- Displays transformer layers and classification head\n",
        "\n",
        "**Line 29**: Counts total parameters\n",
        "- `model.parameters()` yields all parameter tensors\n",
        "- `p.numel()` counts elements in each tensor\n",
        "- `sum()` totals across all parameters\n",
        "- `:,` formats with thousands separators\n",
        "\n",
        "**Line 30**: Counts trainable parameters\n",
        "- `p.requires_grad` filters to only trainable parameters\n",
        "- Should equal total parameters (all are trainable by default)\n",
        "- Some advanced techniques freeze layers (requires_grad=False)\n",
        "\n",
        "**Expected Output**:\n",
        "```\n",
        "Device: cuda\n",
        "Model architecture: [Shows full model structure]\n",
        "Total parameters: ~110,000,000\n",
        "Trainable parameters: ~110,000,000\n",
        "```\n",
        "\n",
        "**Teaching Points**:\n",
        "\n",
        "1. **Model Architecture**:\n",
        "   ```\n",
        "   Input (batch_size, 128)\n",
        "      ↓\n",
        "   Transformer (12 layers)\n",
        "      ↓\n",
        "   [CLS] token (batch_size, 768)\n",
        "      ↓\n",
        "   Dropout\n",
        "      ↓\n",
        "   Linear (batch_size, 5)\n",
        "      ↓\n",
        "   Logits\n",
        "   ```\n",
        "\n",
        "2. **Transfer Learning**: Only classifier is random initialized; transformer has pretrained weights.\n",
        "\n",
        "3. **Fine-tuning vs Feature Extraction**:\n",
        "   - **Fine-tuning** (our approach): Update all parameters\n",
        "   - **Feature extraction**: Freeze transformer, train only classifier\n",
        "\n",
        "4. **Memory Usage**: ~450MB for bert-base (float32), ~225MB (float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a556e1",
      "metadata": {
        "id": "02a556e1"
      },
      "source": [
        "## Section 10: Trainer Class\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee05f019",
      "metadata": {
        "id": "ee05f019"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                 criterion, optimizer, device: torch.device):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    def train_epoch(self) -> float:\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            labels = batch['labels'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def validate_epoch(self) -> tuple:\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                preds = torch.sigmoid(logits) > 0.5\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_labels = np.vstack(all_labels)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "            train_loss = self.train_epoch()\n",
        "            val_loss, val_acc = self.validate_epoch()\n",
        "\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825c3a02",
      "metadata": {
        "id": "825c3a02"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `class Trainer:`**\n",
        "\n",
        "**What**: Defines a custom training orchestration class.\n",
        "\n",
        "**Why**:\n",
        "- Encapsulates training logic for reusability and organization\n",
        "- Manages training loop, validation, and history tracking\n",
        "- Separates model definition from training procedure\n",
        "- Makes code cleaner and more maintainable\n",
        "\n",
        "**How**: Creates a regular Python class (not inheriting from nn.Module) for training utilities.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,`**\n",
        "\n",
        "**What**: Constructor that stores all training components.\n",
        "\n",
        "**Why**: Centralizes all objects needed for training in one place.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `                 criterion, optimizer, device: torch.device):`**\n",
        "\n",
        "**What**: Continuation of constructor parameters.\n",
        "\n",
        "**How**:\n",
        "- `model`: The neural network to train\n",
        "- `train_loader`: DataLoader for training data\n",
        "- `val_loader`: DataLoader for validation data\n",
        "- `criterion`: Loss function\n",
        "- `optimizer`: Optimization algorithm\n",
        "- `device`: CPU or GPU\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `        self.model = model`**\n",
        "\n",
        "**What**: Stores model reference as instance variable.\n",
        "\n",
        "**Why**: Allows all methods to access the model via `self.model`.\n",
        "\n",
        "**How**: Simple attribute assignment.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 6-10: Store remaining components**\n",
        "\n",
        "**What**: Stores train_loader, val_loader, criterion, optimizer, and device as instance variables.\n",
        "\n",
        "**Why**: Makes these accessible throughout the class lifetime.\n",
        "\n",
        "**How**: Each becomes an attribute: `self.train_loader`, `self.val_loader`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 11: `        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}`**\n",
        "\n",
        "**What**: Initializes a dictionary to track metrics across epochs.\n",
        "\n",
        "**Why**:\n",
        "- Enables plotting learning curves after training\n",
        "- Helps diagnose overfitting, underfitting, convergence\n",
        "- Provides historical record for analysis\n",
        "\n",
        "**How**: Creates dict with three empty lists:\n",
        "- `train_loss`: Training loss per epoch\n",
        "- `val_loss`: Validation loss per epoch\n",
        "- `val_acc`: Validation accuracy per epoch\n",
        "\n",
        "---\n",
        "\n",
        "**Line 12: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 13: `    def train_epoch(self) -> float:`**\n",
        "\n",
        "**What**: Defines method to train for one complete epoch.\n",
        "\n",
        "**Why**: Separates single-epoch logic from multi-epoch orchestration.\n",
        "\n",
        "**How**:\n",
        "- Iterates through all training batches once\n",
        "- Returns average loss for the epoch\n",
        "- `-> float` indicates return type\n",
        "\n",
        "---\n",
        "\n",
        "**Line 14: `        self.model.train()`**\n",
        "\n",
        "**What**: Sets model to training mode.\n",
        "\n",
        "**Why**: **CRITICAL** - Enables training-specific behaviors:\n",
        "- Activates dropout (randomly drops neurons)\n",
        "- Enables batch normalization in training mode\n",
        "- Affects any layer with different train/eval behavior\n",
        "\n",
        "**How**:\n",
        "- `model.train()` is inherited from nn.Module\n",
        "- Sets `model.training = True`\n",
        "- Recursively applies to all submodules\n",
        "\n",
        "**Common Mistake**: Forgetting this causes dropout to be disabled during training.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 15: `        total_loss = 0`**\n",
        "\n",
        "**What**: Initializes accumulator for batch losses.\n",
        "\n",
        "**Why**: We sum losses across all batches to compute average epoch loss.\n",
        "\n",
        "**How**: Starts at zero, incremented each batch.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 16: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 17: `        for batch in tqdm(self.train_loader, desc=\"Training\"):`**\n",
        "\n",
        "**What**: Iterates through all training batches with progress bar.\n",
        "\n",
        "**Why**:\n",
        "- Processes entire dataset one batch at a time\n",
        "- `tqdm` provides visual feedback and ETA\n",
        "- Each iteration is one gradient update step\n",
        "\n",
        "**How**:\n",
        "- `self.train_loader` yields batches\n",
        "- `tqdm()` wraps iterator with progress bar\n",
        "- `desc=\"Training\"` labels the progress bar\n",
        "- `batch` is a dictionary: `{'input_ids': tensor, 'attention_mask': tensor, 'labels': tensor}`\n",
        "\n",
        "**Example Progress**: `Training: 50%|██████ | 1/2 [00:03<00:03, 3.5s/it]`\n",
        "\n",
        "---\n",
        "\n",
        "**Line 18: `            input_ids = batch['input_ids'].to(self.device)`**\n",
        "\n",
        "**What**: Extracts input_ids from batch and moves to GPU/CPU.\n",
        "\n",
        "**Why**:\n",
        "- Model and data must be on same device\n",
        "- `.to(device)` transfers tensor to target device\n",
        "- This is the encoded token IDs from tokenizer\n",
        "\n",
        "**How**:\n",
        "- `batch['input_ids']` accesses the key from DataCollator output\n",
        "- `.to(self.device)` creates a copy on target device (if different)\n",
        "- Shape: (batch_size, 128)\n",
        "\n",
        "**Performance Note**: GPU transfer has overhead; batching amortizes this cost.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 19: `            attention_mask = batch['attention_mask'].to(self.device)`**\n",
        "\n",
        "**What**: Moves attention mask to device.\n",
        "\n",
        "**Why**: Indicates which tokens are real (1) vs padding (0).\n",
        "\n",
        "**How**: Same process as input_ids.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 20: `            labels = batch['labels'].to(self.device)`**\n",
        "\n",
        "**What**: Moves ground truth labels to device.\n",
        "\n",
        "**Why**: Needed to compute loss against model predictions.\n",
        "\n",
        "**How**: Shape (batch_size, 5) - binary labels for 5 emotions.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 21: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 22: `            self.optimizer.zero_grad()`**\n",
        "\n",
        "**What**: Resets gradients to zero.\n",
        "\n",
        "**Why**: **CRITICAL** - PyTorch accumulates gradients by default. Without this, gradients from previous batches would interfere with current batch.\n",
        "\n",
        "**How**:\n",
        "- Sets `grad` attribute of all model parameters to zero\n",
        "- Must be called before each backward pass\n",
        "- Prevents gradient accumulation across batches\n",
        "\n",
        "**When Gradient Accumulation is Desired**: Intentionally skip this every N steps to simulate larger batches.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 23: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 24: `            logits = self.model(input_ids=input_ids, attention_mask=attention_mask)`**\n",
        "\n",
        "**What**: Forward pass - gets model predictions.\n",
        "\n",
        "**Why**: Computes output for this batch to compare against labels.\n",
        "\n",
        "**How**:\n",
        "- Calls `model.forward()` implicitly (Python's `__call__` mechanism)\n",
        "- Passes tensors through transformer and classification head\n",
        "- Returns logits of shape (batch_size, 5)\n",
        "- **Gradient tracking is ON** - computation graph is built for backprop\n",
        "\n",
        "---\n",
        "\n",
        "**Line 25: `            loss = self.criterion(logits, labels)`**\n",
        "\n",
        "**What**: Computes loss between predictions and ground truth.\n",
        "\n",
        "**Why**:\n",
        "- Quantifies how wrong the model is\n",
        "- Provides gradient signal for learning\n",
        "- Single scalar value for optimization\n",
        "\n",
        "**How**:\n",
        "- `self.criterion` is BCEWithLogitsLoss\n",
        "- Applies sigmoid to logits internally\n",
        "- Computes binary cross-entropy for each of 5 labels\n",
        "- Averages across labels and batch\n",
        "- Result: scalar tensor with `requires_grad=True`\n",
        "\n",
        "---\n",
        "\n",
        "**Line 26: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 27: `            loss.backward()`**\n",
        "\n",
        "**What**: Backpropagation - computes gradients.\n",
        "\n",
        "**Why**: **CORE OF LEARNING** - Calculates how each parameter should change to reduce loss.\n",
        "\n",
        "**How**:\n",
        "- Traverses computation graph in reverse\n",
        "- Applies chain rule to compute derivatives\n",
        "- Stores gradients in `.grad` attribute of each parameter\n",
        "- Uses automatic differentiation (autograd)\n",
        "\n",
        "**What Happens Internally**:\n",
        "1. Starts at loss (scalar)\n",
        "2. Computes dLoss/dLogits\n",
        "3. Propagates through linear layer: dLoss/dWeights\n",
        "4. Continues through dropout, transformer layers\n",
        "5. Updates ALL parameters' `.grad` attributes\n",
        "\n",
        "---\n",
        "\n",
        "**Line 28: `            self.optimizer.step()`**\n",
        "\n",
        "**What**: Updates model parameters using computed gradients.\n",
        "\n",
        "**Why**: This is where learning actually happens - parameters are adjusted to reduce loss.\n",
        "\n",
        "**How**:\n",
        "- Optimizer (AdamW) applies update rule to each parameter\n",
        "- For AdamW: `param = param - lr * (gradient + weight_decay * param)`\n",
        "- Uses learning rate, momentum, adaptive learning rates\n",
        "- Modifies parameters in-place\n",
        "\n",
        "**The Training Step Sequence**:\n",
        "1. `zero_grad()` - Clear old gradients\n",
        "2. Forward pass - Compute predictions\n",
        "3. Compute loss\n",
        "4. `backward()` - Compute gradients\n",
        "5. `step()` - Update parameters\n",
        "\n",
        "---\n",
        "\n",
        "**Line 29: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 30: `            total_loss += loss.item()`**\n",
        "\n",
        "**What**: Accumulates batch loss for epoch average.\n",
        "\n",
        "**Why**: Track training progress across all batches.\n",
        "\n",
        "**How**:\n",
        "- `loss.item()` extracts scalar value from tensor (detaches from graph)\n",
        "- `.item()` prevents memory leak by not keeping computation graph\n",
        "- Adds to running total\n",
        "\n",
        "**Why .item() is Important**: Keeping tensors in list would prevent garbage collection of computation graphs.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 31: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 32: `        avg_loss = total_loss / len(self.train_loader)`**\n",
        "\n",
        "**What**: Computes average loss across all batches.\n",
        "\n",
        "**Why**: Single metric summarizing epoch performance.\n",
        "\n",
        "**How**:\n",
        "- `len(self.train_loader)` gives number of batches\n",
        "- Divides total by batch count\n",
        "- Result is mean batch loss\n",
        "\n",
        "**With 8 samples, batch_size=4**: len(train_loader) = 2 batches\n",
        "\n",
        "---\n",
        "\n",
        "**Line 33: `        return avg_loss`**\n",
        "\n",
        "**What**: Returns epoch's average training loss.\n",
        "\n",
        "**Why**: Allows caller to track and log training progress.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 34: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 35: `    def validate_epoch(self) -> tuple:`**\n",
        "\n",
        "**What**: Defines method to evaluate on validation set.\n",
        "\n",
        "**Why**:\n",
        "- Measures generalization to unseen data\n",
        "- Detects overfitting\n",
        "- Does NOT update parameters\n",
        "\n",
        "**How**: Returns tuple (avg_loss, accuracy).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 36: `        self.model.eval()`**\n",
        "\n",
        "**What**: Sets model to evaluation mode.\n",
        "\n",
        "**Why**: **CRITICAL** - Disables training-specific behaviors:\n",
        "- Deactivates dropout (all neurons active)\n",
        "- Batch norm uses running statistics instead of batch statistics\n",
        "- Ensures consistent predictions\n",
        "\n",
        "**How**:\n",
        "- `model.eval()` is inherited from nn.Module\n",
        "- Sets `model.training = False`\n",
        "- Recursively applies to all submodules\n",
        "\n",
        "**Impact**: Without this, dropout would randomly affect predictions, making validation inconsistent.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 37-39: Initialize tracking variables**\n",
        "\n",
        "**What**: Prepares containers for validation metrics.\n",
        "\n",
        "**How**:\n",
        "- `total_loss = 0`: Accumulates batch losses\n",
        "- `all_preds = []`: Stores predictions from all batches\n",
        "- `all_labels = []`: Stores ground truth from all batches\n",
        "\n",
        "**Why**: Aggregate across batches to compute overall metrics.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 40: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 41: `        with torch.no_grad():`**\n",
        "\n",
        "**What**: Context manager that disables gradient computation.\n",
        "\n",
        "**Why**:\n",
        "- **Performance**: Saves memory (no computation graph)\n",
        "- **Speed**: Faster forward passes\n",
        "- **Correctness**: Validation should not affect model parameters\n",
        "\n",
        "**How**:\n",
        "- Sets `torch.grad_enabled = False` within block\n",
        "- Automatically restores previous state after block\n",
        "- All operations inside do not track gradients\n",
        "\n",
        "**Memory Savings**: Approximately 2x less memory usage during validation.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 42: `            for batch in tqdm(self.val_loader, desc=\"Validation\"):`**\n",
        "\n",
        "**What**: Iterates through validation batches with progress bar.\n",
        "\n",
        "**Why**: Process entire validation set, show progress.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 43-45: Extract and move batch tensors**\n",
        "\n",
        "**What**: Gets input_ids, attention_mask, labels and moves to device.\n",
        "\n",
        "**Why**: Same as training loop - prepare data for model.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 46: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 47: `                logits = self.model(input_ids=input_ids, attention_mask=attention_mask)`**\n",
        "\n",
        "**What**: Forward pass for validation batch.\n",
        "\n",
        "**Why**: Get predictions to evaluate.\n",
        "\n",
        "**How**: Same as training, but NO gradient tracking (due to `torch.no_grad()`).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 48: `                loss = self.criterion(logits, labels)`**\n",
        "\n",
        "**What**: Computes validation loss.\n",
        "\n",
        "**Why**: Measure how well model performs on unseen data.\n",
        "\n",
        "**How**: Same loss function as training, but not used for backprop.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 49: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 50: `                total_loss += loss.item()`**\n",
        "\n",
        "**What**: Accumulates validation loss.\n",
        "\n",
        "**Why**: Track validation performance.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 51: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 52: `                preds = torch.sigmoid(logits) > 0.5`**\n",
        "\n",
        "**What**: Converts logits to binary predictions.\n",
        "\n",
        "**Why**:\n",
        "- Logits are raw scores\n",
        "- Sigmoid converts to probabilities (0 to 1)\n",
        "- Threshold at 0.5 for binary decision\n",
        "\n",
        "**How**:\n",
        "- `torch.sigmoid(logits)` → probabilities\n",
        "- `> 0.5` → boolean tensor (True/False)\n",
        "- Multi-label: Each emotion independently thresholded\n",
        "- Shape: (batch_size, 5) of booleans\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Logits: [2.3, -1.1, 0.8, -0.3, 1.5]\n",
        "Sigmoid: [0.91, 0.25, 0.69, 0.43, 0.82]\n",
        "Preds: [True, False, True, False, True]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 53: `                all_preds.append(preds.cpu().numpy())`**\n",
        "\n",
        "**What**: Moves predictions to CPU and converts to NumPy.\n",
        "\n",
        "**Why**:\n",
        "- Sklearn metrics require NumPy arrays\n",
        "- `.cpu()` moves from GPU to CPU\n",
        "- `.numpy()` converts torch tensor to NumPy array\n",
        "- Append to list for later concatenation\n",
        "\n",
        "**How**: Stores (batch_size, 5) NumPy array.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 54: `                all_labels.append(labels.cpu().numpy())`**\n",
        "\n",
        "**What**: Stores ground truth labels as NumPy.\n",
        "\n",
        "**Why**: Match format of predictions for metric computation.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 55: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 56: `        avg_loss = total_loss / len(self.val_loader)`**\n",
        "\n",
        "**What**: Computes average validation loss.\n",
        "\n",
        "**Why**: Single metric for validation performance.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 57: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 58: `        all_preds = np.vstack(all_preds)`**\n",
        "\n",
        "**What**: Stacks list of arrays into single 2D array.\n",
        "\n",
        "**Why**: Sklearn metrics need single array, not list.\n",
        "\n",
        "**How**:\n",
        "- `np.vstack()` vertically stacks arrays\n",
        "- Converts list of (batch_size, 5) arrays to (total_samples, 5)\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Input: [array([[1,0,1,0,1], [0,1,0,1,0]]), array([[1,1,0,0,1]])]\n",
        "Output: array([[1,0,1,0,1], [0,1,0,1,0], [1,1,0,0,1]])\n",
        "Shape: (3, 5)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 59: `        all_labels = np.vstack(all_labels)`**\n",
        "\n",
        "**What**: Stacks ground truth labels.\n",
        "\n",
        "**Why**: Match predictions format.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 60: `        accuracy = accuracy_score(all_labels, all_preds)`**\n",
        "\n",
        "**What**: Computes multi-label accuracy.\n",
        "\n",
        "**Why**: Measures exact match accuracy (all 5 labels must be correct).\n",
        "\n",
        "**How**:\n",
        "- Sklearn's `accuracy_score` compares row-by-row\n",
        "- Returns fraction of samples where ALL labels match\n",
        "- **Strict metric**: [1,0,1,0,1] vs [1,0,1,0,0] = incorrect\n",
        "\n",
        "**Multi-Label Accuracy Calculation**:\n",
        "```\n",
        "Sample 1: [1,0,1,0,1] == [1,0,1,0,1] → Correct\n",
        "Sample 2: [0,1,0,1,0] == [0,1,0,0,0] → Incorrect\n",
        "Accuracy: 1/2 = 0.5\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 61: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 62: `        return avg_loss, accuracy`**\n",
        "\n",
        "**What**: Returns validation metrics.\n",
        "\n",
        "**Why**: Allows caller to track validation performance.\n",
        "\n",
        "**How**: Returns tuple of (float, float).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 63: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 64: `    def train(self, num_epochs: int):`**\n",
        "\n",
        "**What**: Main training loop orchestrating multiple epochs.\n",
        "\n",
        "**Why**:\n",
        "- Coordinates training and validation\n",
        "- Tracks history\n",
        "- Provides user feedback\n",
        "\n",
        "**How**: Runs specified number of epochs, calling train_epoch and validate_epoch.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 65: `        print(\"Starting training...\")`**\n",
        "\n",
        "**What**: User feedback.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 66: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 67: `        for epoch in range(num_epochs):`**\n",
        "\n",
        "**What**: Loops through epochs.\n",
        "\n",
        "**Why**: Train for multiple passes through dataset.\n",
        "\n",
        "**How**: `range(num_epochs)` generates 0, 1, 2, ... num_epochs-1.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 68: `            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")`**\n",
        "\n",
        "**What**: Displays current epoch (1-indexed for humans).\n",
        "\n",
        "**Why**: Track progress.\n",
        "\n",
        "**How**: `epoch + 1` converts 0-indexed to 1-indexed.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 69: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 70: `            train_loss = self.train_epoch()`**\n",
        "\n",
        "**What**: Runs one training epoch.\n",
        "\n",
        "**Why**: Update parameters on training data.\n",
        "\n",
        "**How**: Calls train_epoch(), returns average loss.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 71: `            val_loss, val_acc = self.validate_epoch()`**\n",
        "\n",
        "**What**: Runs validation after training epoch.\n",
        "\n",
        "**Why**: Check generalization performance.\n",
        "\n",
        "**How**: Unpacks tuple return (avg_loss, accuracy).\n",
        "\n",
        "---\n",
        "\n",
        "**Line 72: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 73-75: Store metrics in history**\n",
        "\n",
        "**What**: Appends epoch metrics to history dictionary.\n",
        "\n",
        "**Why**: Enables plotting and analysis after training.\n",
        "\n",
        "**How**: Each list grows by one element per epoch.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 76: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 77: `            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")`**\n",
        "\n",
        "**What**: Displays epoch metrics.\n",
        "\n",
        "**Why**: Monitor training progress.\n",
        "\n",
        "**How**:\n",
        "- `:.4f` formats to 4 decimal places\n",
        "- `|` separates metrics visually\n",
        "\n",
        "**Example Output**: `Train Loss: 0.4523 | Val Loss: 0.5102 | Val Acc: 0.6667`\n",
        "\n",
        "---\n",
        "\n",
        "**Line 78: (blank line)**\n",
        "\n",
        "---\n",
        "\n",
        "**Line 79: `        print(\"\\nTraining complete!\")`**\n",
        "\n",
        "**What**: Final status message.\n",
        "\n",
        "---\n",
        "\n",
        "### Teaching Points\n",
        "\n",
        "**1. Training Loop Structure**:\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "```\n",
        "\n",
        "**2. Critical Method Calls**:\n",
        "- `model.train()` before training\n",
        "- `model.eval()` before validation\n",
        "- `optimizer.zero_grad()` before each backward\n",
        "- `torch.no_grad()` during validation\n",
        "\n",
        "**3. Why Separate train_epoch and validate_epoch**:\n",
        "- Modularity and reusability\n",
        "- Different behaviors (gradients on/off)\n",
        "- Clearer logic\n",
        "\n",
        "**4. Multi-label vs Multi-class**:\n",
        "- **Multi-class**: Softmax, one label (cat OR dog)\n",
        "- **Multi-label**: Sigmoid, multiple labels (angry AND sad)\n",
        "\n",
        "**5. Overfitting Diagnosis**:\n",
        "- Train loss decreasing, val loss increasing → Overfitting\n",
        "- Both decreasing → Good\n",
        "- Both high → Underfitting\n",
        "\n",
        "**6. Memory Management**:\n",
        "- `loss.item()` prevents graph retention\n",
        "- `torch.no_grad()` disables gradient tracking\n",
        "- `.cpu()` before accumulating prevents GPU memory leak"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be002d96",
      "metadata": {
        "id": "be002d96"
      },
      "source": [
        "## Section 11: Training Configuration\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c20f105",
      "metadata": {
        "id": "9c20f105"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e257768",
      "metadata": {
        "id": "3e257768"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `criterion = nn.BCEWithLogitsLoss()`**\n",
        "\n",
        "**What**: Creates the loss function for multi-label classification.\n",
        "\n",
        "**Why**:\n",
        "- **Binary Cross-Entropy** is the standard loss for binary/multi-label tasks\n",
        "- **WithLogits** means it accepts raw logits (no sigmoid needed)\n",
        "- Combines sigmoid and BCE for numerical stability\n",
        "- Prevents issues with extreme values (log(0), log(1))\n",
        "\n",
        "**How**:\n",
        "- Instantiates PyTorch's BCEWithLogitsLoss\n",
        "- Internally applies sigmoid then computes binary cross-entropy\n",
        "- Averages loss across all labels and samples\n",
        "\n",
        "**Mathematical Formula**:\n",
        "For each label:\n",
        "$$\\text{BCE} = -[y \\log(\\sigma(x)) + (1-y) \\log(1-\\sigma(x))]$$\n",
        "\n",
        "Where:\n",
        "- $y$ is ground truth (0 or 1)\n",
        "- $x$ is logit (raw score)\n",
        "- $\\sigma$ is sigmoid function: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "**Why Not Separate Sigmoid + BCE?**\n",
        "- **Numerical stability**: Combining operations prevents overflow/underflow\n",
        "- **log-sum-exp trick** for better precision\n",
        "- **Faster computation**: Fused operation\n",
        "\n",
        "**Multi-Label Behavior**:\n",
        "- Computes BCE for each of 5 labels independently\n",
        "- Averages across all 5 labels\n",
        "- Averages across batch\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Logits:     [2.3, -1.1, 0.8, -0.3, 1.5]\n",
        "Labels:     [1,   0,    1,   0,    1]\n",
        "Probs:      [0.91, 0.25, 0.69, 0.43, 0.82]\n",
        "BCE each:   [0.09, 0.29, 0.37, 0.33, 0.20]\n",
        "Mean:       0.26\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: `optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)`**\n",
        "\n",
        "**What**: Creates AdamW optimizer with specific hyperparameters.\n",
        "\n",
        "**Why**:\n",
        "- **AdamW** is state-of-the-art for transformer fine-tuning\n",
        "- Combines adaptive learning rates with proper weight decay\n",
        "- Handles sparse gradients well\n",
        "- More stable than vanilla Adam\n",
        "\n",
        "**How**:\n",
        "- `model.parameters()` provides all trainable parameters\n",
        "- `lr=2e-5` sets learning rate to 0.00002\n",
        "- `weight_decay=0.01` adds L2 regularization\n",
        "\n",
        "**Parameter Breakdown**:\n",
        "\n",
        "1. **model.parameters()**:\n",
        "   - Generator yielding all tensors with `requires_grad=True`\n",
        "   - Includes weights and biases from all layers\n",
        "   - ~110 million parameters for BERT-base\n",
        "\n",
        "2. **lr=2e-5 (Learning Rate)**:\n",
        "   - Controls step size during optimization\n",
        "   - `2e-5 = 0.00002` is small because we are fine-tuning\n",
        "   - Pretrained weights need gentle updates\n",
        "   - Too high → unstable training, loss spikes\n",
        "   - Too low → slow convergence\n",
        "\n",
        "**Why 2e-5 for Transformers?**:\n",
        "- Empirically found to work well for BERT fine-tuning\n",
        "- Original BERT paper recommendation\n",
        "- Pretrained models are already near optimal\n",
        "- Large updates would destroy learned representations\n",
        "\n",
        "3. **weight_decay=0.01**:\n",
        "   - L2 regularization penalty\n",
        "   - Prevents weights from growing too large\n",
        "   - Helps generalization\n",
        "   - AdamW decouples weight decay from gradient updates (better than Adam)\n",
        "\n",
        "**AdamW vs Adam**:\n",
        "- **Adam**: `param -= lr * (grad + wd * param)` (weight decay tied to gradient)\n",
        "- **AdamW**: `param -= lr * grad; param -= lr * wd * param` (decoupled)\n",
        "- AdamW prevents weight decay from being affected by gradient scaling\n",
        "\n",
        "**What AdamW Does Each Step**:\n",
        "1. Computes gradient moving average (momentum)\n",
        "2. Computes squared gradient moving average (adaptive LR)\n",
        "3. Updates parameters using adaptive learning rate\n",
        "4. Applies weight decay separately\n",
        "\n",
        "---\n",
        "\n",
        "**Line 4: `NUM_EPOCHS = 3`**\n",
        "\n",
        "**What**: Sets number of complete passes through training data.\n",
        "\n",
        "**Why**:\n",
        "- Fine-tuning transformers requires few epochs (3-5 typical)\n",
        "- More epochs → risk of overfitting\n",
        "- Pretrained models learn quickly on new tasks\n",
        "\n",
        "**How**: Constant defining training duration.\n",
        "\n",
        "**Why Only 3 Epochs?**:\n",
        "- Pretrained model is already 90% there\n",
        "- Task-specific head learns quickly\n",
        "- More epochs often hurt validation performance\n",
        "- Small datasets especially prone to overfitting\n",
        "\n",
        "**Typical Epoch Counts**:\n",
        "- **Training from scratch**: 50-200 epochs\n",
        "- **Fine-tuning transformers**: 2-5 epochs\n",
        "- **Small datasets**: Fewer epochs\n",
        "- **Large datasets**: Can handle more epochs\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)`**\n",
        "\n",
        "**What**: Instantiates the Trainer class with all training components.\n",
        "\n",
        "**Why**: Packages everything needed for training into single object.\n",
        "\n",
        "**How**:\n",
        "- Calls `Trainer.__init__()`\n",
        "- Stores all arguments as instance variables\n",
        "- Initializes empty history dictionary\n",
        "\n",
        "**What Gets Stored**:\n",
        "- `self.model`: EmotionClassifier instance\n",
        "- `self.train_loader`: Training DataLoader (8 samples, batch_size=4 → 2 batches)\n",
        "- `self.val_loader`: Validation DataLoader (2 samples, batch_size=4 → 1 batch)\n",
        "- `self.criterion`: BCEWithLogitsLoss instance\n",
        "- `self.optimizer`: AdamW instance\n",
        "- `self.device`: 'cuda' or 'cpu'\n",
        "- `self.history`: {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "**Ready to Train**: Can now call `trainer.train(NUM_EPOCHS)`.\n",
        "\n",
        "---\n",
        "\n",
        "### Teaching Points\n",
        "\n",
        "**1. Loss Function Selection Guide**:\n",
        "- **Binary classification**: BCEWithLogitsLoss\n",
        "- **Multi-class classification**: CrossEntropyLoss\n",
        "- **Multi-label classification**: BCEWithLogitsLoss (our case)\n",
        "- **Regression**: MSELoss, L1Loss\n",
        "\n",
        "**2. Optimizer Selection Guide**:\n",
        "- **Transformers**: AdamW (industry standard)\n",
        "- **CNNs**: SGD with momentum, Adam\n",
        "- **RNNs**: Adam, RMSprop\n",
        "- **General**: Adam is good default\n",
        "\n",
        "**3. Learning Rate Guidelines**:\n",
        "- **Fine-tuning transformers**: 1e-5 to 5e-5\n",
        "- **Training from scratch**: 1e-3 to 1e-4\n",
        "- **Too high symptoms**: Loss explodes, NaN values\n",
        "- **Too low symptoms**: Slow convergence, plateaus early\n",
        "\n",
        "**4. Weight Decay Purpose**:\n",
        "- Prevents overfitting\n",
        "- Keeps weights small\n",
        "- Improves generalization\n",
        "- Typical values: 0.01 to 0.1\n",
        "\n",
        "**5. Why BCEWithLogitsLoss vs BCE + Sigmoid?**:\n",
        "```python\n",
        "# Numerically unstable\n",
        "probs = torch.sigmoid(logits)\n",
        "loss = F.binary_cross_entropy(probs, labels)\n",
        "\n",
        "# Stable (preferred)\n",
        "loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "```\n",
        "\n",
        "**6. Epoch vs Iteration**:\n",
        "- **Epoch**: One complete pass through entire dataset\n",
        "- **Iteration/Step**: One batch processed\n",
        "- **With 8 samples, batch_size=4**: 1 epoch = 2 iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59755fdb",
      "metadata": {
        "id": "59755fdb"
      },
      "source": [
        "## Section 12: Execute Training\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4f8177",
      "metadata": {
        "id": "0a4f8177"
      },
      "outputs": [],
      "source": [
        "trainer.train(NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec932d86",
      "metadata": {
        "id": "ec932d86"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `trainer.train(NUM_EPOCHS)`**\n",
        "\n",
        "**What**: Executes the complete training process for 3 epochs.\n",
        "\n",
        "**Why**: This single line triggers all training and validation logic.\n",
        "\n",
        "**How**:\n",
        "- Calls `Trainer.train()` method with `num_epochs=3`\n",
        "- Runs 3 complete passes through training data\n",
        "- Validates after each epoch\n",
        "- Updates model parameters via backpropagation\n",
        "- Prints progress and metrics\n",
        "\n",
        "**What Happens When This Executes**:\n",
        "\n",
        "1. **Prints**: \"Starting training...\"\n",
        "\n",
        "2. **For Epoch 1**:\n",
        "   - Prints: \"Epoch 1/3\"\n",
        "   - **Training Phase** (train_epoch):\n",
        "     - Sets model to training mode (`model.train()`)\n",
        "     - Progress bar: \"Training: 100%|██████| 2/2 [00:XX<00:00]\"\n",
        "     - Processes 2 batches (8 samples ÷ 4 batch_size = 2)\n",
        "     - Each batch: forward pass → loss → backward → optimizer step\n",
        "     - Returns average training loss\n",
        "   - **Validation Phase** (validate_epoch):\n",
        "     - Sets model to eval mode (`model.eval()`)\n",
        "     - Progress bar: \"Validation: 100%|██████| 1/1 [00:XX<00:00]\"\n",
        "     - Processes 1 batch (2 samples with batch_size=4)\n",
        "     - No gradient computation (`torch.no_grad()`)\n",
        "     - Computes predictions and metrics\n",
        "     - Returns validation loss and accuracy\n",
        "   - **Stores metrics** in history\n",
        "   - Prints: \"Train Loss: X.XXXX | Val Loss: X.XXXX | Val Acc: X.XXXX\"\n",
        "\n",
        "3. **Repeats for Epochs 2 and 3**\n",
        "\n",
        "4. **Prints**: \"Training complete!\"\n",
        "\n",
        "**Expected Output Example**:\n",
        "```\n",
        "Starting training...\n",
        "\n",
        "Epoch 1/3\n",
        "Training: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]\n",
        "Validation: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
        "Train Loss: 0.6234 | Val Loss: 0.5891 | Val Acc: 0.5000\n",
        "\n",
        "Epoch 2/3\n",
        "Training: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n",
        "Validation: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
        "Train Loss: 0.4512 | Val Loss: 0.4823 | Val Acc: 0.5000\n",
        "\n",
        "Epoch 3/3\n",
        "Training: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
        "Validation: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
        "Train Loss: 0.3245 | Val Loss: 0.4156 | Val Acc: 1.0000\n",
        "\n",
        "Training complete!\n",
        "```\n",
        "\n",
        "**Interpreting Results**:\n",
        "\n",
        "**Train Loss Decreasing**: Model is learning from training data.\n",
        "\n",
        "**Val Loss Behavior**:\n",
        "- Decreasing with train loss → Good generalization\n",
        "- Increasing while train loss decreases → Overfitting\n",
        "- Staying flat → Model not learning meaningful patterns\n",
        "\n",
        "**Val Accuracy**:\n",
        "- Remember this is exact-match accuracy (all 5 labels must match)\n",
        "- 0.5000 = 1 out of 2 validation samples correct\n",
        "- 1.0000 = 2 out of 2 validation samples correct\n",
        "- More lenient metrics (per-label F1) typically higher\n",
        "\n",
        "**Training Time**:\n",
        "- GPU: ~5-10 seconds per epoch\n",
        "- CPU: ~30-60 seconds per epoch\n",
        "- Small dataset so very fast\n",
        "\n",
        "**After This Cell Completes**:\n",
        "- Model parameters have been updated\n",
        "- `trainer.history` contains metrics for plotting\n",
        "- Model is ready for evaluation or inference\n",
        "\n",
        "---\n",
        "\n",
        "### Teaching Points\n",
        "\n",
        "**1. What Actually Changes During Training**:\n",
        "- **Before**: Model makes random predictions\n",
        "- **After**: Model learns patterns correlating text with emotions\n",
        "- **Mechanism**: ~110M parameters adjusted via gradient descent\n",
        "\n",
        "**2. Progress Bar Information**:\n",
        "```\n",
        "Training: 100%|██████████| 2/2 [00:03<00:00, 1.50s/it]\n",
        "          ^^^^           ^^^   ^^^^^^^^^^^^^^^^^^^^^^\n",
        "          %done        cur/tot  [elapsed<remaining, speed]\n",
        "```\n",
        "\n",
        "**3. Training Dynamics**:\n",
        "- **Early epochs**: Large loss reductions, fast learning\n",
        "- **Later epochs**: Smaller improvements, fine-tuning\n",
        "- **Plateau**: Loss stops improving → may need to stop\n",
        "\n",
        "**4. Why Validation After Each Epoch**:\n",
        "- Monitor generalization continuously\n",
        "- Detect overfitting early\n",
        "- Choose best checkpoint (not necessarily last epoch)\n",
        "\n",
        "**5. Memory Usage During Training**:\n",
        "- **Model**: ~450MB (BERT parameters)\n",
        "- **Optimizer state**: ~900MB (AdamW maintains momentum)\n",
        "- **Activations**: ~200MB (forward pass intermediate values)\n",
        "- **Gradients**: ~450MB (same size as model)\n",
        "- **Total**: ~2GB GPU memory for this small example\n",
        "\n",
        "**6. What If Training Fails**:\n",
        "- **Loss → NaN**: Learning rate too high, reduce by 10x\n",
        "- **Loss not decreasing**: Learning rate too low, increase\n",
        "- **CUDA out of memory**: Reduce batch_size\n",
        "- **Very slow**: Check if GPU is being used (`device`)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59879e53",
      "metadata": {
        "id": "59879e53"
      },
      "source": [
        "## Section 13: Evaluator Class\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a986f5",
      "metadata": {
        "id": "f1a986f5"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "\n",
        "    def __init__(self, model: nn.Module, val_loader: DataLoader, device: torch.device):\n",
        "        self.model = model\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate(self) -> dict:\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, desc=\"Evaluating\"):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.sigmoid(logits) > 0.5\n",
        "\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_labels = np.vstack(all_labels)\n",
        "\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_micro = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "        recall_micro = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "        f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision_macro': precision_macro,\n",
        "            'recall_macro': recall_macro,\n",
        "            'f1_macro': f1_macro,\n",
        "            'precision_micro': precision_micro,\n",
        "            'recall_micro': recall_micro,\n",
        "            'f1_micro': f1_micro,\n",
        "            'predictions': all_preds,\n",
        "            'labels': all_labels\n",
        "        }\n",
        "\n",
        "evaluator = Evaluator(model, val_loader, device)\n",
        "results = evaluator.evaluate()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Exact Match Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"\\nMacro Metrics (average across labels):\")\n",
        "print(f\"  Precision: {results['precision_macro']:.4f}\")\n",
        "print(f\"  Recall:    {results['recall_macro']:.4f}\")\n",
        "print(f\"  F1 Score:  {results['f1_macro']:.4f}\")\n",
        "print(f\"\\nMicro Metrics (aggregate all labels):\")\n",
        "print(f\"  Precision: {results['precision_micro']:.4f}\")\n",
        "print(f\"  Recall:    {results['recall_micro']:.4f}\")\n",
        "print(f\"  F1 Score:  {results['f1_micro']:.4f}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e61bba1",
      "metadata": {
        "id": "6e61bba1"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "This section evaluates the trained model comprehensively. The Evaluator class is simpler than Trainer since it only runs inference without updating parameters.\n",
        "\n",
        "**Lines 1-6: Class initialization**\n",
        "\n",
        "Similar to Trainer but simpler - only needs model, val_loader, and device. No optimizer or criterion needed since we are not training.\n",
        "\n",
        "**Lines 8-22: evaluate method - Prediction gathering**\n",
        "\n",
        "Nearly identical to `Trainer.validate_epoch()` but without loss computation. The key is gathering all predictions and labels for comprehensive metric calculation.\n",
        "\n",
        "**Line 13**: `torch.no_grad()` disables gradient tracking for inference efficiency.\n",
        "\n",
        "**Line 19**: `torch.sigmoid(logits) > 0.5` converts raw scores to binary predictions.\n",
        "\n",
        "**Lines 24-25: Stack arrays**\n",
        "\n",
        "`np.vstack()` combines batch-wise predictions into single arrays for metric computation.\n",
        "\n",
        "**Line 27: `accuracy = accuracy_score(all_labels, all_preds)`**\n",
        "\n",
        "**What**: Exact match accuracy - fraction of samples where ALL labels are correct.\n",
        "\n",
        "**Why**: Strictest metric for multi-label classification.\n",
        "\n",
        "**How**: Only counts as correct if all 5 predictions match all 5 ground truths.\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Sample 1: Pred [1,0,1,0,1] vs True [1,0,1,0,1] → Correct\n",
        "Sample 2: Pred [0,1,0,1,0] vs True [0,1,0,0,0] → Incorrect (4th label differs)\n",
        "Accuracy: 1/2 = 0.5\n",
        "```\n",
        "\n",
        "**Line 28: `precision_score(all_labels, all_preds, average='macro', zero_division=0)`**\n",
        "\n",
        "**What**: Macro-averaged precision across all labels.\n",
        "\n",
        "**Why**:\n",
        "- **Precision** = True Positives / (True Positives + False Positives)\n",
        "- Answers: \"Of all positive predictions, how many were correct?\"\n",
        "- **Macro** averages metrics per label, then averages across labels\n",
        "- Treats all labels equally regardless of frequency\n",
        "\n",
        "**How**:\n",
        "1. Compute precision for each of 5 labels separately\n",
        "2. Average the 5 precision scores\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Label 0 (anger):    Precision = 0.8\n",
        "Label 1 (fear):     Precision = 0.6\n",
        "Label 2 (joy):      Precision = 1.0\n",
        "Label 3 (sadness):  Precision = 0.7\n",
        "Label 4 (surprise): Precision = 0.9\n",
        "Macro Precision = (0.8 + 0.6 + 1.0 + 0.7 + 0.9) / 5 = 0.8\n",
        "```\n",
        "\n",
        "**zero_division=0**: If a label has zero predictions, precision defaults to 0 instead of undefined.\n",
        "\n",
        "**Line 29: `recall_score(..., average='macro')`**\n",
        "\n",
        "**What**: Macro-averaged recall across all labels.\n",
        "\n",
        "**Why**:\n",
        "- **Recall** = True Positives / (True Positives + False Negatives)\n",
        "- Answers: \"Of all actual positives, how many did we find?\"\n",
        "- **Macro** treats each label equally\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Label 0: 3 actual positives, predicted 2 correctly → Recall = 2/3 = 0.67\n",
        "Label 1: 2 actual positives, predicted 2 correctly → Recall = 2/2 = 1.0\n",
        "Macro Recall = (0.67 + 1.0 + ...) / 5\n",
        "```\n",
        "\n",
        "**Line 30: `f1_score(..., average='macro')`**\n",
        "\n",
        "**What**: Macro-averaged F1 score.\n",
        "\n",
        "**Why**:\n",
        "- **F1** = Harmonic mean of precision and recall\n",
        "- Balances precision and recall\n",
        "- Better than accuracy for imbalanced datasets\n",
        "\n",
        "**How**:\n",
        "$$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "**Why Harmonic Mean?**: Punishes extreme values. If precision=1.0 but recall=0.1, F1=0.18 (not 0.55 like arithmetic mean).\n",
        "\n",
        "**Lines 32-34: Micro-averaged metrics**\n",
        "\n",
        "**What**: Micro-averaging computes metrics globally by counting total TP, FP, FN.\n",
        "\n",
        "**Why**: Gives more weight to labels with more samples.\n",
        "\n",
        "**How**:\n",
        "1. Sum all true positives across all labels\n",
        "2. Sum all false positives across all labels\n",
        "3. Compute single precision/recall/F1 from totals\n",
        "\n",
        "**Macro vs Micro**:\n",
        "\n",
        "**Macro (Lines 28-30)**:\n",
        "- Compute metric for each label\n",
        "- Average the metrics\n",
        "- Each label weighted equally\n",
        "\n",
        "**Micro (Lines 32-34)**:\n",
        "- Pool all predictions together\n",
        "- Compute metric on the pool\n",
        "- Labels with more instances have more influence\n",
        "\n",
        "**Example**:\n",
        "\n",
        "```\n",
        "Label 0: 100 samples, Precision=0.9\n",
        "Label 1: 10 samples,  Precision=0.5\n",
        "\n",
        "Macro Precision = (0.9 + 0.5) / 2 = 0.7\n",
        "Micro Precision = (90 + 5) / (100 + 10) = 0.86\n",
        "```\n",
        "\n",
        "Micro gives more weight to Label 0 because it has more samples.\n",
        "\n",
        "**Lines 36-44: Return dictionary**\n",
        "\n",
        "**What**: Returns comprehensive results dictionary.\n",
        "\n",
        "**Why**: Provides multiple evaluation perspectives and raw data.\n",
        "\n",
        "**How**: Dictionary with 9 keys covering all metrics plus raw predictions/labels.\n",
        "\n",
        "**Line 46: `evaluator = Evaluator(model, val_loader, device)`**\n",
        "\n",
        "**What**: Creates evaluator instance.\n",
        "\n",
        "**Why**: Encapsulates evaluation logic.\n",
        "\n",
        "**Line 47: `results = evaluator.evaluate()`**\n",
        "\n",
        "**What**: Runs complete evaluation and stores results.\n",
        "\n",
        "**Why**: Generates all metrics in one call.\n",
        "\n",
        "**Lines 49-62: Print results**\n",
        "\n",
        "**What**: Displays formatted evaluation metrics.\n",
        "\n",
        "**Why**: Human-readable summary of model performance.\n",
        "\n",
        "**How**:\n",
        "- `\"=\" * 50` creates separator line\n",
        "- `.4f` formats to 4 decimal places\n",
        "- Groups metrics logically (exact match, macro, micro)\n",
        "\n",
        "**Expected Output**:\n",
        "```\n",
        "==================================================\n",
        "EVALUATION RESULTS\n",
        "==================================================\n",
        "Exact Match Accuracy: 0.5000\n",
        "\n",
        "Macro Metrics (average across labels):\n",
        "  Precision: 0.7000\n",
        "  Recall:    0.6500\n",
        "  F1 Score:  0.6700\n",
        "\n",
        "Micro Metrics (aggregate all labels):\n",
        "  Precision: 0.7500\n",
        "  Recall:    0.6800\n",
        "  F1 Score:  0.7100\n",
        "==================================================\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Teaching Points\n",
        "\n",
        "**1. Precision vs Recall Trade-off**:\n",
        "- **High Precision, Low Recall**: Conservative predictions (few false positives, many misses)\n",
        "- **High Recall, Low Precision**: Aggressive predictions (few misses, many false alarms)\n",
        "- **F1 Score**: Balances both\n",
        "\n",
        "**2. When to Use Each Metric**:\n",
        "- **Accuracy**: Balanced classes, all labels equally important\n",
        "- **Precision**: Cost of false positives is high (spam detection)\n",
        "- **Recall**: Cost of false negatives is high (disease diagnosis)\n",
        "- **F1**: Balance precision and recall\n",
        "\n",
        "**3. Macro vs Micro Guide**:\n",
        "- **Use Macro** when:\n",
        "  - All labels should be treated equally\n",
        "  - Care about rare classes\n",
        "  - Imbalanced dataset\n",
        "- **Use Micro** when:\n",
        "  - Overall performance matters most\n",
        "  - Larger classes are more important\n",
        "  - Want single aggregate metric\n",
        "\n",
        "**4. Multi-label Specifics**:\n",
        "- Each sample can have 0, 1, or multiple labels\n",
        "- Metrics computed per-label then aggregated\n",
        "- More complex than multi-class (only one label)\n",
        "\n",
        "**5. zero_division Parameter**:\n",
        "- Handles edge case when no predictions for a label\n",
        "- Options: 0, 1, or warn\n",
        "- Common in early training or rare classes\n",
        "\n",
        "**6. Why Separate Evaluator Class**:\n",
        "- Reusability for different datasets\n",
        "- Cleaner than inline evaluation\n",
        "- Easy to extend with more metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f647d978",
      "metadata": {
        "id": "f647d978"
      },
      "source": [
        "## Section 14: Per-Label Performance Analysis\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8042e19b",
      "metadata": {
        "id": "8042e19b"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPer-Label Performance:\")\n",
        "print(\"=\" * 70)\n",
        "emotion_names = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "for idx, emotion in enumerate(emotion_names):\n",
        "    precision = precision_score(results['labels'][:, idx], results['predictions'][:, idx], zero_division=0)\n",
        "    recall = recall_score(results['labels'][:, idx], results['predictions'][:, idx], zero_division=0)\n",
        "    f1 = f1_score(results['labels'][:, idx], results['predictions'][:, idx], zero_division=0)\n",
        "\n",
        "    print(f\"{emotion.capitalize():10s} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863242a0",
      "metadata": {
        "id": "863242a0"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Line 1: `print(\"\\nPer-Label Performance:\")`**\n",
        "\n",
        "**What**: Prints section header.\n",
        "\n",
        "**Why**: Organizes output for readability.\n",
        "\n",
        "**How**: `\\n` adds newline for spacing.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 2: `print(\"=\" * 70)`**\n",
        "\n",
        "**What**: Prints separator line of 70 equal signs.\n",
        "\n",
        "**Why**: Visual separation and formatting.\n",
        "\n",
        "**How**: String multiplication creates \"===...===\" with 70 characters.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `emotion_names = ['anger', 'fear', 'joy', 'sadness', 'surprise']`**\n",
        "\n",
        "**What**: Defines list of emotion labels matching column order.\n",
        "\n",
        "**Why**:\n",
        "- Maps column indices (0-4) to human-readable names\n",
        "- Must match order used during dataset creation\n",
        "- Enables meaningful output instead of \"Label 0, Label 1, etc.\"\n",
        "\n",
        "**How**: Simple Python list with 5 strings.\n",
        "\n",
        "**CRITICAL**: Order must match the original label columns in the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 5: `for idx, emotion in enumerate(emotion_names):`**\n",
        "\n",
        "**What**: Iterates through emotions with their indices.\n",
        "\n",
        "**Why**: Need index to extract correct column from predictions/labels array.\n",
        "\n",
        "**How**:\n",
        "- `enumerate()` returns tuples (index, value)\n",
        "- `idx`: 0, 1, 2, 3, 4\n",
        "- `emotion`: 'anger', 'fear', 'joy', 'sadness', 'surprise'\n",
        "\n",
        "**Example iterations**:\n",
        "```\n",
        "idx=0, emotion='anger'\n",
        "idx=1, emotion='fear'\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 6: `precision = precision_score(results['labels'][:, idx], results['predictions'][:, idx], zero_division=0)`**\n",
        "\n",
        "**What**: Computes precision for a single emotion label.\n",
        "\n",
        "**Why**: Per-label metrics reveal which emotions the model handles well vs poorly.\n",
        "\n",
        "**How**:\n",
        "- `results['labels'][:, idx]` extracts column `idx` from labels array\n",
        "- `:` selects all rows (all samples)\n",
        "- `, idx` selects specific column (specific emotion)\n",
        "- Shape: (num_samples,) - 1D array for one emotion\n",
        "\n",
        "**Array Slicing Example**:\n",
        "```\n",
        "results['labels'] shape: (2, 5)\n",
        "results['labels'][:, 0] extracts column 0 (anger):\n",
        "  [1, 0] (2 samples' anger labels)\n",
        "```\n",
        "\n",
        "**Precision Interpretation**:\n",
        "- 1.0: All predicted positives were correct\n",
        "- 0.5: Half of predicted positives were wrong\n",
        "- 0.0: No correct positive predictions\n",
        "\n",
        "---\n",
        "\n",
        "**Line 7: `recall = recall_score(results['labels'][:, idx], results['predictions'][:, idx], zero_division=0)`**\n",
        "\n",
        "**What**: Computes recall for single emotion.\n",
        "\n",
        "**Why**: Measures what fraction of actual positives were found.\n",
        "\n",
        "**How**: Same slicing as precision.\n",
        "\n",
        "**Recall Interpretation**:\n",
        "- 1.0: Found all actual positives\n",
        "- 0.5: Missed half of actual positives\n",
        "- 0.0: Missed all actual positives\n",
        "\n",
        "---\n",
        "\n",
        "**Line 8: `f1 = f1_score(results['labels'][:, idx], results['predictions'][:, idx], zero_division=0)`**\n",
        "\n",
        "**What**: Computes F1 score for single emotion.\n",
        "\n",
        "**Why**: Harmonic mean balancing precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 10: Print formatted results**\n",
        "\n",
        "**What**: Displays metrics for current emotion.\n",
        "\n",
        "**Why**: Human-readable per-label performance.\n",
        "\n",
        "**How**:\n",
        "- `{emotion.capitalize():10s}`: Capitalizes emotion name, pads to 10 characters\n",
        "- `{precision:.4f}`: Formats precision to 4 decimals\n",
        "- `|`: Visual separator\n",
        "\n",
        "**String Formatting Breakdown**:\n",
        "- `:10s` means \"string padded to 10 characters\"\n",
        "- `:.4f` means \"float with 4 decimal places\"\n",
        "- `Anger     | Precision: 0.8500 | Recall: 0.7500 | F1: 0.8000`\n",
        "\n",
        "**Example Output**:\n",
        "```\n",
        "==================================================\n",
        "Per-Label Performance:\n",
        "==================================================\n",
        "Anger      | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000\n",
        "Fear       | Precision: 0.5000 | Recall: 1.0000 | F1: 0.6667\n",
        "Joy        | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000\n",
        "Sadness    | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000\n",
        "Surprise   | Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000\n",
        "==================================================\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Teaching Points\n",
        "\n",
        "**1. Why Per-Label Analysis Matters**:\n",
        "- **Aggregated metrics** hide label-specific issues\n",
        "- One label might be easy (Joy: F1=1.0)\n",
        "- Another might be hard (Sadness: F1=0.0)\n",
        "- Reveals which emotions need more data or features\n",
        "\n",
        "**2. Interpreting Per-Label Results**:\n",
        "\n",
        "**High Precision, Low Recall**:\n",
        "- Model is conservative for this label\n",
        "- Rarely predicts it, but accurate when it does\n",
        "- Fix: Lower threshold or add more positive training examples\n",
        "\n",
        "**Low Precision, High Recall**:\n",
        "- Model is aggressive for this label\n",
        "- Predicts it often, but many false alarms\n",
        "- Fix: Raise threshold or improve feature learning\n",
        "\n",
        "**Both Low**:\n",
        "- Model doesn't understand this label\n",
        "- Needs more training data or better features\n",
        "- May be ambiguous label\n",
        "\n",
        "**3. Array Slicing Reminder**:\n",
        "```python\n",
        "array = [[1,0,1,0,1],   # Sample 0\n",
        "         [0,1,0,1,0]]   # Sample 1\n",
        "\n",
        "array[:, 0]  # All rows, column 0: [1, 0] (anger for both samples)\n",
        "array[0, :]  # Row 0, all columns: [1,0,1,0,1] (all labels for sample 0)\n",
        "array[1, 2]  # Row 1, column 2: 0 (joy for sample 1)\n",
        "```\n",
        "\n",
        "**4. Column Order Importance**:\n",
        "If emotion_names doesn't match dataset creation order, results will be mislabeled:\n",
        "```python\n",
        "# Dataset creation: ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
        "# Wrong order: ['joy', 'anger', 'fear', 'surprise', 'sadness']\n",
        "# Result: Metrics attributed to wrong emotions!\n",
        "```\n",
        "\n",
        "**5. zero_division Parameter**:\n",
        "- If no samples have ground truth=1 for a label, recall is undefined\n",
        "- If no samples have prediction=1 for a label, precision is undefined\n",
        "- `zero_division=0` treats undefined as 0.0\n",
        "- Alternative: `zero_division=1` treats as 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd46e49",
      "metadata": {
        "id": "ddd46e49"
      },
      "source": [
        "## Section 15: Inference on New Text\n",
        "\n",
        "### Code Block Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcdab5c7",
      "metadata": {
        "id": "bcdab5c7"
      },
      "outputs": [],
      "source": [
        "def predict_emotions(text: str, model: nn.Module, tokenizer, device: torch.device,\n",
        "                     threshold: float = 0.5) -> dict:\n",
        "    model.eval()\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "    predictions = {\n",
        "        'anger': probs[0],\n",
        "        'fear': probs[1],\n",
        "        'joy': probs[2],\n",
        "        'sadness': probs[3],\n",
        "        'surprise': probs[4]\n",
        "    }\n",
        "\n",
        "    detected_emotions = [emotion for emotion, prob in predictions.items() if prob > threshold]\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'probabilities': predictions,\n",
        "        'detected_emotions': detected_emotions\n",
        "    }\n",
        "\n",
        "test_texts = [\n",
        "    \"I am so happy today!\",\n",
        "    \"This is terrifying and awful.\",\n",
        "    \"I can't believe this happened!\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INFERENCE EXAMPLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for text in test_texts:\n",
        "    result = predict_emotions(text, model, tokenizer, device)\n",
        "\n",
        "    print(f\"\\nText: {result['text']}\")\n",
        "    print(\"Probabilities:\")\n",
        "    for emotion, prob in result['probabilities'].items():\n",
        "        print(f\"  {emotion.capitalize():10s}: {prob:.4f}\")\n",
        "    print(f\"Detected emotions: {', '.join(result['detected_emotions']) if result['detected_emotions'] else 'None'}\")\n",
        "    print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88bef619",
      "metadata": {
        "id": "88bef619"
      },
      "source": [
        "### Line-by-Line Breakdown\n",
        "\n",
        "**Lines 1-2: Function definition**\n",
        "\n",
        "**What**: Defines function for predicting emotions on arbitrary text.\n",
        "\n",
        "**Why**:\n",
        "- Enables real-world usage of trained model\n",
        "- Demonstrates complete inference pipeline\n",
        "- Encapsulates prediction logic for reusability\n",
        "\n",
        "**How**:\n",
        "- `text`: Input string to classify\n",
        "- `model`: Trained neural network\n",
        "- `tokenizer`: BERT tokenizer for encoding\n",
        "- `device`: GPU or CPU\n",
        "- `threshold`: Probability cutoff for positive prediction (default 0.5)\n",
        "- Returns dictionary with text, probabilities, and detected emotions\n",
        "\n",
        "---\n",
        "\n",
        "**Line 3: `model.eval()`**\n",
        "\n",
        "**What**: Sets model to evaluation mode.\n",
        "\n",
        "**Why**: **CRITICAL** - Disables dropout for consistent predictions.\n",
        "\n",
        "**How**: Without this, predictions would be random due to active dropout.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 5-11: Tokenization**\n",
        "\n",
        "**What**: Encodes text into token IDs.\n",
        "\n",
        "**Why**: Neural network cannot process raw text, needs numeric representation.\n",
        "\n",
        "**How**: Same tokenization as training data:\n",
        "- Special tokens ([CLS], [SEP])\n",
        "- Padding to MAX_LENGTH (128)\n",
        "- Truncation if too long\n",
        "- Returns PyTorch tensors\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Input: \"I am so happy today!\"\n",
        "Tokens: [CLS] i am so happy today ! [SEP] [PAD] [PAD] ...\n",
        "IDs: [101, 1045, 2572, 2061, 3407, 2651, 999, 102, 0, 0, ...]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 13-14: Move to device**\n",
        "\n",
        "**What**: Transfers input tensors to GPU/CPU.\n",
        "\n",
        "**Why**: Model and inputs must be on same device.\n",
        "\n",
        "**How**: `.to(device)` handles transfer.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 16: `with torch.no_grad():`**\n",
        "\n",
        "**What**: Disables gradient computation.\n",
        "\n",
        "**Why**: Inference doesn't need gradients, saves memory and time.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 17: `logits = model(input_ids=input_ids, attention_mask=attention_mask)`**\n",
        "\n",
        "**What**: Forward pass through model.\n",
        "\n",
        "**Why**: Get raw predictions.\n",
        "\n",
        "**How**:\n",
        "- Processes text through BERT\n",
        "- Returns logits of shape (1, 5) - one sample, 5 emotions\n",
        "- No gradient tracking\n",
        "\n",
        "---\n",
        "\n",
        "**Line 18: `probs = torch.sigmoid(logits).cpu().numpy()[0]`**\n",
        "\n",
        "**What**: Converts logits to probabilities and extracts values.\n",
        "\n",
        "**Why**: Probabilities (0-1) are interpretable, logits are not.\n",
        "\n",
        "**How**:\n",
        "- `torch.sigmoid(logits)`: Applies sigmoid activation\n",
        "  - Maps logits to (0, 1) range\n",
        "  - Each value is independent probability\n",
        "- `.cpu()`: Moves to CPU (required for NumPy)\n",
        "- `.numpy()`: Converts to NumPy array\n",
        "- `[0]`: Extracts first (only) sample\n",
        "- Result: 1D array of 5 probabilities\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Logits:  [2.3, -1.1, 0.8, -0.3, 1.5]\n",
        "Sigmoid: [0.91, 0.25, 0.69, 0.43, 0.82]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 20-26: Create probabilities dictionary**\n",
        "\n",
        "**What**: Maps emotion names to their probabilities.\n",
        "\n",
        "**Why**: Human-readable output instead of unnamed array.\n",
        "\n",
        "**How**:\n",
        "- `probs[0]` is probability for anger\n",
        "- `probs[1]` is probability for fear\n",
        "- etc.\n",
        "\n",
        "**Result**:\n",
        "```python\n",
        "{\n",
        "    'anger': 0.9091,\n",
        "    'fear': 0.2497,\n",
        "    'joy': 0.6900,\n",
        "    'sadness': 0.4256,\n",
        "    'surprise': 0.8176\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Line 28: `detected_emotions = [emotion for emotion, prob in predictions.items() if prob > threshold]`**\n",
        "\n",
        "**What**: Filters emotions above threshold into list.\n",
        "\n",
        "**Why**: Binary decision - which emotions are present?\n",
        "\n",
        "**How**:\n",
        "- List comprehension iterates through predictions\n",
        "- Keeps only emotions with probability > 0.5\n",
        "- Returns list of emotion names\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Probabilities: {'anger': 0.91, 'fear': 0.25, 'joy': 0.69, 'sadness': 0.43, 'surprise': 0.82}\n",
        "Threshold: 0.5\n",
        "Detected: ['anger', 'joy', 'surprise']\n",
        "```\n",
        "\n",
        "**Threshold Impact**:\n",
        "- **0.5**: Balanced (default)\n",
        "- **0.7**: Conservative (fewer false positives)\n",
        "- **0.3**: Aggressive (fewer false negatives)\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 30-33: Return dictionary**\n",
        "\n",
        "**What**: Returns comprehensive prediction results.\n",
        "\n",
        "**Why**: Provides both probabilities (soft) and binary decisions (hard).\n",
        "\n",
        "**How**: Dictionary with three keys:\n",
        "- `text`: Original input\n",
        "- `probabilities`: All 5 probabilities\n",
        "- `detected_emotions`: List of predicted emotions\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 35-38: Test texts**\n",
        "\n",
        "**What**: Defines sample inputs for demonstration.\n",
        "\n",
        "**Why**: Show model predictions on diverse examples.\n",
        "\n",
        "**How**: List of three strings covering different emotions.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 40-42: Print header**\n",
        "\n",
        "**What**: Formats output section.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 44: `for text in test_texts:`**\n",
        "\n",
        "**What**: Iterates through test examples.\n",
        "\n",
        "---\n",
        "\n",
        "**Line 45: `result = predict_emotions(text, model, tokenizer, device)`**\n",
        "\n",
        "**What**: Gets predictions for current text.\n",
        "\n",
        "**Why**: Demonstrates function usage.\n",
        "\n",
        "**How**: Calls predict_emotions, stores result dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 47-53: Print results**\n",
        "\n",
        "**What**: Displays formatted predictions.\n",
        "\n",
        "**Why**: Human-readable output showing probabilities and decisions.\n",
        "\n",
        "**How**:\n",
        "- Line 47: Shows input text\n",
        "- Lines 49-51: Loops through probabilities, formats to 4 decimals\n",
        "- Line 52: Shows detected emotions or \"None\"\n",
        "- Line 53: Separator line\n",
        "\n",
        "**Example Output**:\n",
        "```\n",
        "==================================================\n",
        "INFERENCE EXAMPLES\n",
        "==================================================\n",
        "\n",
        "Text: I am so happy today!\n",
        "Probabilities:\n",
        "  Anger     : 0.0234\n",
        "  Fear      : 0.0189\n",
        "  Joy       : 0.9876\n",
        "  Sadness   : 0.0123\n",
        "  Surprise  : 0.3456\n",
        "Detected emotions: joy\n",
        "--------------------------------------------------\n",
        "\n",
        "Text: This is terrifying and awful.\n",
        "Probabilities:\n",
        "  Anger     : 0.4523\n",
        "  Fear      : 0.8901\n",
        "  Joy       : 0.0234\n",
        "  Sadness   : 0.6789\n",
        "  Surprise  : 0.1234\n",
        "Detected emotions: fear, sadness\n",
        "--------------------------------------------------\n",
        "\n",
        "Text: I can't believe this happened!\n",
        "Probabilities:\n",
        "  Anger     : 0.2345\n",
        "  Fear      : 0.3456\n",
        "  Joy       : 0.1234\n",
        "  Sadness   : 0.2345\n",
        "  Surprise  : 0.8901\n",
        "Detected emotions: surprise\n",
        "--------------------------------------------------\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Teaching Points\n",
        "\n",
        "**1. Inference Pipeline**:\n",
        "```\n",
        "Raw Text\n",
        "  ↓ (Tokenization)\n",
        "Token IDs + Attention Mask\n",
        "  ↓ (Move to device)\n",
        "Tensors on GPU/CPU\n",
        "  ↓ (Model forward pass)\n",
        "Logits\n",
        "  ↓ (Sigmoid)\n",
        "Probabilities\n",
        "  ↓ (Thresholding)\n",
        "Binary Predictions\n",
        "```\n",
        "\n",
        "**2. Threshold Selection**:\n",
        "- **Domain-dependent**: Medical diagnosis (low threshold), spam filter (high threshold)\n",
        "- **Trade-off**: Precision vs Recall\n",
        "- **Tuning**: Use validation set to find optimal threshold per label\n",
        "- **Multiple thresholds**: Can use different thresholds for each emotion\n",
        "\n",
        "**3. Batch vs Single Inference**:\n",
        "This function processes one text at a time. For many texts:\n",
        "```python\n",
        "# Inefficient (one at a time)\n",
        "for text in texts:\n",
        "    predict_emotions(text, ...)\n",
        "\n",
        "# Efficient (batch)\n",
        "encodings = tokenizer(texts, ...)\n",
        "logits = model(**encodings)\n",
        "```\n",
        "\n",
        "**4. model.eval() Importance**:\n",
        "```python\n",
        "# With dropout active (WRONG for inference)\n",
        "model.train()\n",
        "pred1 = model(input)  # Random due to dropout\n",
        "pred2 = model(input)  # Different result!\n",
        "\n",
        "# With dropout disabled (CORRECT)\n",
        "model.eval()\n",
        "pred1 = model(input)  # Deterministic\n",
        "pred2 = model(input)  # Same result\n",
        "```\n",
        "\n",
        "**5. Multi-label Interpretation**:\n",
        "- **Single label**: \"This text is joy\" (mutually exclusive)\n",
        "- **Multi-label**: \"This text has joy AND surprise\" (not exclusive)\n",
        "- Our model supports multiple emotions per text\n",
        "\n",
        "**6. Probability Calibration**:\n",
        "- Probabilities may not be well-calibrated\n",
        "- 0.8 doesn't necessarily mean \"80% confident\"\n",
        "- For calibrated probabilities, use temperature scaling or Platt scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e53a65",
      "metadata": {
        "id": "42e53a65"
      },
      "source": [
        "## Section 16: Summary and Key Takeaways\n",
        "\n",
        "### Complete Pipeline Overview\n",
        "\n",
        "This notebook demonstrated a complete PyTorch pipeline for multi-label emotion classification using transformer models. Here is the comprehensive workflow:\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Flow\n",
        "\n",
        "```\n",
        "Raw Text Input\n",
        "    ↓\n",
        "Tokenization (BERT Tokenizer)\n",
        "    ↓\n",
        "Token IDs + Attention Mask (Tensors)\n",
        "    ↓\n",
        "Embedding Layer (BERT)\n",
        "    ↓\n",
        "12 Transformer Layers (Self-Attention + FFN)\n",
        "    ↓\n",
        "[CLS] Token Representation (768-dim)\n",
        "    ↓\n",
        "Dropout Layer (Regularization)\n",
        "    ↓\n",
        "Linear Classification Head (768 → 5)\n",
        "    ↓\n",
        "Logits (Raw Scores)\n",
        "    ↓\n",
        "Sigmoid Activation\n",
        "    ↓\n",
        "Probabilities (0-1 per emotion)\n",
        "    ↓\n",
        "Thresholding (> 0.5)\n",
        "    ↓\n",
        "Binary Predictions\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Core Components Covered\n",
        "\n",
        "**1. Data Preparation**\n",
        "- Synthesized 10-row emotion dataset with 5 binary labels\n",
        "- Train-test split (80-20) → 8 training, 2 validation samples\n",
        "- Custom Dataset class inheriting from torch.utils.data.Dataset\n",
        "- DataCollator for batch assembly using @dataclass\n",
        "\n",
        "**2. Tokenization**\n",
        "- BERT tokenizer (bert-base-uncased)\n",
        "- Max length 128 tokens\n",
        "- Padding and truncation\n",
        "- Special tokens: [CLS], [SEP], [PAD]\n",
        "- Attention masks to distinguish real tokens from padding\n",
        "\n",
        "**3. Model Architecture**\n",
        "- Base: Pretrained BERT-base-uncased (~110M parameters)\n",
        "- Custom classification head: Linear(768, 5)\n",
        "- Dropout layer (p=0.3) for regularization\n",
        "- Transfer learning: Fine-tune entire model\n",
        "\n",
        "**4. Training Configuration**\n",
        "- Loss: BCEWithLogitsLoss (multi-label binary cross-entropy)\n",
        "- Optimizer: AdamW (lr=2e-5, weight_decay=0.01)\n",
        "- Epochs: 3 (typical for transformer fine-tuning)\n",
        "- Batch size: 4\n",
        "- Device: CUDA if available, else CPU\n",
        "\n",
        "**5. Training Process**\n",
        "- Trainer class orchestrating training and validation\n",
        "- Forward pass → Loss computation → Backpropagation → Parameter update\n",
        "- Per-epoch validation to monitor generalization\n",
        "- History tracking for loss and accuracy\n",
        "\n",
        "**6. Evaluation Metrics**\n",
        "- Exact match accuracy (strict - all labels must match)\n",
        "- Precision, Recall, F1 (both macro and micro averaging)\n",
        "- Per-label analysis to identify strong/weak emotions\n",
        "\n",
        "**7. Inference**\n",
        "- predict_emotions function for new text\n",
        "- Returns probabilities and binary decisions\n",
        "- Threshold-based classification (default 0.5)\n",
        "\n",
        "---\n",
        "\n",
        "### Critical Concepts Explained\n",
        "\n",
        "**Multi-Label vs Multi-Class**\n",
        "- Multi-class: One label per sample (softmax, argmax)\n",
        "- Multi-label: Multiple labels per sample (sigmoid, threshold)\n",
        "- Our task: Multi-label (text can have anger AND sadness)\n",
        "\n",
        "**Transfer Learning**\n",
        "- Start with pretrained BERT (trained on massive text corpora)\n",
        "- Fine-tune on specific task (emotion classification)\n",
        "- Much faster and better than training from scratch\n",
        "- Requires smaller learning rate (2e-5 vs 1e-3)\n",
        "\n",
        "**Key PyTorch Patterns**\n",
        "- `model.train()` before training (enables dropout)\n",
        "- `model.eval()` before evaluation (disables dropout)\n",
        "- `optimizer.zero_grad()` before each backward pass\n",
        "- `torch.no_grad()` during inference (no gradient computation)\n",
        "- `.to(device)` to move tensors/models to GPU/CPU\n",
        "\n",
        "**Gradient Descent Mechanics**\n",
        "1. Forward pass: Compute predictions\n",
        "2. Loss computation: Quantify error\n",
        "3. Backward pass: Compute gradients via chain rule\n",
        "4. Optimizer step: Update parameters to reduce loss\n",
        "\n",
        "**Macro vs Micro Metrics**\n",
        "- Macro: Average per-label metrics (treats all labels equally)\n",
        "- Micro: Aggregate all predictions (larger classes have more weight)\n",
        "- Use macro for balanced importance, micro for overall performance\n",
        "\n",
        "---\n",
        "\n",
        "### Common Pitfalls and Solutions\n",
        "\n",
        "**1. Forgetting model.eval()**\n",
        "- **Problem**: Dropout active during inference → random predictions\n",
        "- **Solution**: Always call `model.eval()` before inference\n",
        "\n",
        "**2. Not using torch.no_grad() during inference**\n",
        "- **Problem**: Memory leak from computation graphs\n",
        "- **Solution**: Wrap inference in `with torch.no_grad():`\n",
        "\n",
        "**3. Not calling optimizer.zero_grad()**\n",
        "- **Problem**: Gradients accumulate across batches → wrong updates\n",
        "- **Solution**: Call `optimizer.zero_grad()` before each backward pass\n",
        "\n",
        "**4. Mismatch between device placement**\n",
        "- **Problem**: Model on GPU but data on CPU → runtime error\n",
        "- **Solution**: Use `.to(device)` consistently for model and data\n",
        "\n",
        "**5. Using .item() incorrectly**\n",
        "- **Problem**: Keeping loss tensors in lists → memory leak\n",
        "- **Solution**: Use `loss.item()` to extract scalar value\n",
        "\n",
        "**6. Wrong averaging for multi-label**\n",
        "- **Problem**: Using softmax instead of sigmoid\n",
        "- **Solution**: Use BCEWithLogitsLoss and sigmoid for multi-label\n",
        "\n",
        "**7. Not detaching predictions before accumulation**\n",
        "- **Problem**: Keeping computation graphs in memory\n",
        "- **Solution**: Use `.cpu().numpy()` or `.detach()`\n",
        "\n",
        "---\n",
        "\n",
        "### Extending This Pipeline\n",
        "\n",
        "**1. Handling Larger Datasets**\n",
        "- Use DataLoader with num_workers > 0 for parallel loading\n",
        "- Implement data augmentation (synonym replacement, back-translation)\n",
        "- Use gradient accumulation for larger effective batch size\n",
        "\n",
        "**2. Improving Performance**\n",
        "- Try different pretrained models (RoBERTa, DistilBERT, ELECTRA)\n",
        "- Tune hyperparameters (learning rate, dropout, epochs)\n",
        "- Use learning rate scheduling (linear warmup, cosine decay)\n",
        "- Implement early stopping to prevent overfitting\n",
        "- Use class weights for imbalanced labels\n",
        "\n",
        "**3. Production Deployment**\n",
        "- Save model: `torch.save(model.state_dict(), 'model.pt')`\n",
        "- Load model: `model.load_state_dict(torch.load('model.pt'))`\n",
        "- Use ONNX for cross-framework deployment\n",
        "- Quantize model for faster inference (FP16 or INT8)\n",
        "- Batch inference for throughput optimization\n",
        "\n",
        "**4. Advanced Techniques**\n",
        "- Label smoothing for better calibration\n",
        "- Focal loss for hard examples\n",
        "- Multi-task learning (predict sentiment + emotions)\n",
        "- Active learning to select informative samples\n",
        "- Model distillation to create smaller models\n",
        "\n",
        "---\n",
        "\n",
        "### Objectives Achieved\n",
        "\n",
        "**Conceptual Understanding**\n",
        "- How transformers process text\n",
        "- Transfer learning and fine-tuning\n",
        "- Multi-label classification mechanics\n",
        "- Gradient descent and backpropagation\n",
        "- Evaluation metrics interpretation\n",
        "\n",
        "**Practical Skills**\n",
        "- Building custom Dataset classes\n",
        "- Creating data collators\n",
        "- Defining neural network architectures\n",
        "- Implementing training loops\n",
        "- Computing and interpreting metrics\n",
        "- Making predictions on new data\n",
        "\n",
        "**Best Practices**\n",
        "- Proper train/eval mode switching\n",
        "- Memory-efficient inference\n",
        "- Gradient management\n",
        "- Device handling\n",
        "- Code organization with classes\n",
        "\n",
        "**PyTorch Proficiency**\n",
        "- nn.Module inheritance\n",
        "- Autograd system\n",
        "- DataLoader usage\n",
        "- Optimizer configuration\n",
        "- Tensor operations\n",
        "\n",
        "---\n",
        "\n",
        "### Final Notes\n",
        "\n",
        "**Dataset Size**: This notebook uses only 10 samples for demonstration. Real-world applications require thousands to millions of samples for robust performance.\n",
        "\n",
        "**Computation Time**: With small data, training is very fast (seconds). Real projects may take hours or days.\n",
        "\n",
        "**Overfitting Risk**: With 110M parameters and 8 training samples, severe overfitting is expected. This is purely educational; production models need much more data.\n",
        "\n",
        "**Next Steps for Learners**:\n",
        "1. Experiment with different hyperparameters\n",
        "2. Try other pretrained models\n",
        "3. Implement additional metrics (ROC-AUC, PR curves)\n",
        "4. Add visualization (confusion matrix, learning curves)\n",
        "5. Save and load trained models\n",
        "6. Deploy model as REST API or web service\n",
        "\n",
        "**Key Resources**:\n",
        "- PyTorch documentation: https://pytorch.org/docs/\n",
        "- Hugging Face Transformers: https://huggingface.co/docs/transformers/\n",
        "- BERT paper: \"Attention is All You Need\" and \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
        "\n",
        "---\n",
        "\n",
        "### Congratulations!\n",
        "\n",
        "You have completed a comprehensive deep learning pipeline covering:\n",
        "- Data preparation and loading\n",
        "- Tokenization and encoding\n",
        "- Model architecture and transfer learning\n",
        "- Training with backpropagation\n",
        "- Evaluation with multiple metrics\n",
        "- Inference on new examples\n",
        "\n",
        "This foundation enables you to tackle diverse NLP tasks including sentiment analysis, named entity recognition, question answering, and text generation.\n",
        "\n",
        "**Remember**: Deep learning is iterative. Experiment, analyze results, and refine. The best models come from understanding both theory and practice through hands-on experience."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}