{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ZAJYfnXZwmr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df=pd.read_csv('final.csv').iloc[:,3:]\n",
        "\n",
        "clean_test=pd.read_csv('cleaned.csv')\n",
        "final_df.shape,clean_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuazn9zVaF6V",
        "outputId": "6373faf7-e5c5-4cd8-8daa-146e480601d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4991, 10), (1707, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "AXe9lkEBoZv_",
        "outputId": "f0d88e71-29d2-475b-be47-30e415115b66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                               text  anger  fear  joy  \\\n",
              "0   0  the dentist that did the work apparently did a...      1     0    0   \n",
              "1   1  i'm gonna absolutely ~~suck~~ be terrible duri...      0     1    0   \n",
              "2   2  bridge: so leave me drowning calling houston, ...      0     1    0   \n",
              "3   3  after that mess i went to see my now ex-girlfr...      1     1    0   \n",
              "4   4  as he stumbled i ran off, afraid it might some...      0     1    0   \n",
              "\n",
              "   sadness  surprise                    emotions  \\\n",
              "0        1         0         ['anger' 'sadness']   \n",
              "1        1         0          ['fear' 'sadness']   \n",
              "2        1         0          ['fear' 'sadness']   \n",
              "3        1         0  ['anger' 'fear' 'sadness']   \n",
              "4        0         0                    ['fear']   \n",
              "\n",
              "                                          clean_text  \\\n",
              "0  the dentist that did the work apparently did a...   \n",
              "1  i m gonna absolutely suck be terrible during m...   \n",
              "2  bridge: so leave me drowning calling houston, ...   \n",
              "3  after that mess i went to see my now ex girlfr...   \n",
              "4  as he stumbled i ran off, afraid it might some...   \n",
              "\n",
              "                                          final_text  \n",
              "0  dentist work apparently lousy job year teeth d...  \n",
              "1  gon na absolutely suck terrible first sexual e...  \n",
              "2  bridge : leave drowning calling houston , let ...  \n",
              "3  mess went see ex girlfriend school refused dri...  \n",
              "4  stumbled ran , afraid might somehow affect job...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e210184-d09b-4438-8f56-8e0b3141e753\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>anger</th>\n",
              "      <th>fear</th>\n",
              "      <th>joy</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>emotions</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>final_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the dentist that did the work apparently did a...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['anger' 'sadness']</td>\n",
              "      <td>the dentist that did the work apparently did a...</td>\n",
              "      <td>dentist work apparently lousy job year teeth d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i'm gonna absolutely ~~suck~~ be terrible duri...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['fear' 'sadness']</td>\n",
              "      <td>i m gonna absolutely suck be terrible during m...</td>\n",
              "      <td>gon na absolutely suck terrible first sexual e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>bridge: so leave me drowning calling houston, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['fear' 'sadness']</td>\n",
              "      <td>bridge: so leave me drowning calling houston, ...</td>\n",
              "      <td>bridge : leave drowning calling houston , let ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>after that mess i went to see my now ex-girlfr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['anger' 'fear' 'sadness']</td>\n",
              "      <td>after that mess i went to see my now ex girlfr...</td>\n",
              "      <td>mess went see ex girlfriend school refused dri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>as he stumbled i ran off, afraid it might some...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['fear']</td>\n",
              "      <td>as he stumbled i ran off, afraid it might some...</td>\n",
              "      <td>stumbled ran , afraid might somehow affect job...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e210184-d09b-4438-8f56-8e0b3141e753')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e210184-d09b-4438-8f56-8e0b3141e753 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e210184-d09b-4438-8f56-8e0b3141e753');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b5630abd-38b2-4413-95ff-6446865f97e2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5630abd-38b2-4413-95ff-6446865f97e2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b5630abd-38b2-4413-95ff-6446865f97e2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_df",
              "summary": "{\n  \"name\": \"final_df\",\n  \"rows\": 4991,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1924,\n        \"min\": 0,\n        \"max\": 6820,\n        \"num_unique_values\": 4991,\n        \"samples\": [\n          701,\n          6254,\n          4102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4986,\n        \"samples\": [\n          \"i then closed the flap, and pressed it with my finger to make sure it was sealed.\",\n          \"youre not mad are you?\",\n          \"i am going to try to get odd camera angles and never really show my face.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"anger\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fear\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"joy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sadness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"surprise\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotions\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"['anger' 'fear' 'joy' 'sadness' 'surprise']\",\n          \"['sadness' 'surprise']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4984,\n        \"samples\": [\n          \"i then closed the flap, and pressed it with my finger to make sure it was sealed.\",\n          \"i used to play halo semi professionally, i know a ton of ppl on xbox live and have met a good amount of them in person at tournaments but a lot i still haven t.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4951,\n        \"samples\": [\n          \"chest .\",\n          \"thing got progressively worse weekend , point blacked eye rolling back head monday morning sitting couch holding aj .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "AV2eTHd0asiX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "YewbpZXGa3z1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init()"
      ],
      "metadata": {
        "id": "Srl8SVZUd78M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"23f3003030-t32025\",\n",
        "    name=\"Day01_EmotionModel_Baselines\",\n",
        "    config={\n",
        "        \"description\": \"Baseline comparison of multiple classifiers on emotion labels\",\n",
        "        \"tfidf_max_features\": 5000,\n",
        "        \"vectorizer_type\": \"TfidfVectorizer (1,2)\",\n",
        "        \"split_ratio\": 0.8,\n",
        "        \"experiment_type\": \"Multi-label classification\"\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "yQQ7BozceE-v",
        "outputId": "2febcacb-f9fe-40bb-c1a4-476363a65989"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251010_053835-he1nvtvb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/he1nvtvb' target=\"_blank\">Day01_EmotionModel_Baselines</a></strong> to <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/he1nvtvb' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/he1nvtvb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/he1nvtvb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78fd11d91850>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2O0c0zYgjw3",
        "outputId": "6b89e64f-8332-4c83-e91c-73276ab4c6ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "üöÄ Industry-Grade Multi-Label Text Classification Pipeline\n",
        "Features: Advanced Experiment Tracking, Comprehensive Metrics, Production-Ready Visualizations\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import wandb\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    f1_score, classification_report, confusion_matrix,\n",
        "    roc_curve, auc, roc_auc_score, precision_recall_curve,\n",
        "    average_precision_score, hamming_loss, jaccard_score,\n",
        "    multilabel_confusion_matrix, accuracy_score\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# =========================\n",
        "# üé® Configuration & Setup\n",
        "# =========================\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "# Set style for professional visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"plots\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"reports\").mkdir(exist_ok=True)\n",
        "\n",
        "# Experiment configuration\n",
        "EXPERIMENT_CONFIG = {\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42,\n",
        "    \"tfidf_max_features\": 5000,\n",
        "    \"ngram_range\": (1, 2),\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "}\n",
        "\n",
        "# Initialize W&B with rich config\n",
        "wandb.init(\n",
        "    project=\"23f3003030-t32025\",\n",
        "    name=f\"multi-label-classification-{EXPERIMENT_CONFIG['timestamp']}\",\n",
        "    config=EXPERIMENT_CONFIG,\n",
        "    tags=[\"multi-label\", \"emotion-detection\", \"production\"],\n",
        "    notes=\"Industry-grade experiment with comprehensive tracking\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# üìä Advanced Visualization Functions\n",
        "# =========================\n",
        "\n",
        "def plot_combined_confusion_matrix(y_true, y_pred, emotions, model_name):\n",
        "    \"\"\"Create professional confusion matrix visualization\"\"\"\n",
        "    n_emotions = len(emotions)\n",
        "    fig, axes = plt.subplots(1, n_emotions, figsize=(4*n_emotions, 3.5))\n",
        "\n",
        "    if n_emotions == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        cm = confusion_matrix(y_true.iloc[:, i], y_pred[:, i])\n",
        "\n",
        "        # Calculate percentages\n",
        "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "        # Create annotations with counts and percentages\n",
        "        annot = np.array([[f'{count}\\n({percent:.1f}%)'\n",
        "                          for count, percent in zip(row_counts, row_percents)]\n",
        "                         for row_counts, row_percents in zip(cm, cm_percent)])\n",
        "\n",
        "        sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', ax=axes[i],\n",
        "                    cbar=False, square=True, linewidths=1, linecolor='gray')\n",
        "        axes[i].set_title(f'{emotion.upper()}', fontsize=12, fontweight='bold', pad=10)\n",
        "        axes[i].set_xlabel('Predicted', fontsize=10)\n",
        "        axes[i].set_ylabel('Actual' if i == 0 else '', fontsize=10)\n",
        "        axes[i].set_xticklabels(['No', 'Yes'])\n",
        "        axes[i].set_yticklabels(['No', 'Yes'])\n",
        "\n",
        "    fig.suptitle(f'{model_name} - Confusion Matrices (All Emotions)',\n",
        "                 fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{model_name}_confusion_matrix.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{model_name}/confusion_matrix\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_roc_curves(y_true, y_pred_proba, emotions, model_name):\n",
        "    \"\"\"Plot ROC curves with AUC scores\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 9))\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(emotions)))\n",
        "\n",
        "    roc_auc_scores = {}\n",
        "\n",
        "    for i, (emotion, color) in enumerate(zip(emotions, colors)):\n",
        "        fpr, tpr, _ = roc_curve(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_auc_scores[emotion] = roc_auc\n",
        "\n",
        "        ax.plot(fpr, tpr, color=color, lw=3,\n",
        "                label=f'{emotion.capitalize()} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "    # Add diagonal line\n",
        "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.5)', alpha=0.6)\n",
        "\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'{model_name} - ROC Curves', fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.legend(loc=\"lower right\", fontsize=11, framealpha=0.9)\n",
        "    ax.grid(alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add mean AUC text box\n",
        "    mean_auc = np.mean(list(roc_auc_scores.values()))\n",
        "    textstr = f'Mean AUC: {mean_auc:.4f}'\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
        "    ax.text(0.65, 0.15, textstr, transform=ax.transAxes, fontsize=13,\n",
        "            verticalalignment='top', bbox=props, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{model_name}_roc_curves.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{model_name}/roc_curves\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "    return roc_auc_scores\n",
        "\n",
        "\n",
        "def plot_precision_recall_curves(y_true, y_pred_proba, emotions, model_name):\n",
        "    \"\"\"Plot Precision-Recall curves\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 9))\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(emotions)))\n",
        "\n",
        "    avg_precision_scores = {}\n",
        "\n",
        "    for i, (emotion, color) in enumerate(zip(emotions, colors)):\n",
        "        precision, recall, _ = precision_recall_curve(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        avg_precision = average_precision_score(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        avg_precision_scores[emotion] = avg_precision\n",
        "\n",
        "        ax.plot(recall, precision, color=color, lw=3,\n",
        "                label=f'{emotion.capitalize()} (AP = {avg_precision:.4f})')\n",
        "\n",
        "    ax.set_xlabel('Recall', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Precision', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'{model_name} - Precision-Recall Curves', fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.legend(loc=\"lower left\", fontsize=11, framealpha=0.9)\n",
        "    ax.grid(alpha=0.4, linestyle='--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "\n",
        "    # Add mean AP text box\n",
        "    mean_ap = np.mean(list(avg_precision_scores.values()))\n",
        "    textstr = f'Mean AP: {mean_ap:.4f}'\n",
        "    props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
        "    ax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=13,\n",
        "            verticalalignment='top', bbox=props, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{model_name}_precision_recall.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{model_name}/precision_recall\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "    return avg_precision_scores\n",
        "\n",
        "\n",
        "def plot_classification_report(class_report_dict, emotions, model_name):\n",
        "    \"\"\"Visualize classification report as heatmap\"\"\"\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "    data = []\n",
        "\n",
        "    for emotion in emotions:\n",
        "        if emotion in class_report_dict:\n",
        "            row = [class_report_dict[emotion][m] for m in metrics]\n",
        "            data.append(row)\n",
        "\n",
        "    df_report = pd.DataFrame(data, index=[e.capitalize() for e in emotions], columns=metrics)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.heatmap(df_report, annot=True, fmt='.4f', cmap='RdYlGn',\n",
        "                cbar_kws={'label': 'Score'}, vmin=0, vmax=1, ax=ax,\n",
        "                linewidths=2, linecolor='white', annot_kws={\"size\": 12, \"weight\": \"bold\"})\n",
        "    ax.set_title(f'{model_name} - Classification Metrics Heatmap',\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Metrics', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Emotions', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{model_name}_classification_report.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{model_name}/classification_report\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_per_emotion_metrics(y_true, y_pred, emotions, model_name):\n",
        "    \"\"\"Bar plot of per-emotion performance metrics\"\"\"\n",
        "    metrics_data = {'Emotion': [], 'Precision': [], 'Recall': [], 'F1-Score': []}\n",
        "\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        from sklearn.metrics import precision_recall_fscore_support\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_true.iloc[:, i], y_pred[:, i], average='binary', zero_division=0\n",
        "        )\n",
        "        metrics_data['Emotion'].append(emotion.capitalize())\n",
        "        metrics_data['Precision'].append(precision)\n",
        "        metrics_data['Recall'].append(recall)\n",
        "        metrics_data['F1-Score'].append(f1)\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_data)\n",
        "    df_melted = df_metrics.melt(id_vars='Emotion', var_name='Metric', value_name='Score')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    sns.barplot(data=df_melted, x='Emotion', y='Score', hue='Metric', ax=ax, palette='muted')\n",
        "\n",
        "    ax.set_title(f'{model_name} - Per-Emotion Performance',\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Emotion', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    ax.legend(title='Metric', fontsize=12, title_fontsize=13)\n",
        "    ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', fontsize=9, padding=3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = OUTPUT_DIR / \"plots\" / f'{model_name}_per_emotion_metrics.png'\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"{model_name}/per_emotion_metrics\": wandb.Image(str(filename))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_label_distribution(y_data, title, filename):\n",
        "    \"\"\"Plot label distribution\"\"\"\n",
        "    label_counts = y_data.sum().sort_values(ascending=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.bar(range(len(label_counts)), label_counts.values, color=plt.cm.viridis(np.linspace(0, 1, len(label_counts))))\n",
        "    ax.set_xticks(range(len(label_counts)))\n",
        "    ax.set_xticklabels([label.capitalize() for label in label_counts.index], fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = OUTPUT_DIR / \"plots\" / filename\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    wandb.log({f\"data_analysis/{filename.replace('.png', '')}\": wandb.Image(str(save_path))})\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compute_advanced_metrics(y_true, y_pred, y_pred_proba, emotions):\n",
        "    \"\"\"Compute comprehensive metrics\"\"\"\n",
        "    metrics = {\n",
        "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
        "        'jaccard_score': jaccard_score(y_true, y_pred, average='samples', zero_division=0),\n",
        "        'subset_accuracy': accuracy_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "    # Per-emotion metrics\n",
        "    for i, emotion in enumerate(emotions):\n",
        "        metrics[f'{emotion}_f1'] = f1_score(y_true.iloc[:, i], y_pred[:, i], zero_division=0)\n",
        "        metrics[f'{emotion}_auc'] = roc_auc_score(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "        metrics[f'{emotion}_ap'] = average_precision_score(y_true.iloc[:, i], y_pred_proba[:, i])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£ Data Preparation & EDA\n",
        "# =========================\n",
        "print(\"=\" * 80)\n",
        "print(\"üöÄ STARTING EXPERIMENT PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "X = final_df['final_text'].fillna('')\n",
        "y = final_df[['anger', 'fear', 'joy', 'sadness', 'surprise']]\n",
        "emotions = y.columns.tolist()\n",
        "\n",
        "# Log dataset info\n",
        "wandb.log({\n",
        "    \"dataset/total_samples\": len(X),\n",
        "    \"dataset/num_emotions\": len(emotions),\n",
        "    \"dataset/feature_name\": \"final_text\"\n",
        "})\n",
        "\n",
        "# Analyze and log label distribution\n",
        "print(\"\\nüìä Analyzing Label Distribution...\")\n",
        "plot_label_distribution(y, \"Training Data - Emotion Distribution\", \"train_label_distribution.png\")\n",
        "\n",
        "# Log label statistics\n",
        "label_stats = {}\n",
        "for emotion in emotions:\n",
        "    label_stats[f\"dataset/{emotion}_count\"] = int(y[emotion].sum())\n",
        "    label_stats[f\"dataset/{emotion}_percentage\"] = float(y[emotion].sum() / len(y) * 100)\n",
        "\n",
        "wandb.log(label_stats)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=EXPERIMENT_CONFIG['test_size'],\n",
        "    random_state=EXPERIMENT_CONFIG['random_state']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train set size: {len(X_train)} | Validation set size: {len(X_val)}\")\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ Feature Engineering\n",
        "# =========================\n",
        "print(\"\\nüîß Extracting TF-IDF Features...\")\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=EXPERIMENT_CONFIG['tfidf_max_features'],\n",
        "    ngram_range=EXPERIMENT_CONFIG['ngram_range']\n",
        ")\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.transform(X_val)\n",
        "\n",
        "print(f\"‚úÖ TF-IDF shape: {X_train_tfidf.shape}\")\n",
        "wandb.log({\"features/tfidf_shape\": X_train_tfidf.shape[1]})\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ Model Configuration\n",
        "# =========================\n",
        "classifiers = {\n",
        "    'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=300, random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
        "    'LinearSVC': LinearSVC(max_iter=5000, random_state=42),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'KNeighbors': KNeighborsClassifier(n_neighbors=8, n_jobs=-1),\n",
        "    'XGBoost': XGBClassifier(\n",
        "        eval_metric='logloss', n_estimators=500, max_depth=6,\n",
        "        learning_rate=0.05, random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    'LightGBM': LGBMClassifier(\n",
        "        n_estimators=500, max_depth=8, learning_rate=0.05,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    ),\n",
        "    'CatBoost': CatBoostClassifier(\n",
        "        iterations=500, depth=7, learning_rate=0.05,\n",
        "        verbose=0, random_state=42, thread_count=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ Model Training & Evaluation\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ü§ñ TRAINING AND EVALUATING MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "all_model_metrics = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîÑ Training: {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Train model\n",
        "    model = OneVsRestClassifier(clf, n_jobs=-1)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_val_tfidf)\n",
        "\n",
        "    # Get probabilities\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_proba = model.predict_proba(X_val_tfidf)\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_pred_proba = model.decision_function(X_val_tfidf)\n",
        "        # Normalize to [0, 1]\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        y_pred_proba = scaler.fit_transform(y_pred_proba)\n",
        "    else:\n",
        "        y_pred_proba = y_pred\n",
        "\n",
        "    # ---- Compute Comprehensive Metrics ----\n",
        "    metrics = compute_advanced_metrics(y_val, y_pred, y_pred_proba, emotions)\n",
        "    all_model_metrics[name] = metrics\n",
        "\n",
        "    # Log all metrics to W&B\n",
        "    for metric_name, value in metrics.items():\n",
        "        wandb.log({f\"{name}/metrics/{metric_name}\": value})\n",
        "\n",
        "    # ---- Generate Classification Report ----\n",
        "    class_report = classification_report(\n",
        "        y_val, y_pred, target_names=emotions,\n",
        "        output_dict=True, zero_division=0\n",
        "    )\n",
        "\n",
        "    # ---- Create All Visualizations ----\n",
        "    print(f\"   üìä Generating visualizations...\")\n",
        "\n",
        "    # 1. Combined Confusion Matrix\n",
        "    plot_combined_confusion_matrix(y_val, y_pred, emotions, name)\n",
        "\n",
        "    # 2. ROC Curves\n",
        "    try:\n",
        "        roc_scores = plot_roc_curves(y_val, y_pred_proba, emotions, name)\n",
        "        wandb.log({f\"{name}/metrics/mean_roc_auc\": np.mean(list(roc_scores.values()))})\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Could not plot ROC curves: {e}\")\n",
        "\n",
        "    # 3. Precision-Recall Curves\n",
        "    try:\n",
        "        pr_scores = plot_precision_recall_curves(y_val, y_pred_proba, emotions, name)\n",
        "        wandb.log({f\"{name}/metrics/mean_average_precision\": np.mean(list(pr_scores.values()))})\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Could not plot PR curves: {e}\")\n",
        "\n",
        "    # 4. Classification Report Heatmap\n",
        "    plot_classification_report(class_report, emotions, name)\n",
        "\n",
        "    # 5. Per-Emotion Metrics\n",
        "    plot_per_emotion_metrics(y_val, y_pred, emotions, name)\n",
        "\n",
        "    # ---- Save detailed results ----\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'F1_Macro': metrics['f1_macro'],\n",
        "        'F1_Micro': metrics['f1_micro'],\n",
        "        'F1_Weighted': metrics['f1_weighted'],\n",
        "        'Hamming_Loss': metrics['hamming_loss'],\n",
        "        'Jaccard_Score': metrics['jaccard_score'],\n",
        "        'Subset_Accuracy': metrics['subset_accuracy']\n",
        "    })\n",
        "\n",
        "    print(f\"   ‚úÖ {name} completed!\")\n",
        "    print(f\"      F1-Macro: {metrics['f1_macro']:.4f} | Hamming Loss: {metrics['hamming_loss']:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 5Ô∏è‚É£ Model Comparison & Best Model Selection\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà MODEL COMPARISON RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(by='F1_Macro', ascending=False)\n",
        "print(\"\\n\", results_df.to_string(index=False))\n",
        "\n",
        "# Save results to CSV\n",
        "results_path = OUTPUT_DIR / \"reports\" / \"model_comparison.csv\"\n",
        "results_df.to_csv(results_path, index=False)\n",
        "wandb.save(str(results_path))\n",
        "\n",
        "# Create comparison visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics_to_plot = ['F1_Macro', 'F1_Micro', 'F1_Weighted', 'Hamming_Loss', 'Jaccard_Score', 'Subset_Accuracy']\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx]\n",
        "    data = results_df.sort_values(by=metric, ascending=(metric == 'Hamming_Loss'))\n",
        "\n",
        "    bars = ax.barh(data['Model'], data[metric], color=plt.cm.viridis(np.linspace(0, 1, len(data))))\n",
        "    ax.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(metric.replace('_', ' '), fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='x', alpha=0.4, linestyle='--')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "                f'{width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Model Performance Comparison - All Metrics', fontsize=20, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "\n",
        "comparison_path = OUTPUT_DIR / \"plots\" / \"model_comparison_all_metrics.png\"\n",
        "plt.savefig(comparison_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "wandb.log({\"comparison/all_metrics\": wandb.Image(str(comparison_path))})\n",
        "plt.close()\n",
        "\n",
        "# Log results table to W&B\n",
        "wandb.log({\"comparison/results_table\": wandb.Table(dataframe=results_df)})\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Best Model Retraining\n",
        "# =========================\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"   F1-Macro Score: {results_df.iloc[0]['F1_Macro']:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"best_model/name\": best_model_name,\n",
        "    \"best_model/f1_macro\": results_df.iloc[0]['F1_Macro']\n",
        "})\n",
        "\n",
        "# Retrain on full data\n",
        "print(f\"\\nüîÑ Retraining {best_model_name} on full dataset...\")\n",
        "best_clf = classifiers[best_model_name]\n",
        "best_model = OneVsRestClassifier(best_clf, n_jobs=-1)\n",
        "\n",
        "X_full_tfidf = tfidf.fit_transform(X)\n",
        "best_model.fit(X_full_tfidf, y)\n",
        "\n",
        "print(\"   ‚úÖ Best model retrained on full dataset\")\n",
        "\n",
        "# =========================\n",
        "# 7Ô∏è‚É£ Test Set Prediction\n",
        "# =========================\n",
        "print(\"\\nüìù Generating predictions on test set...\")\n",
        "clean_test['final_text'] = clean_test['final_text'].fillna('')\n",
        "X_test_tfidf = tfidf.transform(clean_test['final_text'])\n",
        "y_test_pred = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame(y_test_pred, columns=y.columns)\n",
        "submission['id'] = clean_test['id']\n",
        "submission = submission[['id'] + list(y.columns)]\n",
        "\n",
        "submission_path = OUTPUT_DIR / \"submission.csv\"\n",
        "submission.to_csv(submission_path, index=False)\n",
        "wandb.save(str(submission_path))\n",
        "\n",
        "print(f\"   ‚úÖ Submission saved: {submission_path}\")\n",
        "\n",
        "# =========================\n",
        "# 8Ô∏è‚É£ Final Summary Report\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary = {\n",
        "    \"timestamp\": EXPERIMENT_CONFIG['timestamp'],\n",
        "    \"best_model\": best_model_name,\n",
        "    \"best_f1_macro\": float(results_df.iloc[0]['F1_Macro']),\n",
        "    \"total_models_trained\": len(classifiers),\n",
        "    \"dataset_size\": len(X),\n",
        "    \"validation_size\": len(X_val),\n",
        "    \"num_emotions\": len(emotions),\n",
        "    \"emotions\": emotions\n",
        "}\n",
        "\n",
        "summary_path = OUTPUT_DIR / \"reports\" / \"experiment_summary.json\"\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=4)\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(f\"\\n‚úÖ Summary saved: {summary_path}\")\n",
        "\n",
        "wandb.log({\"experiment/summary\": summary})\n",
        "\n",
        "# =========================\n",
        "# üéØ Finish Experiment\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìÅ All outputs saved in: {OUTPUT_DIR}\")\n",
        "print(f\"üîó View results in W&B: {wandb.run.get_url()}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0AY5v5oxfjEz",
        "outputId": "b59fdb2d-991e-4ea7-f12c-6b8568ea8f6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LogisticRegression/metrics/anger_ap</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/anger_auc</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/anger_f1</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/f1_macro</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/f1_micro</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/f1_weighted</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/fear_ap</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/fear_auc</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/fear_f1</td><td>‚ñÅ</td></tr><tr><td>LogisticRegression/metrics/hamming_loss</td><td>‚ñÅ</td></tr><tr><td>+26</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LogisticRegression/metrics/anger_ap</td><td>0.30956</td></tr><tr><td>LogisticRegression/metrics/anger_auc</td><td>0.74547</td></tr><tr><td>LogisticRegression/metrics/anger_f1</td><td>0.01818</td></tr><tr><td>LogisticRegression/metrics/f1_macro</td><td>0.31603</td></tr><tr><td>LogisticRegression/metrics/f1_micro</td><td>0.50582</td></tr><tr><td>LogisticRegression/metrics/f1_weighted</td><td>0.43506</td></tr><tr><td>LogisticRegression/metrics/fear_ap</td><td>0.77294</td></tr><tr><td>LogisticRegression/metrics/fear_auc</td><td>0.71811</td></tr><tr><td>LogisticRegression/metrics/fear_f1</td><td>0.72084</td></tr><tr><td>LogisticRegression/metrics/hamming_loss</td><td>0.23784</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">multi-label-classification-20251010_055257</strong> at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/aiwithajay-indian-institute-of-technology-madras/runs/sj3hvpko' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/aiwithajay-indian-institute-of-technology-madras/runs/sj3hvpko</a><br> View project at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/aiwithajay-indian-institute-of-technology-madras' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/aiwithajay-indian-institute-of-technology-madras</a><br>Synced 3 W&B file(s), 6 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251010_055257-sj3hvpko/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251010_055437-d3bzjxc1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/d3bzjxc1' target=\"_blank\">multi-label-classification-20251010_055437</a></strong> to <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/d3bzjxc1' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/d3bzjxc1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üöÄ STARTING EXPERIMENT PIPELINE\n",
            "================================================================================\n",
            "\n",
            "üìä Analyzing Label Distribution...\n",
            "‚úÖ Train set size: 3992 | Validation set size: 999\n",
            "\n",
            "üîß Extracting TF-IDF Features...\n",
            "‚úÖ TF-IDF shape: (3992, 5000)\n",
            "\n",
            "================================================================================\n",
            "ü§ñ TRAINING AND EVALUATING MODELS\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: LogisticRegression\n",
            "============================================================\n",
            "   üìä Generating visualizations...\n",
            "   ‚úÖ LogisticRegression completed!\n",
            "      F1-Macro: 0.3160 | Hamming Loss: 0.2378\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: RandomForest\n",
            "============================================================\n",
            "   üìä Generating visualizations...\n",
            "   ‚úÖ RandomForest completed!\n",
            "      F1-Macro: 0.4584 | Hamming Loss: 0.2350\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: LinearSVC\n",
            "============================================================\n",
            "   üìä Generating visualizations...\n",
            "   ‚úÖ LinearSVC completed!\n",
            "      F1-Macro: 0.4584 | Hamming Loss: 0.2517\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: MultinomialNB\n",
            "============================================================\n",
            "   üìä Generating visualizations...\n",
            "   ‚úÖ MultinomialNB completed!\n",
            "      F1-Macro: 0.2706 | Hamming Loss: 0.2386\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: KNeighbors\n",
            "============================================================\n",
            "   üìä Generating visualizations...\n",
            "   ‚úÖ KNeighbors completed!\n",
            "      F1-Macro: 0.2212 | Hamming Loss: 0.3029\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: XGBoost\n",
            "============================================================\n",
            "   üìä Generating visualizations...\n",
            "   ‚úÖ XGBoost completed!\n",
            "      F1-Macro: 0.3907 | Hamming Loss: 0.2412\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: LightGBM\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üìä Generating visualizations...\n",
            "   ‚úÖ LightGBM completed!\n",
            "      F1-Macro: 0.3613 | Hamming Loss: 0.2589\n",
            "\n",
            "============================================================\n",
            "üîÑ Training: CatBoost\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üìä Generating visualizations...\n",
            "   ‚úÖ CatBoost completed!\n",
            "      F1-Macro: 0.4143 | Hamming Loss: 0.2352\n",
            "\n",
            "================================================================================\n",
            "üìà MODEL COMPARISON RESULTS\n",
            "================================================================================\n",
            "\n",
            "              Model  F1_Macro  F1_Micro  F1_Weighted  Hamming_Loss  Jaccard_Score  Subset_Accuracy\n",
            "      RandomForest  0.458411  0.560629     0.533863      0.235035       0.402519         0.282282\n",
            "         LinearSVC  0.458362  0.547027     0.533251      0.251652       0.391208         0.268268\n",
            "          CatBoost  0.414270  0.536123     0.498277      0.235235       0.374741         0.277277\n",
            "           XGBoost  0.390683  0.534210     0.486221      0.241241       0.378879         0.256256\n",
            "          LightGBM  0.361329  0.482593     0.450200      0.258859       0.324675         0.223223\n",
            "LogisticRegression  0.316025  0.505824     0.435056      0.237838       0.347931         0.256256\n",
            "     MultinomialNB  0.270570  0.500838     0.398865      0.238639       0.353604         0.242242\n",
            "        KNeighbors  0.221153  0.440252     0.338767      0.302903       0.292626         0.151151\n",
            "\n",
            "üèÜ BEST MODEL: RandomForest\n",
            "   F1-Macro Score: 0.4584\n",
            "\n",
            "üîÑ Retraining RandomForest on full dataset...\n",
            "   ‚úÖ Best model retrained on full dataset\n",
            "\n",
            "üìù Generating predictions on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ Submission saved: outputs/submission.csv\n",
            "\n",
            "================================================================================\n",
            "üìã EXPERIMENT SUMMARY\n",
            "================================================================================\n",
            "{\n",
            "  \"timestamp\": \"20251010_055437\",\n",
            "  \"best_model\": \"RandomForest\",\n",
            "  \"best_f1_macro\": 0.4584105219752548,\n",
            "  \"total_models_trained\": 8,\n",
            "  \"dataset_size\": 4991,\n",
            "  \"validation_size\": 999,\n",
            "  \"num_emotions\": 5,\n",
            "  \"emotions\": [\n",
            "    \"anger\",\n",
            "    \"fear\",\n",
            "    \"joy\",\n",
            "    \"sadness\",\n",
            "    \"surprise\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "‚úÖ Summary saved: outputs/reports/experiment_summary.json\n",
            "\n",
            "================================================================================\n",
            "‚úÖ EXPERIMENT COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "üìÅ All outputs saved in: outputs\n",
            "üîó View results in W&B: https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/d3bzjxc1\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CatBoost/metrics/anger_ap</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/anger_auc</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/anger_f1</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/f1_macro</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/f1_micro</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/f1_weighted</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/fear_ap</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/fear_auc</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/fear_f1</td><td>‚ñÅ</td></tr><tr><td>CatBoost/metrics/hamming_loss</td><td>‚ñÅ</td></tr><tr><td>+188</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CatBoost/metrics/anger_ap</td><td>0.3405</td></tr><tr><td>CatBoost/metrics/anger_auc</td><td>0.73805</td></tr><tr><td>CatBoost/metrics/anger_f1</td><td>0.208</td></tr><tr><td>CatBoost/metrics/f1_macro</td><td>0.41427</td></tr><tr><td>CatBoost/metrics/f1_micro</td><td>0.53612</td></tr><tr><td>CatBoost/metrics/f1_weighted</td><td>0.49828</td></tr><tr><td>CatBoost/metrics/fear_ap</td><td>0.74552</td></tr><tr><td>CatBoost/metrics/fear_auc</td><td>0.68644</td></tr><tr><td>CatBoost/metrics/fear_f1</td><td>0.70878</td></tr><tr><td>CatBoost/metrics/hamming_loss</td><td>0.23524</td></tr><tr><td>+190</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">multi-label-classification-20251010_055437</strong> at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/d3bzjxc1' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025/runs/d3bzjxc1</a><br> View project at: <a href='https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025' target=\"_blank\">https://wandb.ai/aiwithajay-indian-institute-of-technology-madras/23f3003030-t32025</a><br>Synced 5 W&B file(s), 43 media file(s), 2 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251010_055437-d3bzjxc1/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLdlq6C9hsq3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs a structured **inference report, results summary, and conclusion** based on your multi-label emotion classification experiment with 8 models:\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Inference Report**\n",
        "\n",
        "* **Task:** Multi-label emotion classification on 5 emotions (`anger`, `fear`, `joy`, `sadness`, `surprise`)\n",
        "\n",
        "* **Dataset:**\n",
        "\n",
        "  * Train: 3992 samples\n",
        "  * Validation: 999 samples\n",
        "  * Features: TF-IDF vectorization of text (`max_features=5000`, n-grams 1‚Äì2)\n",
        "\n",
        "* **Models Trained:**\n",
        "  Logistic Regression, Random Forest, LinearSVC, MultinomialNB, KNeighbors, XGBoost, LightGBM, CatBoost\n",
        "\n",
        "* **Metrics Used:**\n",
        "\n",
        "  * `F1-macro`, `F1-micro`, `F1-weighted`\n",
        "  * Hamming Loss\n",
        "  * Jaccard Score\n",
        "  * Subset Accuracy\n",
        "  * Per-class `AP`, `AUC`, `F1`\n",
        "\n",
        "* **Hamming Loss Interpretation:**\n",
        "  Fraction of labels incorrectly predicted. Lower is better.\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Results Summary**\n",
        "\n",
        "| Model              | F1-Macro | F1-Micro | Hamming Loss | Subset Accuracy |\n",
        "| ------------------ | -------- | -------- | ------------ | --------------- |\n",
        "| RandomForest       | 0.4584   | 0.5606   | 0.2350       | 0.2823          |\n",
        "| LinearSVC          | 0.4584   | 0.5470   | 0.2517       | 0.2683          |\n",
        "| CatBoost           | 0.4143   | 0.5361   | 0.2352       | 0.2773          |\n",
        "| XGBoost            | 0.3907   | 0.5342   | 0.2412       | 0.2563          |\n",
        "| LightGBM           | 0.3613   | 0.4826   | 0.2589       | 0.2232          |\n",
        "| LogisticRegression | 0.3160   | 0.5058   | 0.2378       | 0.2563          |\n",
        "| MultinomialNB      | 0.2706   | 0.5008   | 0.2386       | 0.2422          |\n",
        "| KNeighbors         | 0.2212   | 0.4403   | 0.3029       | 0.1512          |\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "1. **Best Overall Model:** `RandomForest`\n",
        "\n",
        "   * Highest `F1-macro` (0.4584) and `F1-micro` (0.5606)\n",
        "   * Hamming loss is among the lowest (0.2350) ‚Üí good balance between false positives and false negatives\n",
        "   * Subset accuracy is low (~28%), typical for multi-label tasks\n",
        "\n",
        "2. **LinearSVC:** Almost identical macro F1 to RandomForest, slightly higher Hamming loss ‚Üí slightly less reliable in individual label predictions\n",
        "\n",
        "3. **CatBoost:** Strong F1-micro and competitive Hamming loss; slightly lower F1-macro ‚Üí good at predicting frequent labels but less on rare ones\n",
        "\n",
        "4. **Traditional Baselines (LogisticRegression, MultinomialNB, KNeighbors):**\n",
        "\n",
        "   * Underperforming on F1-macro, especially KNeighbors (0.2212)\n",
        "   * Hamming loss higher ‚Üí many label prediction errors\n",
        "\n",
        "5. **Boosting Models (XGBoost, LightGBM, CatBoost):**\n",
        "\n",
        "   * Perform better than simpler baselines\n",
        "   * CatBoost is more stable across macro/micro F1\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Inference Highlights**\n",
        "\n",
        "* **Rare label issue:**\n",
        "\n",
        "  * `anger` has very low F1 (e.g., 0.018 for LogisticRegression, 0.208 for CatBoost)\n",
        "  * Imbalanced label distribution is causing low per-class performance\n",
        "* **Hamming Loss:**\n",
        "\n",
        "  * Average ~0.24‚Äì0.25 for top models ‚Üí ~24‚Äì25% of individual label predictions are wrong\n",
        "* **F1-macro vs F1-micro:**\n",
        "\n",
        "  * Macro gives equal weight to all labels ‚Üí lower for rare labels\n",
        "  * Micro favors frequent labels ‚Üí top models achieve ~0.56\n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Conclusions**\n",
        "\n",
        "1. **Best Performing Model:**\n",
        "\n",
        "   * `RandomForest` is recommended for production use\n",
        "   * Retrained on full dataset and predictions generated on the test set\n",
        "\n",
        "2. **Multi-Label Challenge:**\n",
        "\n",
        "   * Rare labels (`anger`, `surprise`) remain difficult to predict accurately\n",
        "   * Consider strategies like **label-specific class weighting**, **oversampling**, or **focal loss**\n",
        "\n",
        "3. **Next Steps / Improvements:**\n",
        "\n",
        "   * Implement **ensemble of top models** (RandomForest + CatBoost + LinearSVC)\n",
        "   * Experiment with **deep learning models** (BERT-based) for better text feature extraction\n",
        "   * Visualize **label correlations** to exploit dependencies between emotions\n",
        "\n",
        "4. **Submission:**\n",
        "\n",
        "   * Generated predictions saved in `outputs/submission.csv`\n",
        "   * Ready for evaluation in competition or downstream use\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Overall:** The experiment shows that traditional tree-based models (RandomForest, CatBoost) are more effective than simple baselines in multi-label emotion classification. Rare labels remain challenging, indicating room for improvement with advanced techniques.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can **also generate a single visual summary plot** combining:\n",
        "\n",
        "* Confusion matrix of all 5 labels in **one 5x5 matrix**\n",
        "* Per-class F1, precision, recall\n",
        "* ROC-AUC curves\n",
        "\n",
        "This will make the report visually much more interpretable.\n",
        "\n",
        "Do you want me to do that next?\n"
      ],
      "metadata": {
        "id": "Up8NDpwawZsy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwpzdrt9waFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Let‚Äôs break **F1 score** in a very simple, intuitive way and explain each term.\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ What is F1 Score?**\n",
        "\n",
        "F1 Score is the **harmonic mean of Precision and Recall**.\n",
        "It balances **how many predictions were correct** with **how many true positives were found**.\n",
        "\n",
        "[\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "]\n",
        "\n",
        "It ranges from **0 to 1**:\n",
        "\n",
        "* **1** ‚Üí perfect predictions\n",
        "* **0** ‚Üí worst predictions\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Components of F1 Score**\n",
        "\n",
        "### **a) True Positives (TP)**\n",
        "\n",
        "* **What it means:** Correctly predicted positive examples.\n",
        "* **Example:** You predicted `fear` and it was actually `fear`. ‚úÖ\n",
        "\n",
        "### **b) False Positives (FP)**\n",
        "\n",
        "* **What it means:** Predicted positive but it was actually negative.\n",
        "* **Example:** You predicted `anger` but the true label wasn‚Äôt `anger`. ‚ùå\n",
        "\n",
        "### **c) False Negatives (FN)**\n",
        "\n",
        "* **What it means:** Didn‚Äôt predict positive but it was actually positive.\n",
        "* **Example:** You didn‚Äôt predict `joy` but the text actually had `joy`. ‚ùå\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Precision**\n",
        "\n",
        "[\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "]\n",
        "\n",
        "* **Meaning:** Of all the labels you predicted as positive, how many were actually correct?\n",
        "* **Focus:** **Avoiding false alarms**.\n",
        "* **Example:** You predicted `fear` 100 times, but only 70 were correct ‚Üí Precision = 0.7\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Recall (Sensitivity)**\n",
        "\n",
        "[\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "]\n",
        "\n",
        "* **Meaning:** Of all actual positive labels, how many did you predict correctly?\n",
        "* **Focus:** **Finding all true positives**.\n",
        "* **Example:** There were 80 true `fear` labels, you predicted 70 ‚Üí Recall = 0.875\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ F1 Score**\n",
        "\n",
        "* **Meaning:** Combines Precision and Recall into one number.\n",
        "* **Harmonic mean** is used instead of average ‚Üí punishes extreme imbalance between Precision and Recall.\n",
        "* **Example:**\n",
        "\n",
        "  * Precision = 0.7, Recall = 0.875\n",
        "  * F1 = 2 * (0.7*0.875)/(0.7+0.875) ‚âà **0.778**\n",
        "\n",
        "‚úÖ Interpretation: Good balance between avoiding false positives and capturing true positives.\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ Why F1 Score is Useful**\n",
        "\n",
        "* If you **care about both precision and recall**, not just one ‚Üí use F1.\n",
        "* Especially useful in **imbalanced datasets** (like `anger` vs `fear` in your example).\n",
        "\n",
        "---\n",
        "\n",
        "### **Quick Intuition Example Using Your Emotion Data**\n",
        "\n",
        "Suppose we predict `anger` (rare label) in 5 texts:\n",
        "\n",
        "| True Labels | Predicted Labels |\n",
        "| ----------- | ---------------- |\n",
        "| anger       | anger            |\n",
        "| fear        | anger            |\n",
        "| anger       | -                |\n",
        "| sadness     | -                |\n",
        "| anger       | anger            |\n",
        "\n",
        "* **TP** = 2 (`anger` correctly predicted)\n",
        "* **FP** = 1 (`anger` predicted but actually `fear`)\n",
        "* **FN** = 1 (`anger` was true but not predicted)\n",
        "\n",
        "[\n",
        "\\text{Precision} = 2 / (2+1) = 0.667\n",
        "]\n",
        "[\n",
        "\\text{Recall} = 2 / (2+1) = 0.667\n",
        "]\n",
        "[\n",
        "\\text{F1 Score} = 2 * (0.667*0.667)/(0.667+0.667) = 0.667\n",
        "]\n",
        "\n",
        "* **Interpretation:** Your model is moderately good at detecting `anger`, balancing correct predictions and missing some.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can **draw a simple visual diagram showing TP, FP, FN, Precision, Recall, and F1** for your 5 emotions dataset.\n",
        "\n",
        "Do you want me to make that visual?\n"
      ],
      "metadata": {
        "id": "e5eNVRqeyfNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "laa6RXZZyfv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Let‚Äôs break down each metric in a **simple, intuitive way** using your multi-label emotion classification example. I‚Äôll explain **what it means, why it‚Äôs important, and how it helps you understand model performance**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ F1-Macro**\n",
        "\n",
        "**Definition (Easy):**\n",
        "\n",
        "* F1-Macro calculates the F1 score **separately for each label** and then takes the **average**.\n",
        "* It gives **equal importance to all labels**, whether they are frequent or rare.\n",
        "\n",
        "**Why we use it:**\n",
        "\n",
        "* In multi-label classification, some labels (like `anger`) are rare and some (like `fear`) are frequent.\n",
        "* F1-Macro ensures the model does **well on every label**, not just the common ones.\n",
        "\n",
        "**How it helps:**\n",
        "\n",
        "* Tells us if the model is **fair across all emotions**, not just the ones that appear most often.\n",
        "\n",
        "**Example (your data):**\n",
        "\n",
        "* `fear` is common, `anger` is rare.\n",
        "* LogisticRegression: F1 for `anger` = 0.018, F1 for `fear` = 0.720 ‚Üí Macro F1 = 0.316\n",
        "* Even though `fear` is high, Macro F1 is low because `anger` is poorly predicted.\n",
        "* **Lesson:** Macro F1 highlights weaknesses on rare emotions.\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ F1-Micro**\n",
        "\n",
        "**Definition (Easy):**\n",
        "\n",
        "* F1-Micro calculates **global F1 score** across **all individual label predictions**.\n",
        "* It treats every label prediction equally and sums over **all true positives, false positives, and false negatives**.\n",
        "\n",
        "**Why we use it:**\n",
        "\n",
        "* Focuses on **overall accuracy across all predictions**, giving more weight to frequent labels.\n",
        "\n",
        "**How it helps:**\n",
        "\n",
        "* Shows how well the model performs **overall**, regardless of label rarity.\n",
        "\n",
        "**Example (your data):**\n",
        "\n",
        "* LogisticRegression F1-Micro = 0.505 ‚Üí 50% of all label predictions are correct on average.\n",
        "* Favors `fear` (common label) over `anger` (rare label) because there are more `fear` labels.\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Hamming Loss**\n",
        "\n",
        "**Definition (Easy):**\n",
        "\n",
        "* Fraction of labels **predicted incorrectly** (either missed or wrong) over all labels.\n",
        "* Lower is better.\n",
        "\n",
        "**Why we use it:**\n",
        "\n",
        "* Multi-label classification can have **multiple correct labels per sample**.\n",
        "* Hamming Loss tells you **how often the model makes mistakes on individual labels**, rather than the whole sample.\n",
        "\n",
        "**How it helps:**\n",
        "\n",
        "* Gives a fine-grained view of **per-label errors**.\n",
        "* Useful to understand **error rate for each emotion**.\n",
        "\n",
        "**Example (your data):**\n",
        "\n",
        "* Hamming Loss for RandomForest = 0.235 ‚Üí ~23.5% of all label predictions are wrong.\n",
        "* If a text has 5 labels, 1‚Äì2 labels are predicted incorrectly on average.\n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Subset Accuracy (Exact Match Ratio)**\n",
        "\n",
        "**Definition (Easy):**\n",
        "\n",
        "* The percentage of samples where **all labels are predicted correctly**.\n",
        "* Very strict metric.\n",
        "\n",
        "**Why we use it:**\n",
        "\n",
        "* Measures **complete prediction correctness**, not just individual labels.\n",
        "* Important when you need **all labels to be correct** for a sample.\n",
        "\n",
        "**How it helps:**\n",
        "\n",
        "* Shows how often the model gets the **entire set of emotions right**.\n",
        "* Very low for multi-label problems with rare labels.\n",
        "\n",
        "**Example (your data):**\n",
        "\n",
        "* Subset Accuracy for RandomForest = 0.282 ‚Üí Only ~28% of texts had **all 5 emotions predicted exactly right**.\n",
        "* Even if F1-macro or F1-micro is good, subset accuracy can be low because predicting every label correctly is hard.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Summary Table of Metrics (Intuitive)**\n",
        "\n",
        "| Metric          | What it Measures                            | Focus                          | Example Insight                                                     |\n",
        "| --------------- | ------------------------------------------- | ------------------------------ | ------------------------------------------------------------------- |\n",
        "| F1-Macro        | Average F1 across all labels                | Rare & frequent labels equally | LogisticRegression F1-Macro = 0.316 ‚Üí rare labels predicted poorly  |\n",
        "| F1-Micro        | Global F1 across all label predictions      | Frequent labels dominate       | F1-Micro = 0.505 ‚Üí overall good performance, favors frequent labels |\n",
        "| Hamming Loss    | Fraction of label prediction mistakes       | Per-label error rate           | 0.235 ‚Üí 23.5% of labels wrong                                       |\n",
        "| Subset Accuracy | Fraction of samples with all labels correct | Exact full prediction          | 0.282 ‚Üí only 28% of texts got all labels right                      |\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Key Takeaways:**\n",
        "\n",
        "* **Macro F1** ‚Üí fairness across labels\n",
        "* **Micro F1** ‚Üí overall performance, dominated by common labels\n",
        "* **Hamming Loss** ‚Üí fine-grained per-label error\n",
        "* **Subset Accuracy** ‚Üí strict measure, complete correctness\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can **draw a visual diagram showing all 4 metrics using your RandomForest results** so you can **see clearly how each metric differs**.\n",
        "\n",
        "Do you want me to make that visual?\n"
      ],
      "metadata": {
        "id": "INZ1ggUnyryM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kn1k3jkMysK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}